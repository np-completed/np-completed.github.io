{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"So much emptyness","title":"Home"},{"location":"gh-actions/broken_links_report/","text":"Report Broken URL Links Technical documentation requires URL links. These links provide essential support for robust programmatic methods. Broken URL links are easy to test and fix. This is a great use case for unit testing. Unit testing evaluates atomic elements of code; the objective is to isolate discrete parts of the code for testing. The devdocs static HTML site is a great use case for unit testing. There are hundreds of files and each file can contain multiple URLs. Manual editing to fix will never happen, so automation summarizes the work for efficiency. Also, project management tools are not productive for this problem. Summary Enable github repo to test for broken url links Review report Read the report for our team devdocs github repo: https://github.com/bayer-int/smol-cls-cloud-docs navigate to issues Open the issue with the report, showing something similar to \"Link Checker Report\". The summary shows the total URL links reviewed \"Total\" (1,166) and \"Errors\" (115). Each markdown file is displayed with URL errors. The network error should also be displayed. Developers can search for their individual files and make corrections.","title":"Broken URL Report"},{"location":"gh-actions/broken_links_report/#report-broken-url-links","text":"Technical documentation requires URL links. These links provide essential support for robust programmatic methods. Broken URL links are easy to test and fix. This is a great use case for unit testing. Unit testing evaluates atomic elements of code; the objective is to isolate discrete parts of the code for testing. The devdocs static HTML site is a great use case for unit testing. There are hundreds of files and each file can contain multiple URLs. Manual editing to fix will never happen, so automation summarizes the work for efficiency. Also, project management tools are not productive for this problem.","title":"Report Broken URL Links"},{"location":"gh-actions/broken_links_report/#summary","text":"Enable github repo to test for broken url links Review report Read the report for our team devdocs github repo: https://github.com/bayer-int/smol-cls-cloud-docs navigate to issues Open the issue with the report, showing something similar to \"Link Checker Report\". The summary shows the total URL links reviewed \"Total\" (1,166) and \"Errors\" (115). Each markdown file is displayed with URL errors. The network error should also be displayed. Developers can search for their individual files and make corrections.","title":"Summary"},{"location":"gh-actions/build-push-image/","text":"Github Actions Build and Push Image to Repository Package Manager No PAT required. This github action uses the GITHUB_TOKEN , available to each repository. # Checkout the files from the Git repository. # Login to the ghcr.io container registry. # Setup Docker # Get metadata for use later in Docker. This avoids having to do manual work to set up the tags and labels for the Docker images. # Finally, build the image and push it. The build and push has two steps name: Docker Build & Publish to GitHub Container Registry on: push: branches: - 'main' # anything under a build/ folder will be used as testing the build processes. # - 'build/*' tags: - 'v*' workflow_dispatch: inputs: git-ref: description: Git Ref (Optional) required: false env: REGISTRY: ghcr.io IMAGE_NAME: ${{ github.repository }} jobs: build-and-push-docker-image: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - name: Log into registry ${{ env.REGISTRY }} uses: docker/login-action@v1 with: registry: ${{ env.REGISTRY }} username: ${{ github.actor }} password: ${{ secrets.GITHUB_TOKEN }} - name: Setup Docker buildx uses: docker/setup-buildx-action@v1 - name: Extract Docker metadata id: meta uses: docker/metadata-action@v2 with: images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }} # - name: Build and Push Versioned Docker Image # id: build-and-push # uses: docker/build-push-action@v2 # if: ${{ github.ref != 'refs/heads/main' }} # with: # context: ./build-push-image # push: true # tags: ${{ steps.meta.outputs.tags }} # labels: ${{ steps.meta.outputs.labels }} - name: Build and Push Latest Docker Image id: build-and-push-latest uses: docker/build-push-action@v2 if: ${{ github.ref == 'refs/heads/main' }} with: context: ./build-push-image push: true tags: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest labels: ${{ steps.meta.outputs.labels }}","title":"Build and Push Image to ghcr"},{"location":"gh-actions/build-push-image/#github-actions-build-and-push-image-to-repository-package-manager","text":"No PAT required. This github action uses the GITHUB_TOKEN , available to each repository. # Checkout the files from the Git repository. # Login to the ghcr.io container registry. # Setup Docker # Get metadata for use later in Docker. This avoids having to do manual work to set up the tags and labels for the Docker images. # Finally, build the image and push it. The build and push has two steps name: Docker Build & Publish to GitHub Container Registry on: push: branches: - 'main' # anything under a build/ folder will be used as testing the build processes. # - 'build/*' tags: - 'v*' workflow_dispatch: inputs: git-ref: description: Git Ref (Optional) required: false env: REGISTRY: ghcr.io IMAGE_NAME: ${{ github.repository }} jobs: build-and-push-docker-image: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - name: Log into registry ${{ env.REGISTRY }} uses: docker/login-action@v1 with: registry: ${{ env.REGISTRY }} username: ${{ github.actor }} password: ${{ secrets.GITHUB_TOKEN }} - name: Setup Docker buildx uses: docker/setup-buildx-action@v1 - name: Extract Docker metadata id: meta uses: docker/metadata-action@v2 with: images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }} # - name: Build and Push Versioned Docker Image # id: build-and-push # uses: docker/build-push-action@v2 # if: ${{ github.ref != 'refs/heads/main' }} # with: # context: ./build-push-image # push: true # tags: ${{ steps.meta.outputs.tags }} # labels: ${{ steps.meta.outputs.labels }} - name: Build and Push Latest Docker Image id: build-and-push-latest uses: docker/build-push-action@v2 if: ${{ github.ref == 'refs/heads/main' }} with: context: ./build-push-image push: true tags: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest labels: ${{ steps.meta.outputs.labels }}","title":"Github Actions Build and Push Image to Repository Package Manager"},{"location":"gh-actions/cachetest/","text":"Caching Docker builds in github actions: Which approach is fastest? https://dev.to/dtinth/caching-docker-builds-in-github-actions-which-approach-is-the-fastest-a-research-18ei Abstract: In this post, I experimented with 6 different approaches for caching Docker builds in GitHub Actions to speed up the build process and compared the results. After trying out every approach, 10 times each, the results show that using GitHub Packages\u2019 Docker registry as a build cache, as opposed to GitHub Actions\u2019 built-in cache, yields the highest performance gain. Unlike self-hosted runners like Jenkins, most cloud-hosted build runners are stateless, providing us with a pristine environment each run. We cannot keep files from the previous runs around; anything that needs to be persisted must be externalized. GitHub Actions has a built-in cache to help do this. But there are many ways of creating that cache ( docker save and docker load first comes to mind). Will the performance gains outweight the overhead caused by saving and loading caches? Are there more approaches other than using GitHub Action\u2019s built-in cache? That\u2019s what this research is about. Dockerfile FROM node:14.21.2 RUN yarn create react-app my-react-app RUN cd my-react-app && yarn build RUN npm install -g @vue/cli && (yes | vue create my-vue-app --default) RUN cd my-vue-app && yarn build RUN mkdir -p my-tests && cd my-tests && yarn add playwright # test # test # test # test # test # test # test # test # test # test","title":"Cache Testing"},{"location":"gh-actions/cachetest/#caching-docker-builds-in-github-actions-which-approach-is-fastest","text":"https://dev.to/dtinth/caching-docker-builds-in-github-actions-which-approach-is-the-fastest-a-research-18ei","title":"Caching Docker builds in github actions: Which approach is fastest?"},{"location":"gh-actions/cachetest/#abstract","text":"In this post, I experimented with 6 different approaches for caching Docker builds in GitHub Actions to speed up the build process and compared the results. After trying out every approach, 10 times each, the results show that using GitHub Packages\u2019 Docker registry as a build cache, as opposed to GitHub Actions\u2019 built-in cache, yields the highest performance gain. Unlike self-hosted runners like Jenkins, most cloud-hosted build runners are stateless, providing us with a pristine environment each run. We cannot keep files from the previous runs around; anything that needs to be persisted must be externalized. GitHub Actions has a built-in cache to help do this. But there are many ways of creating that cache ( docker save and docker load first comes to mind). Will the performance gains outweight the overhead caused by saving and loading caches? Are there more approaches other than using GitHub Action\u2019s built-in cache? That\u2019s what this research is about.","title":"Abstract:"},{"location":"gh-actions/cachetest/#dockerfile","text":"FROM node:14.21.2 RUN yarn create react-app my-react-app RUN cd my-react-app && yarn build RUN npm install -g @vue/cli && (yes | vue create my-vue-app --default) RUN cd my-vue-app && yarn build RUN mkdir -p my-tests && cd my-tests && yarn add playwright # test # test # test # test # test # test # test # test # test # test","title":"Dockerfile"},{"location":"gh-actions/dependabot/","text":"Github Actions Dependabot Create pull requests to keep dependencies up-to-date. version: 2 updates: - package-ecosystem: \"github-actions\" directory: \"/\" schedule: interval: \"weekly\" labels: \"github-actions-dependencies\" assignees: - \"memadsen\" - package-ecosystem: \"pip\" # poetry directory: \"/\" schedule: interval: \"weekly\" labels: \"python-dependencies\" assignees: - \"memadsen\"","title":"Requirements Dependabot"},{"location":"gh-actions/dependabot/#github-actions","text":"","title":"Github Actions"},{"location":"gh-actions/dependabot/#dependabot","text":"Create pull requests to keep dependencies up-to-date. version: 2 updates: - package-ecosystem: \"github-actions\" directory: \"/\" schedule: interval: \"weekly\" labels: \"github-actions-dependencies\" assignees: - \"memadsen\" - package-ecosystem: \"pip\" # poetry directory: \"/\" schedule: interval: \"weekly\" labels: \"python-dependencies\" assignees: - \"memadsen\"","title":"Dependabot"},{"location":"gh-actions/docker_build_push/","text":"Docker Build and Push Image Use github actions to build and push an image to github packages. Source code has an example. The background displayed in these repos is out of scope with the utility of github actions and docker. In this document I reduce the problem to focus on using github actions for docker development (building and debugging docker containers). Artifactory and ECR are optimal however I simplify the problem by ignoring these options and use the repository for hosting the image. hf_ghcr repo in bayer-int huggingface_ghcr repo in memadsen Github Action To authenticate to a GitHub Packages registry within a GitHub Actions workflow, you can use: GITHUB_TOKEN to publish packages associated with the workflow repository. a personal access token (classic) with at least read:packages scope to install packages associated with other private repositories (which GITHUB_TOKEN can't access). Authenticate to github, on behalf of github actions using GITHUB_TOKEN . This allows us to write our docker container to github. Github Actions documentation on token authentication name: docker_build_push on: workflow_dispatch: jobs: build: runs-on:: ubuntu-latest steps: - uses: actions/checkout@v2 - name: docker build python run: | docker build ./python -t name: CI on: workflow_dispatch: jobs: # This workflow contains a single job called \"build\" build: runs-on: ubuntu-latest # Steps represent a sequence of tasks that will be executed as part of the job steps: # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it - uses: actions/checkout@v3 - name: Set up Docker Buildx uses: docker/setup-buildx-action@v1 - name: Authenticate to GitHub container registry uses: docker/login-action@v1.10.0 with: registry: ghcr.io username: ${{ github.actor }} password: ${{ secrets.GITHUB_TOKEN }} #password: ${{ github.token }} - name: Lowercase the repo name and username run: echo \"REPO=${GITHUB_REPOSITORY,,}\" >>${GITHUB_ENV} - name: Build and push container image to registry uses: docker/build-push-action@v2 with: push: true tags: ghcr.io/${{ env.REPO }}:${{ github.sha }} file: ./Dockerfile_pip Dockerfile FROM python:3.9 WORKDIR /code COPY ./requirements.txt /code/requirements.txt RUN pip install --no-cache-dir --upgrade -r /code/requirements.txt COPY ./app /code/app CMD [\"uvicorn\", \"app.main:app\", \"--proxy-headers\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"] Python 3.7.5 dockerfile FROM python:3.7.5-slim # Set up and activate virtual environment ENV VIRTUAL_ENV \"/venv\" RUN python -m venv $VIRTUAL_ENV ENV PATH \"$VIRTUAL_ENV/bin:$PATH\" # Python commands run inside the virtual environment RUN python -m pip install \\ parse \\ realpython-reader Python Latest dockerfile Code for hello.py is print (\"Hello World\") Code for a.py is print (\"Overriden Hello\") #Deriving the latest base image FROM python:latest #Labels as key value pair LABEL Maintainer=\"michael.madsen@bayer.com\" ADD hello.py /home/hello.py ADD a.py /home/a.py CMD [\"/home/hello.py\"] ENTRYPOINT [\"python\"]","title":"Docker Build and Push"},{"location":"gh-actions/docker_build_push/#docker-build-and-push-image","text":"Use github actions to build and push an image to github packages. Source code has an example. The background displayed in these repos is out of scope with the utility of github actions and docker. In this document I reduce the problem to focus on using github actions for docker development (building and debugging docker containers). Artifactory and ECR are optimal however I simplify the problem by ignoring these options and use the repository for hosting the image. hf_ghcr repo in bayer-int huggingface_ghcr repo in memadsen","title":"Docker Build and Push Image"},{"location":"gh-actions/docker_build_push/#github-action","text":"To authenticate to a GitHub Packages registry within a GitHub Actions workflow, you can use: GITHUB_TOKEN to publish packages associated with the workflow repository. a personal access token (classic) with at least read:packages scope to install packages associated with other private repositories (which GITHUB_TOKEN can't access). Authenticate to github, on behalf of github actions using GITHUB_TOKEN . This allows us to write our docker container to github. Github Actions documentation on token authentication name: docker_build_push on: workflow_dispatch: jobs: build: runs-on:: ubuntu-latest steps: - uses: actions/checkout@v2 - name: docker build python run: | docker build ./python -t name: CI on: workflow_dispatch: jobs: # This workflow contains a single job called \"build\" build: runs-on: ubuntu-latest # Steps represent a sequence of tasks that will be executed as part of the job steps: # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it - uses: actions/checkout@v3 - name: Set up Docker Buildx uses: docker/setup-buildx-action@v1 - name: Authenticate to GitHub container registry uses: docker/login-action@v1.10.0 with: registry: ghcr.io username: ${{ github.actor }} password: ${{ secrets.GITHUB_TOKEN }} #password: ${{ github.token }} - name: Lowercase the repo name and username run: echo \"REPO=${GITHUB_REPOSITORY,,}\" >>${GITHUB_ENV} - name: Build and push container image to registry uses: docker/build-push-action@v2 with: push: true tags: ghcr.io/${{ env.REPO }}:${{ github.sha }} file: ./Dockerfile_pip","title":"Github Action"},{"location":"gh-actions/docker_build_push/#dockerfile","text":"FROM python:3.9 WORKDIR /code COPY ./requirements.txt /code/requirements.txt RUN pip install --no-cache-dir --upgrade -r /code/requirements.txt COPY ./app /code/app CMD [\"uvicorn\", \"app.main:app\", \"--proxy-headers\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]","title":"Dockerfile"},{"location":"gh-actions/docker_build_push/#python-375-dockerfile","text":"FROM python:3.7.5-slim # Set up and activate virtual environment ENV VIRTUAL_ENV \"/venv\" RUN python -m venv $VIRTUAL_ENV ENV PATH \"$VIRTUAL_ENV/bin:$PATH\" # Python commands run inside the virtual environment RUN python -m pip install \\ parse \\ realpython-reader","title":"Python 3.7.5 dockerfile"},{"location":"gh-actions/docker_build_push/#python-latest-dockerfile","text":"Code for hello.py is print (\"Hello World\") Code for a.py is print (\"Overriden Hello\") #Deriving the latest base image FROM python:latest #Labels as key value pair LABEL Maintainer=\"michael.madsen@bayer.com\" ADD hello.py /home/hello.py ADD a.py /home/a.py CMD [\"/home/hello.py\"] ENTRYPOINT [\"python\"]","title":"Python Latest dockerfile"},{"location":"gh-actions/ecr_actions_workflow/","text":"CICD Pipeline for Docker Images This is a new repository using DevOps best practices and Python. A project scaffold is setup to use cloud microservices in a CICD workflow. This scaffold can be modified to fit a typical data science project. Access this code at (bayer-int/smol-cls-ecr)[https://github.com/bayer-int/smol-cls-ecr] Summary Build production docker images using CICD pipeline (GH Actions), lint docker/python and push to AWS ECR. Authentication using OIDC is preferred, because there are no keys/secrets to manage. GH Actions is used to build a docker image and directly pushed to ECR from GH Actions. MLOps Best Practices My web application is a microservice flowchart LR CICD(CICD) --> Container(Container) Microservice(Microservice) Container --> Microservice fxn(Function) <--> Web(Web) Microservice --> fxn Scaffold My preferred process for every new project. This GitHub repo has all the ingredients for a building a container based API deployed to AWS: Makefile , requirements.txt , virtual environment, directory structure, application files, application test files and Dockerfile . flowchart LR GH(GitHub Checkout) --> code(Codebase <br> _______ <br> - App Scripts <br> - App Directories <br> - Test App Logic <br> - Virtual Environment <br> - Library Requirements <br> - Container) The Makefile and GH Actions .yml orchestrate all the logic. This minimizes debugging and collaboration/ramping-up. The GH Actions runner implements directly from the Makefile . In addition the Makefile can be used for early development in local environment. Production code should run exactly like local code but in practice something usually breaks and minor manual changes are required; typically there is a bias towards production. Project Makefile : install: # Install Py lib pip install --upgrade pip &&\\ pip install -r requirements.txt #cat requirements.txt | xargs poetry add post-install: python -m textblob.download_corpora git: # Update Git Repo bash sync_git.sh format: # format code using black black *.py mylib/*.py lint: # format code using pylint or flake8 pylint --disable=R,C *.py mylib/*.py test: # test code python -m pytest -vv --cov=mylib --cov=main test_*.py build: #build container docker build -t deploy-fastapi . run: #run docker docker run -p 127.0.0.1:8080:8080 deploy-fastapi deploy: aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 722540083300.dkr.ecr.us-east-1.amazonaws.com docker build -t smol-cls-ecr . ## This is the ECR name docker tag smol-cls-ecr:latest 722540083300.dkr.ecr.us-east-1.amazonaws.com/smol-cls-ecr:latest docker push 722540083300.dkr.ecr.us-east-1.amazonaws.com/smol-cls-ecr:latest all: install post-install lint test deploy Development Write code in local environment, use GH Actions to test and then deploy to AWS. Workflow OIDC authenticate Allow GH Actions to authenticate to AWS using OIDC. Build docker image using GH Actions and push to ECR. In your cloud provider, create an OIDC trust between your cloud role and your GitHub workflow(s) that need access to the cloud. Every time your job runs, GitHub's OIDC Provider auto-generates an OIDC token. This token contains multiple claims to establish a security-hardened and verifiable identity about the specific workflow that is trying to authenticate. You could include a step or action in your job to request this token from GitHub's OIDC provider, and present it to the cloud provider. Once the cloud provider successfully validates the claims presented in the token, it then provides a short-lived cloud access token that is available only for the duration of the job. :closed_book: For more information see the github docs for security hardening production deployments link: https://docs.github.com/en/actions/deployment/security-hardening-your-deployments/about-security-hardening-with-openid-connect AWS Create Service catalog product [ ] TODO Launch this CF stack using CLI. This project is currently provisioned using the AWS console for CF and IAM. Start Work: Navigate to AWS Console. From the AWS console >> Service Catalog >> Products the github-oidc-provider will show up like this: Next, build CloudFormation stack for cloud engineering product: github oidc provider . In CloudFormation parameters set bayer-int and repo name smol-cls-cicd :pencil: Update repo name for each new project. Next set CF parameters. parameter content required GitHub org bayer-int true OIDCProviderArn When catalog item was already used once. Use arn of generated OIDC Provider. false Repository Name Technical name of the repository, as seen in the url. e.g smol-cls-cicd true AWS IAM setup Trusted entitites policy is attached during CloudFormation build. It should look like this: { \"Version\": \"2008-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::722540083300:oidc-provider/token.actions.githubusercontent.com\", \"AWS\": \"arn:aws:iam::722540083300:role/cloudops\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringLike\": { \"token.actions.githubusercontent.com:sub\": \"repo:bayer-int/smol-cls-mwaa-cicd:*\" } } }, { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::722540083300:role/cloudops\" }, \"Action\": \"sts:AssumeRole\" } ] } Attach ECR policy to OIDC role Attach existing policies directly. In \"find policies\" search for AmazonEC2ContainerRegistryFullAccess and check it which will give this User Account permission to push on Private ECR. GitHub Actions Add file for GH Actions and document workflow events. A simple project has the following directories: \u251c\u2500\u2500 .github \u2502 \u2514\u2500\u2500 workflows \u2502 \u2514\u2500\u2500 deploy_ecr.yml ## GitHub Actions \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 Dockerfile ## Dockerfile in project root \u251c\u2500\u2500 Makefile ## Makefile instructs my GH Actions \u251c\u2500\u2500 README.md \u251c\u2500\u2500 img \u251c\u2500\u2500 main.py \u251c\u2500\u2500 mylib ## python logic \u251c\u2500\u2500 requirements.txt ## virtual environment \u251c\u2500\u2500 sync_git.sh ## optional sync git \u2514\u2500\u2500 test_main.py Include GitHub actions permissions ## This allows your GitHub Actions job to save the temporary ## credentials it gets when authenticating with the Cloud Provider. permissions: contents: 'read' id-token: 'write' Use the aws-actions/configure-aws-credentials action. - name: Configure AWS Credentials uses: aws-actions/configure-aws-credentials@v1 with: role-to-assume: arn:aws:iam::123456789100:role/github-actions-role aws-region: eu-east-1 Test GH Actions server runs as expected Push code to git Verify docker image is in ECR Reference This project builds from the OIDC auth method documented in Bayer go docs. To review the details go to Authenticate to Cloud - Bayer go docs End","title":"ECR Build & Push"},{"location":"gh-actions/ecr_actions_workflow/#cicd-pipeline-for-docker-images","text":"This is a new repository using DevOps best practices and Python. A project scaffold is setup to use cloud microservices in a CICD workflow. This scaffold can be modified to fit a typical data science project. Access this code at (bayer-int/smol-cls-ecr)[https://github.com/bayer-int/smol-cls-ecr]","title":"CICD Pipeline for Docker Images"},{"location":"gh-actions/ecr_actions_workflow/#summary","text":"Build production docker images using CICD pipeline (GH Actions), lint docker/python and push to AWS ECR. Authentication using OIDC is preferred, because there are no keys/secrets to manage. GH Actions is used to build a docker image and directly pushed to ECR from GH Actions.","title":"Summary"},{"location":"gh-actions/ecr_actions_workflow/#mlops-best-practices","text":"My web application is a microservice flowchart LR CICD(CICD) --> Container(Container) Microservice(Microservice) Container --> Microservice fxn(Function) <--> Web(Web) Microservice --> fxn","title":"MLOps Best Practices"},{"location":"gh-actions/ecr_actions_workflow/#scaffold","text":"My preferred process for every new project. This GitHub repo has all the ingredients for a building a container based API deployed to AWS: Makefile , requirements.txt , virtual environment, directory structure, application files, application test files and Dockerfile . flowchart LR GH(GitHub Checkout) --> code(Codebase <br> _______ <br> - App Scripts <br> - App Directories <br> - Test App Logic <br> - Virtual Environment <br> - Library Requirements <br> - Container) The Makefile and GH Actions .yml orchestrate all the logic. This minimizes debugging and collaboration/ramping-up. The GH Actions runner implements directly from the Makefile . In addition the Makefile can be used for early development in local environment. Production code should run exactly like local code but in practice something usually breaks and minor manual changes are required; typically there is a bias towards production.","title":"Scaffold"},{"location":"gh-actions/ecr_actions_workflow/#project-makefile","text":"install: # Install Py lib pip install --upgrade pip &&\\ pip install -r requirements.txt #cat requirements.txt | xargs poetry add post-install: python -m textblob.download_corpora git: # Update Git Repo bash sync_git.sh format: # format code using black black *.py mylib/*.py lint: # format code using pylint or flake8 pylint --disable=R,C *.py mylib/*.py test: # test code python -m pytest -vv --cov=mylib --cov=main test_*.py build: #build container docker build -t deploy-fastapi . run: #run docker docker run -p 127.0.0.1:8080:8080 deploy-fastapi deploy: aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 722540083300.dkr.ecr.us-east-1.amazonaws.com docker build -t smol-cls-ecr . ## This is the ECR name docker tag smol-cls-ecr:latest 722540083300.dkr.ecr.us-east-1.amazonaws.com/smol-cls-ecr:latest docker push 722540083300.dkr.ecr.us-east-1.amazonaws.com/smol-cls-ecr:latest all: install post-install lint test deploy","title":"Project Makefile:"},{"location":"gh-actions/ecr_actions_workflow/#development","text":"Write code in local environment, use GH Actions to test and then deploy to AWS.","title":"Development"},{"location":"gh-actions/ecr_actions_workflow/#workflow","text":"","title":"Workflow"},{"location":"gh-actions/ecr_actions_workflow/#oidc-authenticate","text":"Allow GH Actions to authenticate to AWS using OIDC. Build docker image using GH Actions and push to ECR. In your cloud provider, create an OIDC trust between your cloud role and your GitHub workflow(s) that need access to the cloud. Every time your job runs, GitHub's OIDC Provider auto-generates an OIDC token. This token contains multiple claims to establish a security-hardened and verifiable identity about the specific workflow that is trying to authenticate. You could include a step or action in your job to request this token from GitHub's OIDC provider, and present it to the cloud provider. Once the cloud provider successfully validates the claims presented in the token, it then provides a short-lived cloud access token that is available only for the duration of the job. :closed_book: For more information see the github docs for security hardening production deployments link: https://docs.github.com/en/actions/deployment/security-hardening-your-deployments/about-security-hardening-with-openid-connect","title":"OIDC authenticate"},{"location":"gh-actions/ecr_actions_workflow/#aws-create-service-catalog-product","text":"[ ] TODO Launch this CF stack using CLI. This project is currently provisioned using the AWS console for CF and IAM. Start Work: Navigate to AWS Console. From the AWS console >> Service Catalog >> Products the github-oidc-provider will show up like this: Next, build CloudFormation stack for cloud engineering product: github oidc provider . In CloudFormation parameters set bayer-int and repo name smol-cls-cicd :pencil: Update repo name for each new project. Next set CF parameters. parameter content required GitHub org bayer-int true OIDCProviderArn When catalog item was already used once. Use arn of generated OIDC Provider. false Repository Name Technical name of the repository, as seen in the url. e.g smol-cls-cicd true","title":"AWS Create Service catalog product"},{"location":"gh-actions/ecr_actions_workflow/#aws-iam-setup","text":"Trusted entitites policy is attached during CloudFormation build. It should look like this: { \"Version\": \"2008-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::722540083300:oidc-provider/token.actions.githubusercontent.com\", \"AWS\": \"arn:aws:iam::722540083300:role/cloudops\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringLike\": { \"token.actions.githubusercontent.com:sub\": \"repo:bayer-int/smol-cls-mwaa-cicd:*\" } } }, { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::722540083300:role/cloudops\" }, \"Action\": \"sts:AssumeRole\" } ] }","title":"AWS IAM setup"},{"location":"gh-actions/ecr_actions_workflow/#attach-ecr-policy-to-oidc-role","text":"Attach existing policies directly. In \"find policies\" search for AmazonEC2ContainerRegistryFullAccess and check it which will give this User Account permission to push on Private ECR.","title":"Attach ECR policy to OIDC role"},{"location":"gh-actions/ecr_actions_workflow/#github-actions","text":"Add file for GH Actions and document workflow events. A simple project has the following directories: \u251c\u2500\u2500 .github \u2502 \u2514\u2500\u2500 workflows \u2502 \u2514\u2500\u2500 deploy_ecr.yml ## GitHub Actions \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 Dockerfile ## Dockerfile in project root \u251c\u2500\u2500 Makefile ## Makefile instructs my GH Actions \u251c\u2500\u2500 README.md \u251c\u2500\u2500 img \u251c\u2500\u2500 main.py \u251c\u2500\u2500 mylib ## python logic \u251c\u2500\u2500 requirements.txt ## virtual environment \u251c\u2500\u2500 sync_git.sh ## optional sync git \u2514\u2500\u2500 test_main.py","title":"GitHub Actions"},{"location":"gh-actions/ecr_actions_workflow/#include-github-actions-permissions","text":"## This allows your GitHub Actions job to save the temporary ## credentials it gets when authenticating with the Cloud Provider. permissions: contents: 'read' id-token: 'write' Use the aws-actions/configure-aws-credentials action. - name: Configure AWS Credentials uses: aws-actions/configure-aws-credentials@v1 with: role-to-assume: arn:aws:iam::123456789100:role/github-actions-role aws-region: eu-east-1","title":"Include GitHub actions permissions"},{"location":"gh-actions/ecr_actions_workflow/#test-gh-actions-server-runs-as-expected","text":"Push code to git Verify docker image is in ECR","title":"Test GH Actions server runs as expected"},{"location":"gh-actions/ecr_actions_workflow/#reference","text":"This project builds from the OIDC auth method documented in Bayer go docs. To review the details go to Authenticate to Cloud - Bayer go docs","title":"Reference"},{"location":"gh-actions/ecr_actions_workflow/#end","text":"","title":"End"},{"location":"gh-actions/env_var/","text":"Github Actions Environment Variables Example jobs.<job_id>.if Only run job for specific repository. This example uses if to control when the production-deploy job can run. It will only run if the repository is named MY_REPO and is within the bayer-int organization. Otherwise, the job will be marked as skipped. github actions workflow syntax docs name: example-workflow on: [push] jobs: production-deploy: if: github.repository == 'bayer-int/MY_REPO' runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - uses: actions/setup-node@v3 with: node-version: '14' - run: echo 'Hello World' Contexts Contexts are a way to access information about workflow runs, variables, runner environments, jobs, and steps. Each context is an object that contains properties, which can be strings or other objects. github actions contexts docs Example usage of vars context This example workflow shows how configuration variables set at the repository, environment, or organization levels are automatically available using the vars context. on: workflow_dispatch: env: # Setting an environment variable with the value of a configuration variable env_var: ${{ vars.ENV_CONTEXT_VAR }} jobs: display-variables: name: ${{ vars.JOB_NAME }} # You can use configuration variables with the `vars` context for dynamic jobs if: ${{ vars.USE_VARIABLES == 'true' }} runs-on: ${{ vars.RUNNER }} environment: ${{ vars.ENVIRONMENT_STAGE }} steps: - name: Use variables run: | echo \"repository variable : ${{ vars.REPOSITORY_VAR }}\" echo \"organization variable : ${{ vars.ORGANIZATION_VAR }}\" echo \"overridden variable : ${{ vars.OVERRIDE_VAR }}\" echo \"variable from shell environment : $env_var\"","title":"Github Actions Environment Variables"},{"location":"gh-actions/env_var/#github-actions-environment-variables","text":"","title":"Github Actions Environment Variables"},{"location":"gh-actions/env_var/#example-jobsjob_idif","text":"Only run job for specific repository. This example uses if to control when the production-deploy job can run. It will only run if the repository is named MY_REPO and is within the bayer-int organization. Otherwise, the job will be marked as skipped. github actions workflow syntax docs name: example-workflow on: [push] jobs: production-deploy: if: github.repository == 'bayer-int/MY_REPO' runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - uses: actions/setup-node@v3 with: node-version: '14' - run: echo 'Hello World'","title":"Example jobs.&lt;job_id&gt;.if"},{"location":"gh-actions/env_var/#contexts","text":"Contexts are a way to access information about workflow runs, variables, runner environments, jobs, and steps. Each context is an object that contains properties, which can be strings or other objects. github actions contexts docs","title":"Contexts"},{"location":"gh-actions/env_var/#example-usage-of-vars-context","text":"This example workflow shows how configuration variables set at the repository, environment, or organization levels are automatically available using the vars context. on: workflow_dispatch: env: # Setting an environment variable with the value of a configuration variable env_var: ${{ vars.ENV_CONTEXT_VAR }} jobs: display-variables: name: ${{ vars.JOB_NAME }} # You can use configuration variables with the `vars` context for dynamic jobs if: ${{ vars.USE_VARIABLES == 'true' }} runs-on: ${{ vars.RUNNER }} environment: ${{ vars.ENVIRONMENT_STAGE }} steps: - name: Use variables run: | echo \"repository variable : ${{ vars.REPOSITORY_VAR }}\" echo \"organization variable : ${{ vars.ORGANIZATION_VAR }}\" echo \"overridden variable : ${{ vars.OVERRIDE_VAR }}\" echo \"variable from shell environment : $env_var\"","title":"Example usage of vars context"},{"location":"gh-actions/gh_actions_cheatsheet/","text":"GitHub Actions Basic Template Template for using GitHub actions. This file lives in the workflows directory, <my_repo/><.github/workflows/gh_action.yml> My directory tree showing the workflows dir, relative to the project root. Actions Template . \u251c\u2500\u2500 docs \u251c\u2500\u2500 .github \u2502 \u2514\u2500\u2500 workflows \u2502 \u2514\u2500\u2500 gh_action.yml name: OIDC auth Push ECR from GH Actions docker build ## Trigger #on: [push] on: push: paths: - 's3_bucket/**' - '.github/**' workflow_dispatch: permissions: contents: 'read' id-token: 'write' jobs: test: name: github oidc sync to AWS runs-on: ubuntu-latest steps: - name: Git clone the repository uses: actions/checkout@v2 ## Requires team_users.txt with github users, e.g. memadsen - name: User list to authorize GH Action build run: | # if user not in list, exit while read line; do if [ ${{ github.actor }} == \"$line\" ]; then echo \"----- ${{ github.actor }} is authorized user -----\" ALLOWED_USER=${{ github.actor }} break fi done < \".github/team_users.txt\" if [ -z \"$ALLOWED_USER\"]; then echo \"----- Job triggered by unauthorized user -----\" exit 1 fi ## OIDC Auth ## Requires AWS IAM Role ARN - name: Configure AWS Credentials uses: aws-actions/configure-aws-credentials@v1 with: role-to-assume: arn:aws:iam::073416988478:role/SC-073416988478-pp-jgiui7c5d7xx2-Role-SLI0NYCKDL9J aws-region: us-east-1 - name: STS verify AWS connect to OIDC auth run: aws sts get-caller-identity - name: s3 sync and rm S3 files not in local run: | aws s3 sync s3_bucket/ s3://mulesoft-fastapi/local_folder/ --delete - name: Update AWS Lambda run: | aws lambda update-function-code \\ --function-name mulesoft-fastapi \\ --s3-bucket mulesoft-fastapi \\ --s3-key local_folder/deployment-package.zip \\ --publish Event Triggers Event to run job. Trigger using a regex pattern. on: push: tags: - 'v*' You can make a advanced rule that only triggers on v1 and up or excludes -alpha releases, if that matters to you. Trigger on a push to main or a tag on: push: branches: - main tags: Trigger on a push to main and a tag. on: push: branches: - main tags: jobs: build-deploy: if: startsWith(github.ref, 'refs/tags/') steps: # ... There is no on.tag or on.tags option, but there is on.release - see below.","title":"GH Actions Cheatsheet"},{"location":"gh-actions/gh_actions_cheatsheet/#github-actions-basic-template","text":"Template for using GitHub actions. This file lives in the workflows directory, <my_repo/><.github/workflows/gh_action.yml> My directory tree showing the workflows dir, relative to the project root.","title":"GitHub Actions Basic Template"},{"location":"gh-actions/gh_actions_cheatsheet/#actions-template","text":". \u251c\u2500\u2500 docs \u251c\u2500\u2500 .github \u2502 \u2514\u2500\u2500 workflows \u2502 \u2514\u2500\u2500 gh_action.yml name: OIDC auth Push ECR from GH Actions docker build ## Trigger #on: [push] on: push: paths: - 's3_bucket/**' - '.github/**' workflow_dispatch: permissions: contents: 'read' id-token: 'write' jobs: test: name: github oidc sync to AWS runs-on: ubuntu-latest steps: - name: Git clone the repository uses: actions/checkout@v2 ## Requires team_users.txt with github users, e.g. memadsen - name: User list to authorize GH Action build run: | # if user not in list, exit while read line; do if [ ${{ github.actor }} == \"$line\" ]; then echo \"----- ${{ github.actor }} is authorized user -----\" ALLOWED_USER=${{ github.actor }} break fi done < \".github/team_users.txt\" if [ -z \"$ALLOWED_USER\"]; then echo \"----- Job triggered by unauthorized user -----\" exit 1 fi ## OIDC Auth ## Requires AWS IAM Role ARN - name: Configure AWS Credentials uses: aws-actions/configure-aws-credentials@v1 with: role-to-assume: arn:aws:iam::073416988478:role/SC-073416988478-pp-jgiui7c5d7xx2-Role-SLI0NYCKDL9J aws-region: us-east-1 - name: STS verify AWS connect to OIDC auth run: aws sts get-caller-identity - name: s3 sync and rm S3 files not in local run: | aws s3 sync s3_bucket/ s3://mulesoft-fastapi/local_folder/ --delete - name: Update AWS Lambda run: | aws lambda update-function-code \\ --function-name mulesoft-fastapi \\ --s3-bucket mulesoft-fastapi \\ --s3-key local_folder/deployment-package.zip \\ --publish","title":"Actions Template"},{"location":"gh-actions/gh_actions_cheatsheet/#event-triggers","text":"Event to run job. Trigger using a regex pattern. on: push: tags: - 'v*' You can make a advanced rule that only triggers on v1 and up or excludes -alpha releases, if that matters to you. Trigger on a push to main or a tag on: push: branches: - main tags: Trigger on a push to main and a tag. on: push: branches: - main tags: jobs: build-deploy: if: startsWith(github.ref, 'refs/tags/') steps: # ... There is no on.tag or on.tags option, but there is on.release - see below.","title":"Event Triggers"},{"location":"gh-actions/gh_actions_tutorial/","text":"GitHub Actions Github actions is a virtual machine used to do work. Unlike self-hosted runners like Jenkins, most cloud-hosted build runners are stateless, providing us with a pristine environment each run. We cannot keep files from the previous runs around; anything that needs to be persisted must be externalized. Github Actions - Architecture GitHub Actions are formed by a set of components. There are six main components of GitHub Actions: Workflows: Automated procedure added to the repository, and is the actual Action itself Events: An activity that triggers a workflow; these can be based on events such as push or pull requests, but they can also be scheduled using the crontab syntax Jobs: A group of one or more steps that are executed inside a runner Steps: These are tasks from a job that can be used to run commands Actions: The standalone commands from the steps Runners: A server that has the GHA runner application installed. The runner is a server provided by GitHub to run your workflows (also known as Actions). They are deployed and are terminated after your automation is completed. Although there are some limits regarding this service, users do not pay anything to use it, even with a free GitHub account. A useful cheatsheet for syntax to GitHub action workflows Network security testing on GitHub Actions End","title":"GH Actions Description"},{"location":"gh-actions/gh_actions_tutorial/#github-actions","text":"Github actions is a virtual machine used to do work. Unlike self-hosted runners like Jenkins, most cloud-hosted build runners are stateless, providing us with a pristine environment each run. We cannot keep files from the previous runs around; anything that needs to be persisted must be externalized.","title":"GitHub Actions"},{"location":"gh-actions/gh_actions_tutorial/#github-actions-architecture","text":"GitHub Actions are formed by a set of components. There are six main components of GitHub Actions: Workflows: Automated procedure added to the repository, and is the actual Action itself Events: An activity that triggers a workflow; these can be based on events such as push or pull requests, but they can also be scheduled using the crontab syntax Jobs: A group of one or more steps that are executed inside a runner Steps: These are tasks from a job that can be used to run commands Actions: The standalone commands from the steps Runners: A server that has the GHA runner application installed. The runner is a server provided by GitHub to run your workflows (also known as Actions). They are deployed and are terminated after your automation is completed. Although there are some limits regarding this service, users do not pay anything to use it, even with a free GitHub account. A useful cheatsheet for syntax to GitHub action workflows Network security testing on GitHub Actions","title":"Github Actions - Architecture"},{"location":"gh-actions/gh_actions_tutorial/#end","text":"","title":"End"},{"location":"gh-actions/push_file_remote/","text":"Update Remote Repository A centralized repository is used for documenting data science governance. Information on CSP, best practices and announcements. This is the \"source of truth\" to provide guidance to CLS SMol teams, working on data science projects. Update projects at scale Documentation updates need to scale across projects. Github is the standard for source code management. Templates provide boilerplate to ensure alignment on sharing data and ML-governance. Project source code is updated automagically from the ML governance documentation. Project files are independent of updates from ML-Governance. gitGraph commit id: \"1\" branch ML-Governance commit id: \"2\" commit id: \"3\" commit id: \"4\" checkout main commit id: \"5\" commit id: \"6\" merge ML-Governance id: \"Merge ML Docs\" tag: \"V 1.0\" type: HIGHLIGHT commit id: \"7\" commit id: \"8\" Implementation Generate PAT in my personal repo and scope for GH Actions. Navigate to my personal repo and open settings --> Developer settings: Next generate a personal access token and save it somewhere safe: In another tab open the source repo used to write files, bayer-int/smol-cls-docs . Open the Security --> Secrets --> Actions and add the secret. GH Actions Workflow The workflow to push files to another repo. name: Push File on: push: branches: - main workflow_dispatch: jobs: copy-file: runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v2 - name: Pushes test file uses: dmnemec/copy_file_to_another_repo_action@main env: API_TOKEN_GITHUB: ${{ secrets.PUSH_FILE }} with: source_file: 'poetry.lock' destination_repo: 'bayer-int/hf_ghcr' destination_folder: 'test-dir' user_email: 'michael.madsen@bayer.com' user_name: 'memadsen' commit_message: 'A custom message for the commit'","title":"Push Remote File"},{"location":"gh-actions/push_file_remote/#update-remote-repository","text":"A centralized repository is used for documenting data science governance. Information on CSP, best practices and announcements. This is the \"source of truth\" to provide guidance to CLS SMol teams, working on data science projects.","title":"Update Remote Repository"},{"location":"gh-actions/push_file_remote/#update-projects-at-scale","text":"Documentation updates need to scale across projects. Github is the standard for source code management. Templates provide boilerplate to ensure alignment on sharing data and ML-governance. Project source code is updated automagically from the ML governance documentation. Project files are independent of updates from ML-Governance. gitGraph commit id: \"1\" branch ML-Governance commit id: \"2\" commit id: \"3\" commit id: \"4\" checkout main commit id: \"5\" commit id: \"6\" merge ML-Governance id: \"Merge ML Docs\" tag: \"V 1.0\" type: HIGHLIGHT commit id: \"7\" commit id: \"8\"","title":"Update projects at scale"},{"location":"gh-actions/push_file_remote/#implementation","text":"Generate PAT in my personal repo and scope for GH Actions. Navigate to my personal repo and open settings --> Developer settings: Next generate a personal access token and save it somewhere safe: In another tab open the source repo used to write files, bayer-int/smol-cls-docs . Open the Security --> Secrets --> Actions and add the secret.","title":"Implementation"},{"location":"gh-actions/push_file_remote/#gh-actions-workflow","text":"The workflow to push files to another repo. name: Push File on: push: branches: - main workflow_dispatch: jobs: copy-file: runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v2 - name: Pushes test file uses: dmnemec/copy_file_to_another_repo_action@main env: API_TOKEN_GITHUB: ${{ secrets.PUSH_FILE }} with: source_file: 'poetry.lock' destination_repo: 'bayer-int/hf_ghcr' destination_folder: 'test-dir' user_email: 'michael.madsen@bayer.com' user_name: 'memadsen' commit_message: 'A custom message for the commit'","title":"GH Actions Workflow"},{"location":"gh-actions/read_project_file/","text":"Github Actions Read Files in Project Configure github actions to access files locally. Team User List User list limits user access to the github action build. Add file in .github/team_users.txt and add github user, e.g. memadsen. Now, only memadsen is authorized to build on github actions. jobs: build: runs-on: ubuntu-latest # Steps represent a sequence of tasks that will be executed as part of the job steps: - name: Checks-out repository under $GITHUB_WORKSPACE, so your job can access it uses: actions/checkout@v3 ## Requires team_users.txt with github users, e.g. memadsen - name: User list to authorize GH Action build run: | # if user not in list, exit while read line; do if [ ${{ github.actor }} == \"$line\" ]; then echo \"----- ${{ github.actor }} is authorized user -----\" ALLOWED_USER=${{ github.actor }} break fi done < \".github/team_users.txt\" if [ -z \"$ALLOWED_USER\"]; then echo \"----- Job triggered by unauthorized user -----\" exit 1 fi Github metadata and environment variables name: Context Example on: [push] jobs: build: name: Inspect context runs-on: ubuntu-latest steps: - name: Inspect context run: | echo 'Github Repository ------ \\n ${{ github.repository }} ' echo 'Github use ---------- \\n ${{ github.actor }}'' build: runs-on: ubuntu-latest # Steps represent a sequence of tasks that will be executed as part of the job steps: # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it - uses: actions/checkout@v2 # Runs commands using the runners shell - name: Run the build php script run: | cd build php build.php # This file reads a few json files and then creates a set of html files in the doc folder cd ../ - name: Commit files # transfer the new html files back into the repository run: | git config --local user.name \"jpadfield\" git add ./docs git commit -m \"Updating the repository GitHub html pages in the docs folder\" - name: Push changes # push the output folder to your repo uses: ad-m/github-push-action@master with: github_token: ${{ secrets.GITHUB_TOKEN }} force: true Check if File exists Check (e.g. with git diff --exit-code ) if there actually are any changes, and skip the commit if not. Definitely better than committing deletion first and then (possibly identical) new files. Example: - id: commit_files name: Commit files run: git config --local user.name actions-user git config --local user.email \"actions@github.com\" if ! git diff --exit-code; then git add your/files/here git commit -am \"GH Action Files added $(date)\" git push -f origin main fi","title":"Read Project File"},{"location":"gh-actions/read_project_file/#github-actions-read-files-in-project","text":"Configure github actions to access files locally.","title":"Github Actions Read Files in Project"},{"location":"gh-actions/read_project_file/#team-user-list","text":"User list limits user access to the github action build. Add file in .github/team_users.txt and add github user, e.g. memadsen. Now, only memadsen is authorized to build on github actions. jobs: build: runs-on: ubuntu-latest # Steps represent a sequence of tasks that will be executed as part of the job steps: - name: Checks-out repository under $GITHUB_WORKSPACE, so your job can access it uses: actions/checkout@v3 ## Requires team_users.txt with github users, e.g. memadsen - name: User list to authorize GH Action build run: | # if user not in list, exit while read line; do if [ ${{ github.actor }} == \"$line\" ]; then echo \"----- ${{ github.actor }} is authorized user -----\" ALLOWED_USER=${{ github.actor }} break fi done < \".github/team_users.txt\" if [ -z \"$ALLOWED_USER\"]; then echo \"----- Job triggered by unauthorized user -----\" exit 1 fi","title":"Team User List"},{"location":"gh-actions/read_project_file/#github-metadata-and-environment-variables","text":"name: Context Example on: [push] jobs: build: name: Inspect context runs-on: ubuntu-latest steps: - name: Inspect context run: | echo 'Github Repository ------ \\n ${{ github.repository }} ' echo 'Github use ---------- \\n ${{ github.actor }}'' build: runs-on: ubuntu-latest # Steps represent a sequence of tasks that will be executed as part of the job steps: # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it - uses: actions/checkout@v2 # Runs commands using the runners shell - name: Run the build php script run: | cd build php build.php # This file reads a few json files and then creates a set of html files in the doc folder cd ../ - name: Commit files # transfer the new html files back into the repository run: | git config --local user.name \"jpadfield\" git add ./docs git commit -m \"Updating the repository GitHub html pages in the docs folder\" - name: Push changes # push the output folder to your repo uses: ad-m/github-push-action@master with: github_token: ${{ secrets.GITHUB_TOKEN }} force: true","title":"Github metadata and environment variables"},{"location":"gh-actions/read_project_file/#check-if-file-exists","text":"Check (e.g. with git diff --exit-code ) if there actually are any changes, and skip the commit if not. Definitely better than committing deletion first and then (possibly identical) new files. Example: - id: commit_files name: Commit files run: git config --local user.name actions-user git config --local user.email \"actions@github.com\" if ! git diff --exit-code; then git add your/files/here git commit -am \"GH Action Files added $(date)\" git push -f origin main fi","title":"Check if File exists"},{"location":"gh-actions/runner/","text":"Self Hosted Runner Github Actions using self hosted runner. name: Github runner demo on: [push] jobs: runner-demo: runs-on: [self-hosted, linux] steps: - run: echo \"I'm using my self-hosted runner for this.\"","title":"Self Hosted Runner"},{"location":"gh-actions/runner/#self-hosted-runner","text":"Github Actions using self hosted runner. name: Github runner demo on: [push] jobs: runner-demo: runs-on: [self-hosted, linux] steps: - run: echo \"I'm using my self-hosted runner for this.\"","title":"Self Hosted Runner"},{"location":"gh-actions/sbom/","text":"SBOM Software Bill of Material with Docker Container and Github Actions Generate an SBOM from within a GitHub Actions workflow. In this way, the SBOM is shipped with the container image and is made available without having to scan the image each time. What is SBOM? From Wikipedia: A software bill of materials (SBOM) is a list of components in a piece of software. Software vendors often create products by assembling open source and commercial software components. The SBOM describes the components in a product. It is analogous to a list of ingredients on food packaging: where you might consult a label to avoid foods that may cause an allergies, SBOMs can help companies avoid consumption of software that could harm their organization. The concept of a BOM is well-established in traditional manufacturing as part of supply chain management. A manufacturer uses a BOM to track the parts it uses to create a product. If defects are later found in a specific part, the BOM makes it easy to locate affected products. Crane crane is a tool for interacting with remote images and registries. Install crane sudo snap install go --classic ## Install go go install github.com/google/go-containerregistry/cmd/crane@latest curl -sL \"https://github.com/google/go-containerregistry/releases/latest/download/go-containerregistry_Linux_x86_64.tar.gz\" > go-containerregistry.tar.gz curl -sL https://api.github.com/repos/konveyor/crane/releases/latest | jq -r \".assets[] | select(.name | contains(\\\"amd64-linux\\\")) | .browser_download_url\" | wget -i- Imagine you have the following Dockerfile: FROM alpine:3.17.0 RUN apk add --no-cache curl ca-certificates CMD [\"curl\", \"https://www.google.com\"] I know that there's a vulnerability in alpine 3.17.0 in the OpenSSL library. How do I know that? I recently updated every OpenFaaS Pro component to use 3.17.1 to fix a specific vulnerability. Now a typical workflow for this Dockerfile would look like the below: name: build on: push: branches: [ master, main ] pull_request: branches: [ master, main ] permissions: actions: read checks: write contents: read packages: write jobs: publish: runs-on: ubuntu-latest steps: - uses: actions/checkout@master with: fetch-depth: 1 - name: Build Docker image run: docker build . --file Dockerfile.sbom --tag my-image-name:$(date +%s) # - name: Set up Docker Buildx # uses: docker/setup-buildx-action@v2 # - name: Login to Docker Registry # uses: docker/login-action@v2 # with: # username: ${{ github.repository_owner }} # password: ${{ secrets.GITHUB_TOKEN }} # registry: ghcr.io - name: Publish image uses: docker/build-push-action@v3 with: build-args: | GitCommit=${{ github.sha }} outputs: \"type=registry,push=true\" tags: | ghcr.io/alexellis/gha-sbom:${{ github.sha }} Upon each commit, an image is published to GitHub's Container Registry with the image name of: ghcr.io/np-completed/gha-sbom:SHA . To generate an SBOM, we just need to update the docker/build-push-action to use the sbom flag: - name: Local build id: local_build uses: docker/build-push-action@v3 with: sbom: true provenance: false View the SBOM: syft ghcr.io/np-completed/gha-sbom:840fc5b2f3c94099e5dc74434a6bbcc156407a6f \u2714 Loaded image \u2714 Parsed image \u2714 Cataloged packages [21 packages] NAME VERSION TYPE alpine-baselayout 3.4.0-r0 apk alpine-baselayout-data 3.4.0-r0 apk alpine-keys 2.4-r1 apk apk-tools 2.12.10-r1 apk brotli-libs 1.0.9-r9 apk busybox 1.35.0 binary busybox 1.35.0-r29 apk busybox-binsh 1.35.0-r29 apk ca-certificates 20220614-r4 apk ca-certificates-bundle 20220614-r2 apk curl 7.87.0-r1 apk libc-utils 0.7.2-r3 apk libcrypto3 3.0.7-r0 apk libcurl 7.87.0-r1 apk libssl3 3.0.7-r0 apk musl 1.2.3-r4 apk musl-utils 1.2.3-r4 apk nghttp2-libs 1.51.0-r0 apk scanelf 1.3.5-r1 apk ssl_client 1.35.0-r29 apk zlib 1.2.13-r0 apk grype ghcr.io/np-completed/gha-sbom:840fc5b2f3c94099e5dc74434a6bbcc156407a6f \u2714 Vulnerability DB [no update available] \u2714 Pulled image \u2714 Loaded image \u2714 Parsed image \u2714 Cataloged packages [21 packages] \u2714 Scanned image [2 vulnerabilities] NAME INSTALLED FIXED-IN TYPE VULNERABILITY SEVERITY libcrypto3 3.0.7-r0 3.0.7-r2 apk CVE-2022-3996 High libssl3 3.0.7-r0 3.0.7-r2 apk CVE-2022-3996 High The image: alpine:3.17.0 contains two High vulnerabilities, and from reading the notes, we can see that both have been fixed. We can resolve the issue by changing the Dockerfile to use alpine:latest instead, and re-running the build. syft ghcr.io/np-completed/gha-sbom:1a7f4c36bad2a506d7403fe4dd904a356bd8bbfa \u2714 Pulled image \u2714 Loaded image \u2714 Parsed image \u2714 Cataloged packages [21 packages] NAME VERSION TYPE alpine-baselayout 3.4.0-r0 apk alpine-baselayout-data 3.4.0-r0 apk alpine-keys 2.4-r1 apk apk-tools 2.12.10-r1 apk brotli-libs 1.0.9-r9 apk busybox 1.35.0 binary busybox 1.35.0-r29 apk busybox-binsh 1.35.0-r29 apk ca-certificates 20220614-r4 apk ca-certificates-bundle 20220614-r4 apk curl 7.87.0-r1 apk libc-utils 0.7.2-r3 apk libcrypto3 3.0.7-r2 apk libcurl 7.87.0-r1 apk libssl3 3.0.7-r2 apk musl 1.2.3-r4 apk musl-utils 1.2.3-r4 apk nghttp2-libs 1.51.0-r0 apk scanelf 1.3.5-r1 apk ssl_client 1.35.0-r29 apk zlib 1.2.13-r0 apk grype ghcr.io/np-completed/gha-sbom:1a7f4c36bad2a506d7403fe4dd904a356bd8bbfa \u2714 Vulnerability DB [no update available] \u2714 Loaded image \u2714 Parsed image \u2714 Cataloged packages [21 packages] \u2714 Scanned image [0 vulnerabilities] Install Grype and Syft Anchore provides commercial solutions for creating, managing and inspecting SBOMs, however they also have two very useful open source tools that we can try out for free. syft - a command line tool that can be used to generate an SBOM for a container image. grype - a command line tool that can be used to scan an SBOM for vulnerabilities. Grype Run the following command to install the latest version of Grype to the /usr/local/bin directory: wget -qO - https://raw.githubusercontent.com/anchore/grype/main/install.sh | sudo bash -s -- -b /usr/local/bin ## Verify install worked grype version Run the grype command and specify the container image as argument: $ grype ubuntu:latest \u2714 Vulnerability DB [updated] \u2714 Pulled image \u2714 Loaded image \u2714 Parsed image \u2714 Cataloged packages [101 packages] \u2714 Scanned image [18 vulnerabilities] NAME INSTALLED FIXED-IN TYPE VULNERABILITY SEVERITY bash 5.1-6ubuntu1 deb CVE-2022-3715 Low coreutils 8.32-4.1ubuntu1 deb CVE-2016-2781 Low gpgv 2.2.27-3ubuntu2.1 deb CVE-2022-3219 Low libc-bin 2.35-0ubuntu3.1 deb CVE-2016-20013 Negligible libc6 2.35-0ubuntu3.1 deb CVE-2016-20013 Negligible libgssapi-krb5-2 1.19.2-2 deb CVE-2022-42898 Medium libk5crypto3 1.19.2-2 deb CVE-2022-42898 Medium Uninstall grype sudo rm -rf /usr/local/bin/grype and remove vulnerabilities database rm -rf ~/.cache/grype Install Syft Run the following command to install the latest version of Syft to the /usr/local/bin directory: wget -qO - https://raw.githubusercontent.com/anchore/syft/main/install.sh | sudo bash -s -- -b /usr/local/bin ## Verify install worked syft version Application: syft Version: 0.68.1 JsonSchemaVersion: 6.2.0 BuildDate: 2023-01-25T17:46:33Z GitCommit: 4c0aef09b8d7fb78200b04416f474b90b79370de GitDescription: v0.68.1 Platform: linux/amd64 GoVersion: go1.18.10 Compiler: gc Test Syft Generate an SBOM for a container image: syft <image> The above output includes only software that is visible in the container (i.e., the squashed representation of the image). To include software from all image layers in the SBOM, regardless of its presence in the final image, provide --scope all-layers : syft <image> --scope all-layers","title":"Software Bill of Materials"},{"location":"gh-actions/sbom/#sbom-software-bill-of-material-with-docker-container-and-github-actions","text":"Generate an SBOM from within a GitHub Actions workflow. In this way, the SBOM is shipped with the container image and is made available without having to scan the image each time.","title":"SBOM Software Bill of Material with Docker Container and Github Actions"},{"location":"gh-actions/sbom/#what-is-sbom","text":"From Wikipedia: A software bill of materials (SBOM) is a list of components in a piece of software. Software vendors often create products by assembling open source and commercial software components. The SBOM describes the components in a product. It is analogous to a list of ingredients on food packaging: where you might consult a label to avoid foods that may cause an allergies, SBOMs can help companies avoid consumption of software that could harm their organization. The concept of a BOM is well-established in traditional manufacturing as part of supply chain management. A manufacturer uses a BOM to track the parts it uses to create a product. If defects are later found in a specific part, the BOM makes it easy to locate affected products.","title":"What is SBOM?"},{"location":"gh-actions/sbom/#crane","text":"crane is a tool for interacting with remote images and registries. Install crane sudo snap install go --classic ## Install go go install github.com/google/go-containerregistry/cmd/crane@latest curl -sL \"https://github.com/google/go-containerregistry/releases/latest/download/go-containerregistry_Linux_x86_64.tar.gz\" > go-containerregistry.tar.gz curl -sL https://api.github.com/repos/konveyor/crane/releases/latest | jq -r \".assets[] | select(.name | contains(\\\"amd64-linux\\\")) | .browser_download_url\" | wget -i-","title":"Crane"},{"location":"gh-actions/sbom/#imagine-you-have-the-following-dockerfile","text":"FROM alpine:3.17.0 RUN apk add --no-cache curl ca-certificates CMD [\"curl\", \"https://www.google.com\"] I know that there's a vulnerability in alpine 3.17.0 in the OpenSSL library. How do I know that? I recently updated every OpenFaaS Pro component to use 3.17.1 to fix a specific vulnerability. Now a typical workflow for this Dockerfile would look like the below: name: build on: push: branches: [ master, main ] pull_request: branches: [ master, main ] permissions: actions: read checks: write contents: read packages: write jobs: publish: runs-on: ubuntu-latest steps: - uses: actions/checkout@master with: fetch-depth: 1 - name: Build Docker image run: docker build . --file Dockerfile.sbom --tag my-image-name:$(date +%s) # - name: Set up Docker Buildx # uses: docker/setup-buildx-action@v2 # - name: Login to Docker Registry # uses: docker/login-action@v2 # with: # username: ${{ github.repository_owner }} # password: ${{ secrets.GITHUB_TOKEN }} # registry: ghcr.io - name: Publish image uses: docker/build-push-action@v3 with: build-args: | GitCommit=${{ github.sha }} outputs: \"type=registry,push=true\" tags: | ghcr.io/alexellis/gha-sbom:${{ github.sha }} Upon each commit, an image is published to GitHub's Container Registry with the image name of: ghcr.io/np-completed/gha-sbom:SHA . To generate an SBOM, we just need to update the docker/build-push-action to use the sbom flag: - name: Local build id: local_build uses: docker/build-push-action@v3 with: sbom: true provenance: false View the SBOM: syft ghcr.io/np-completed/gha-sbom:840fc5b2f3c94099e5dc74434a6bbcc156407a6f \u2714 Loaded image \u2714 Parsed image \u2714 Cataloged packages [21 packages] NAME VERSION TYPE alpine-baselayout 3.4.0-r0 apk alpine-baselayout-data 3.4.0-r0 apk alpine-keys 2.4-r1 apk apk-tools 2.12.10-r1 apk brotli-libs 1.0.9-r9 apk busybox 1.35.0 binary busybox 1.35.0-r29 apk busybox-binsh 1.35.0-r29 apk ca-certificates 20220614-r4 apk ca-certificates-bundle 20220614-r2 apk curl 7.87.0-r1 apk libc-utils 0.7.2-r3 apk libcrypto3 3.0.7-r0 apk libcurl 7.87.0-r1 apk libssl3 3.0.7-r0 apk musl 1.2.3-r4 apk musl-utils 1.2.3-r4 apk nghttp2-libs 1.51.0-r0 apk scanelf 1.3.5-r1 apk ssl_client 1.35.0-r29 apk zlib 1.2.13-r0 apk grype ghcr.io/np-completed/gha-sbom:840fc5b2f3c94099e5dc74434a6bbcc156407a6f \u2714 Vulnerability DB [no update available] \u2714 Pulled image \u2714 Loaded image \u2714 Parsed image \u2714 Cataloged packages [21 packages] \u2714 Scanned image [2 vulnerabilities] NAME INSTALLED FIXED-IN TYPE VULNERABILITY SEVERITY libcrypto3 3.0.7-r0 3.0.7-r2 apk CVE-2022-3996 High libssl3 3.0.7-r0 3.0.7-r2 apk CVE-2022-3996 High The image: alpine:3.17.0 contains two High vulnerabilities, and from reading the notes, we can see that both have been fixed. We can resolve the issue by changing the Dockerfile to use alpine:latest instead, and re-running the build. syft ghcr.io/np-completed/gha-sbom:1a7f4c36bad2a506d7403fe4dd904a356bd8bbfa \u2714 Pulled image \u2714 Loaded image \u2714 Parsed image \u2714 Cataloged packages [21 packages] NAME VERSION TYPE alpine-baselayout 3.4.0-r0 apk alpine-baselayout-data 3.4.0-r0 apk alpine-keys 2.4-r1 apk apk-tools 2.12.10-r1 apk brotli-libs 1.0.9-r9 apk busybox 1.35.0 binary busybox 1.35.0-r29 apk busybox-binsh 1.35.0-r29 apk ca-certificates 20220614-r4 apk ca-certificates-bundle 20220614-r4 apk curl 7.87.0-r1 apk libc-utils 0.7.2-r3 apk libcrypto3 3.0.7-r2 apk libcurl 7.87.0-r1 apk libssl3 3.0.7-r2 apk musl 1.2.3-r4 apk musl-utils 1.2.3-r4 apk nghttp2-libs 1.51.0-r0 apk scanelf 1.3.5-r1 apk ssl_client 1.35.0-r29 apk zlib 1.2.13-r0 apk grype ghcr.io/np-completed/gha-sbom:1a7f4c36bad2a506d7403fe4dd904a356bd8bbfa \u2714 Vulnerability DB [no update available] \u2714 Loaded image \u2714 Parsed image \u2714 Cataloged packages [21 packages] \u2714 Scanned image [0 vulnerabilities]","title":"Imagine you have the following Dockerfile:"},{"location":"gh-actions/sbom/#install-grype-and-syft","text":"Anchore provides commercial solutions for creating, managing and inspecting SBOMs, however they also have two very useful open source tools that we can try out for free. syft - a command line tool that can be used to generate an SBOM for a container image. grype - a command line tool that can be used to scan an SBOM for vulnerabilities.","title":"Install Grype and Syft"},{"location":"gh-actions/sbom/#grype","text":"Run the following command to install the latest version of Grype to the /usr/local/bin directory: wget -qO - https://raw.githubusercontent.com/anchore/grype/main/install.sh | sudo bash -s -- -b /usr/local/bin ## Verify install worked grype version Run the grype command and specify the container image as argument: $ grype ubuntu:latest \u2714 Vulnerability DB [updated] \u2714 Pulled image \u2714 Loaded image \u2714 Parsed image \u2714 Cataloged packages [101 packages] \u2714 Scanned image [18 vulnerabilities] NAME INSTALLED FIXED-IN TYPE VULNERABILITY SEVERITY bash 5.1-6ubuntu1 deb CVE-2022-3715 Low coreutils 8.32-4.1ubuntu1 deb CVE-2016-2781 Low gpgv 2.2.27-3ubuntu2.1 deb CVE-2022-3219 Low libc-bin 2.35-0ubuntu3.1 deb CVE-2016-20013 Negligible libc6 2.35-0ubuntu3.1 deb CVE-2016-20013 Negligible libgssapi-krb5-2 1.19.2-2 deb CVE-2022-42898 Medium libk5crypto3 1.19.2-2 deb CVE-2022-42898 Medium","title":"Grype"},{"location":"gh-actions/sbom/#uninstall-grype","text":"sudo rm -rf /usr/local/bin/grype and remove vulnerabilities database rm -rf ~/.cache/grype","title":"Uninstall grype"},{"location":"gh-actions/sbom/#install-syft","text":"Run the following command to install the latest version of Syft to the /usr/local/bin directory: wget -qO - https://raw.githubusercontent.com/anchore/syft/main/install.sh | sudo bash -s -- -b /usr/local/bin ## Verify install worked syft version Application: syft Version: 0.68.1 JsonSchemaVersion: 6.2.0 BuildDate: 2023-01-25T17:46:33Z GitCommit: 4c0aef09b8d7fb78200b04416f474b90b79370de GitDescription: v0.68.1 Platform: linux/amd64 GoVersion: go1.18.10 Compiler: gc","title":"Install Syft"},{"location":"gh-actions/sbom/#test-syft","text":"Generate an SBOM for a container image: syft <image> The above output includes only software that is visible in the container (i.e., the squashed representation of the image). To include software from all image layers in the SBOM, regardless of its presence in the final image, provide --scope all-layers : syft <image> --scope all-layers","title":"Test Syft"},{"location":"gh-actions/share_docker_job/","text":"Share Artifacts Sharing a Docker image between two jobs, here is how I did it: jobs: docker-build: name: Docker build runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v2 - name: Build Docker image run: | docker build -t foo/bar:$GITHUB_SHA mkdir -p path/to/artifacts docker save foo/bar:$GITHUB_SHA > path/to/artifacts/docker-image.tar - name: Temporarily save Docker image uses: actions/upload-artifact@v2 with: name: docker-artifact path: path/to/artifacts retention-days: 1 docker-deploy: name: Deploy to Docker Hub runs-on: ubuntu-latest needs: docker-build steps: - name: Checkout uses: actions/checkout@v2 - name: Retrieve saved Docker image uses: actions/download-artifact@v2 with: name: docker-artifact path: path/to/artifacts - name: Docker load run: | cd path/to/artifacts docker load < docker-image.tar # docker_build_push.sh","title":"Share Artifacts"},{"location":"gh-actions/share_docker_job/#share-artifacts","text":"Sharing a Docker image between two jobs, here is how I did it: jobs: docker-build: name: Docker build runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v2 - name: Build Docker image run: | docker build -t foo/bar:$GITHUB_SHA mkdir -p path/to/artifacts docker save foo/bar:$GITHUB_SHA > path/to/artifacts/docker-image.tar - name: Temporarily save Docker image uses: actions/upload-artifact@v2 with: name: docker-artifact path: path/to/artifacts retention-days: 1 docker-deploy: name: Deploy to Docker Hub runs-on: ubuntu-latest needs: docker-build steps: - name: Checkout uses: actions/checkout@v2 - name: Retrieve saved Docker image uses: actions/download-artifact@v2 with: name: docker-artifact path: path/to/artifacts - name: Docker load run: | cd path/to/artifacts docker load < docker-image.tar # docker_build_push.sh","title":"Share Artifacts"},{"location":"gh-actions/url_test_in_cloudfront/","text":"Github Actions Check URL Links Check for broken url links in published markdown html. The json and yaml files are required. links.yml is located in .github/workflows/links.yml name: Check Markdown Links on: workflow_call jobs: markdown-link-check: runs-on: ubuntu-latest env: ZENML_DEBUG: 1 ZENML_ANALYTICS_OPT_IN: false steps: - uses: actions/checkout@master - uses: gaurav-nelson/github-action-markdown-link-check@v1 with: use-quiet-mode: 'yes' use-verbose-mode: 'no' config-file: .github/workflows/markdown_check_config.json continue-on-error: true { \"ignorePatterns\": [ { \"pattern\": \"^http://0.0.0.0\" }, { \"pattern\": \"^http://127.0.0.1\" }, { \"pattern\": \"^http://localhost\" } ] } Broken links report to issue name: Broken Links Report to GH Issues on: push: branches: - main repository_dispatch: workflow_dispatch: # schedule: # - cron: \"00 18 * * *\" concurrency: # New commit on branch cancels running workflows of the same branch group: ${{ github.workflow }}-${{ github.ref }} cancel-in-progress: true jobs: links: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2.3.4 - name: Link Checker uses: lycheeverse/lychee-action@v1.5.1 with: # TODO TOML is causing kernel panick and fails to render report --config .github/lychee.toml # TODO --exclude-path .lychee.excludes NOT WORKING AS EXPECTED args: > --exclude-mail --no-progress --timeout 45 './**/*.md' './**/*.html' # --cache --max-cache-age 1d -- **/*.md *.md **/*.html # --verbose # --accept=200,403,429 # --exclude-mail **/*.html **/*.md **/*.txt **/*.json --exclude-file .lychee.excludes # --exclude-all-private --insecure # --max-retries 4 # -- **/*.md *.md '**/*.md' # --cache --max-cache-age 1d '**/*.md' README.md patterns/ output: out.md jobSummary: true env: GITHUB_TOKEN: ${{ secrets.actions }} - name: Create Issue From File # if: github.event_name != 'pull_request' # if: steps.lychee.outputs.exit_code != 0 uses: peter-evans/create-issue-from-file@v4 with: title: Link Checker Report content-filepath: out.md # token: ${{ secrets.actions }} #content-filepath: ./link-checker/out.md labels: | report links automated issue issue-number: 15 Use file to ignore URLs. Add .lycheeingnore to project root directory: file://* http://127.0.0.1:8000/ ON Event ## Manual Trigger from Github UI on: workflow_dispatch: ## Push Trigger on: push: paths: - 'docs/**' - 'mkdocs.yml' - '.github/workflows/main.yml' - 'pyproject.toml' branches: - main pull_request: # The branches below must be a subset of the branches above ## types: [opened, synchronize, reopened] branches: [ \"main\" ] concurrency: # New commit on branch cancels running workflows of the same branch group: ${{ github.workflow }}-${{ github.ref }} cancel-in-progress: true ## Workflow Call on: workflow_call Reusable workflows TODO Workflow for Github Actions TODO Using Github Actions to push to AWS ECR with Credentials https://aws.plainenglish.io/build-a-docker-image-and-publish-it-to-aws-ecr-using-github-actions-f20accd774c3 Using Github Actions OpenID Connect to push to AWS ECR without Credentials https://blog.tedivm.com/guides/2021/10/github-actions-push-to-aws-ecr-without-credentials-oidc/ Pushing the Container WIth all that out of the way here\u2019s full action to build and deploy an image to ECR. In this we\u2019re chaining together a variety of published actions from other vendors- actions/checkout to actually pull the repository. docker/setup-qemu-action to install an emulation layer to build multiplatform images. docker/setup-buildx-action to use the docker buildx system, again for multiplatform images. aws-actions/configure-aws-credentials to log into AWS. aws-actions/amazon-ecr-login to log into AWS ECR. docker/metadata-action to create a bunch of tags for our docker container. docker/build-push-action to push the container.","title":"Test URLs in Cloudfront"},{"location":"gh-actions/url_test_in_cloudfront/#github-actions","text":"","title":"Github Actions"},{"location":"gh-actions/url_test_in_cloudfront/#check-url-links","text":"Check for broken url links in published markdown html. The json and yaml files are required. links.yml is located in .github/workflows/links.yml name: Check Markdown Links on: workflow_call jobs: markdown-link-check: runs-on: ubuntu-latest env: ZENML_DEBUG: 1 ZENML_ANALYTICS_OPT_IN: false steps: - uses: actions/checkout@master - uses: gaurav-nelson/github-action-markdown-link-check@v1 with: use-quiet-mode: 'yes' use-verbose-mode: 'no' config-file: .github/workflows/markdown_check_config.json continue-on-error: true { \"ignorePatterns\": [ { \"pattern\": \"^http://0.0.0.0\" }, { \"pattern\": \"^http://127.0.0.1\" }, { \"pattern\": \"^http://localhost\" } ] }","title":"Check URL Links"},{"location":"gh-actions/url_test_in_cloudfront/#broken-links-report-to-issue","text":"name: Broken Links Report to GH Issues on: push: branches: - main repository_dispatch: workflow_dispatch: # schedule: # - cron: \"00 18 * * *\" concurrency: # New commit on branch cancels running workflows of the same branch group: ${{ github.workflow }}-${{ github.ref }} cancel-in-progress: true jobs: links: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2.3.4 - name: Link Checker uses: lycheeverse/lychee-action@v1.5.1 with: # TODO TOML is causing kernel panick and fails to render report --config .github/lychee.toml # TODO --exclude-path .lychee.excludes NOT WORKING AS EXPECTED args: > --exclude-mail --no-progress --timeout 45 './**/*.md' './**/*.html' # --cache --max-cache-age 1d -- **/*.md *.md **/*.html # --verbose # --accept=200,403,429 # --exclude-mail **/*.html **/*.md **/*.txt **/*.json --exclude-file .lychee.excludes # --exclude-all-private --insecure # --max-retries 4 # -- **/*.md *.md '**/*.md' # --cache --max-cache-age 1d '**/*.md' README.md patterns/ output: out.md jobSummary: true env: GITHUB_TOKEN: ${{ secrets.actions }} - name: Create Issue From File # if: github.event_name != 'pull_request' # if: steps.lychee.outputs.exit_code != 0 uses: peter-evans/create-issue-from-file@v4 with: title: Link Checker Report content-filepath: out.md # token: ${{ secrets.actions }} #content-filepath: ./link-checker/out.md labels: | report links automated issue issue-number: 15 Use file to ignore URLs. Add .lycheeingnore to project root directory: file://* http://127.0.0.1:8000/","title":"Broken links report to issue"},{"location":"gh-actions/url_test_in_cloudfront/#on-event","text":"## Manual Trigger from Github UI on: workflow_dispatch: ## Push Trigger on: push: paths: - 'docs/**' - 'mkdocs.yml' - '.github/workflows/main.yml' - 'pyproject.toml' branches: - main pull_request: # The branches below must be a subset of the branches above ## types: [opened, synchronize, reopened] branches: [ \"main\" ] concurrency: # New commit on branch cancels running workflows of the same branch group: ${{ github.workflow }}-${{ github.ref }} cancel-in-progress: true ## Workflow Call on: workflow_call","title":"ON Event"},{"location":"gh-actions/url_test_in_cloudfront/#reusable-workflows","text":"TODO","title":"Reusable workflows"},{"location":"gh-actions/url_test_in_cloudfront/#workflow-for-github-actions","text":"TODO","title":"Workflow for Github Actions"},{"location":"gh-actions/url_test_in_cloudfront/#using-github-actions-to-push-to-aws-ecr-with-credentials","text":"https://aws.plainenglish.io/build-a-docker-image-and-publish-it-to-aws-ecr-using-github-actions-f20accd774c3","title":"Using Github Actions to push to AWS ECR with Credentials"},{"location":"gh-actions/url_test_in_cloudfront/#using-github-actions-openid-connect-to-push-to-aws-ecr-without-credentials","text":"https://blog.tedivm.com/guides/2021/10/github-actions-push-to-aws-ecr-without-credentials-oidc/","title":"Using Github Actions OpenID Connect to push to AWS ECR without Credentials"},{"location":"gh-actions/url_test_in_cloudfront/#pushing-the-container","text":"WIth all that out of the way here\u2019s full action to build and deploy an image to ECR. In this we\u2019re chaining together a variety of published actions from other vendors- actions/checkout to actually pull the repository. docker/setup-qemu-action to install an emulation layer to build multiplatform images. docker/setup-buildx-action to use the docker buildx system, again for multiplatform images. aws-actions/configure-aws-credentials to log into AWS. aws-actions/amazon-ecr-login to log into AWS ECR. docker/metadata-action to create a bunch of tags for our docker container. docker/build-push-action to push the container.","title":"Pushing the Container"},{"location":"gh-actions/checks/actions/","text":"Actions Connection Check What is this check for? Make sure the runner has access to actions service for GitHub.com or GitHub Enterprise Server For GitHub.com The runner needs to access https://api.github.com for downloading actions. The runner needs to access https://vstoken.actions.githubusercontent.com/_apis/.../ for requesting an access token. The runner needs to access https://pipelines.actions.githubusercontent.com/_apis/.../ for receiving workflow jobs. These can by tested by running the following curl commands from your self-hosted runner machine: ``` curl -v https://api.github.com/api/v3/zen curl -v https://vstoken.actions.githubusercontent.com/_apis/health curl -v https://pipelines.actions.githubusercontent.com/_apis/health ``` For GitHub Enterprise Server The runner needs to access https://[hostname]/api/v3 for downloading actions. The runner needs to access https://[hostname]/_services/vstoken/_apis/.../ for requesting an access token. The runner needs to access https://[hostname]/_services/pipelines/_apis/.../ for receiving workflow jobs. These can by tested by running the following curl commands from your self-hosted runner machine, replacing [hostname] with the hostname of your appliance, for instance github.example.com : ``` curl -v https://[hostname]/api/v3/zen curl -v https://[hostname]/_services/vstoken/_apis/health curl -v https://[hostname]/_services/pipelines/_apis/health ``` A common cause of this these connectivity issues is if your to GitHub Enterprise Server appliance is using [the self-signed certificate that is enabled the first time](https://docs.github.com/en/enterprise-server/admin/configuration/configuring-network-settings/configuring-tls) your appliance is started. As self-signed certificates are not trusted by web browsers and Git clients, these clients (including the GitHub Actions runner) will report certificate warnings. We recommend [upload a certificate signed by a trusted authority](https://docs.github.com/en/enterprise-server/admin/configuration/configuring-network-settings/configuring-tls) to GitHub Enterprise Server, or enabling the built-in ][Let's Encrypt support](https://docs.github.com/en/enterprise-server/admin/configuration/configuring-network-settings/configuring-tls). What is checked? DNS lookup for api.github.com or myGHES.com using dotnet Ping api.github.com or myGHES.com using dotnet Make HTTP GET to https://api.github.com or https://myGHES.com/api/v3 using dotnet, check response headers contains X-GitHub-Request-Id DNS lookup for vstoken.actions.githubusercontent.com using dotnet Ping vstoken.actions.githubusercontent.com using dotnet Make HTTP GET to https://vstoken.actions.githubusercontent.com/_apis/health or https://myGHES.com/_services/vstoken/_apis/health using dotnet, check response headers contains x-vss-e2eid DNS lookup for pipelines.actions.githubusercontent.com using dotnet Ping pipelines.actions.githubusercontent.com using dotnet Make HTTP GET to https://pipelines.actions.githubusercontent.com/_apis/health or https://myGHES.com/_services/pipelines/_apis/health using dotnet, check response headers contains x-vss-e2eid Make HTTP POST to https://pipelines.actions.githubusercontent.com/_apis/health or https://myGHES.com/_services/pipelines/_apis/health using dotnet, check response headers contains x-vss-e2eid How to fix the issue? 1. Check the common network issue Please check the network doc 2. SSL certificate related issue If you are seeing System.Net.Http.HttpRequestException: The SSL connection could not be established, see inner exception. in the log, it means the runner can't connect to Actions service due to SSL handshake failure. Please check the SSL cert doc Still not working? Contact GitHub Support if you have further questuons, or log an issue at https://github.com/actions/runner if you think it's a runner issue.","title":"Actions"},{"location":"gh-actions/checks/actions/#actions-connection-check","text":"","title":"Actions Connection Check"},{"location":"gh-actions/checks/actions/#what-is-this-check-for","text":"Make sure the runner has access to actions service for GitHub.com or GitHub Enterprise Server For GitHub.com The runner needs to access https://api.github.com for downloading actions. The runner needs to access https://vstoken.actions.githubusercontent.com/_apis/.../ for requesting an access token. The runner needs to access https://pipelines.actions.githubusercontent.com/_apis/.../ for receiving workflow jobs. These can by tested by running the following curl commands from your self-hosted runner machine: ``` curl -v https://api.github.com/api/v3/zen curl -v https://vstoken.actions.githubusercontent.com/_apis/health curl -v https://pipelines.actions.githubusercontent.com/_apis/health ``` For GitHub Enterprise Server The runner needs to access https://[hostname]/api/v3 for downloading actions. The runner needs to access https://[hostname]/_services/vstoken/_apis/.../ for requesting an access token. The runner needs to access https://[hostname]/_services/pipelines/_apis/.../ for receiving workflow jobs. These can by tested by running the following curl commands from your self-hosted runner machine, replacing [hostname] with the hostname of your appliance, for instance github.example.com : ``` curl -v https://[hostname]/api/v3/zen curl -v https://[hostname]/_services/vstoken/_apis/health curl -v https://[hostname]/_services/pipelines/_apis/health ``` A common cause of this these connectivity issues is if your to GitHub Enterprise Server appliance is using [the self-signed certificate that is enabled the first time](https://docs.github.com/en/enterprise-server/admin/configuration/configuring-network-settings/configuring-tls) your appliance is started. As self-signed certificates are not trusted by web browsers and Git clients, these clients (including the GitHub Actions runner) will report certificate warnings. We recommend [upload a certificate signed by a trusted authority](https://docs.github.com/en/enterprise-server/admin/configuration/configuring-network-settings/configuring-tls) to GitHub Enterprise Server, or enabling the built-in ][Let's Encrypt support](https://docs.github.com/en/enterprise-server/admin/configuration/configuring-network-settings/configuring-tls).","title":"What is this check for?"},{"location":"gh-actions/checks/actions/#what-is-checked","text":"DNS lookup for api.github.com or myGHES.com using dotnet Ping api.github.com or myGHES.com using dotnet Make HTTP GET to https://api.github.com or https://myGHES.com/api/v3 using dotnet, check response headers contains X-GitHub-Request-Id DNS lookup for vstoken.actions.githubusercontent.com using dotnet Ping vstoken.actions.githubusercontent.com using dotnet Make HTTP GET to https://vstoken.actions.githubusercontent.com/_apis/health or https://myGHES.com/_services/vstoken/_apis/health using dotnet, check response headers contains x-vss-e2eid DNS lookup for pipelines.actions.githubusercontent.com using dotnet Ping pipelines.actions.githubusercontent.com using dotnet Make HTTP GET to https://pipelines.actions.githubusercontent.com/_apis/health or https://myGHES.com/_services/pipelines/_apis/health using dotnet, check response headers contains x-vss-e2eid Make HTTP POST to https://pipelines.actions.githubusercontent.com/_apis/health or https://myGHES.com/_services/pipelines/_apis/health using dotnet, check response headers contains x-vss-e2eid","title":"What is checked?"},{"location":"gh-actions/checks/actions/#how-to-fix-the-issue","text":"","title":"How to fix the issue?"},{"location":"gh-actions/checks/actions/#1-check-the-common-network-issue","text":"Please check the network doc","title":"1. Check the common network issue"},{"location":"gh-actions/checks/actions/#2-ssl-certificate-related-issue","text":"If you are seeing System.Net.Http.HttpRequestException: The SSL connection could not be established, see inner exception. in the log, it means the runner can't connect to Actions service due to SSL handshake failure. Please check the SSL cert doc","title":"2. SSL certificate related issue"},{"location":"gh-actions/checks/actions/#still-not-working","text":"Contact GitHub Support if you have further questuons, or log an issue at https://github.com/actions/runner if you think it's a runner issue.","title":"Still not working?"},{"location":"gh-actions/checks/auth/","text":"Runner Authentication and Authorization Goals Support runner installs in untrusted domains. The account that configures or runs the runner process is not relevant for accessing GitHub resources. Accessing GitHub resources is done with a per-job token which expires when job completes. The token is granted to trusted parts of the system including the runner, actions and script steps specified by the workflow author as trusted. All OAuth tokens that come from the Token Service that the runner uses to access Actions Service resources are the same. It's just the scope and expiration of the token that may vary. Configuration Configuring a self-hosted runner is covered here in the documentation . Configuration is done with the user being authenticated via a time-limited, GitHub runner registration token. Your credentials are never used for registering the runner with the service. During configuration, an RSA public/private key pair is created, the private key is stored in file on disk. On Windows, the content is protected with DPAPI (machine level encrypted - runner only valid on that machine) and on Linux/OSX with chmod permissions. Using your credentials, the runner is registered with the service by sending the public key to the service which adds that runner to the pool and stores the public key, the Token Service will generate a clientId associated with the public key. Start and Listen After configuring the runner, the runner can be started interactively ( ./run.cmd or ./run.sh ) or as a service. On start, the runner listener process loads the RSA private key (on Windows decrypting with machine key DPAPI), and asks the Token Service for an OAuth token which is signed with the RSA private key. The server then responds with an OAuth token that grants permission to access the message queue (HTTP long poll), allowing the runner to acquire the messages it will eventually run. Run a workflow When a workflow is run, its labels are evaluated, it is matched to a runner and a message is placed in a queue of messages for that runner. The runner then starts listening for jobs via the message queue HTTP long poll. The message is encrypted with the runner's public key, stored during runner configuration. A workflow is queued as a result of a triggered event . Workflows can be scheduled to run at specific UTC times using POSIX cron syntax. An OAuth token is generated, granting limited access to the host in Actions Service associated with the github.com repository/organization. The lifetime of the OAuth token is the lifetime of the run or at most the job timeout (default: 6 hours) , plus 10 additional minutes. Accessing GitHub resources The job message sent to the runner contains the OAuth token to talk back to the Actions Service. The runner listener parent process will spawn a runner worker process for that job and send it the job message over IPC. The token is never persisted. Each action is run as a unique subprocess. The encrypted access token will be provided as an environment variable in each action subprocess. The token is registered with the runner as a secret and scrubbed from the logs as they are written. Authentication in a workflow run to github.com can be accomplished by using the GITHUB_TOKEN ) secret. This token expires after 60 minutes. Please note that this token is different from the OAuth token that the runner uses to talk to the Actions Service. Hosted runner authentication Hosted runner authentication differs from self-hosted authentication in that runners do not undergo a registration process, but instead, the hosted runners get the OAuth token directly by reading the .credentials file. The scope of this particular token is limited for a given workflow job execution, and the token is revoked as soon as the job is finished.","title":"Auth"},{"location":"gh-actions/checks/auth/#runner-authentication-and-authorization","text":"","title":"Runner Authentication and Authorization"},{"location":"gh-actions/checks/auth/#goals","text":"Support runner installs in untrusted domains. The account that configures or runs the runner process is not relevant for accessing GitHub resources. Accessing GitHub resources is done with a per-job token which expires when job completes. The token is granted to trusted parts of the system including the runner, actions and script steps specified by the workflow author as trusted. All OAuth tokens that come from the Token Service that the runner uses to access Actions Service resources are the same. It's just the scope and expiration of the token that may vary.","title":"Goals"},{"location":"gh-actions/checks/auth/#configuration","text":"Configuring a self-hosted runner is covered here in the documentation . Configuration is done with the user being authenticated via a time-limited, GitHub runner registration token. Your credentials are never used for registering the runner with the service. During configuration, an RSA public/private key pair is created, the private key is stored in file on disk. On Windows, the content is protected with DPAPI (machine level encrypted - runner only valid on that machine) and on Linux/OSX with chmod permissions. Using your credentials, the runner is registered with the service by sending the public key to the service which adds that runner to the pool and stores the public key, the Token Service will generate a clientId associated with the public key.","title":"Configuration"},{"location":"gh-actions/checks/auth/#start-and-listen","text":"After configuring the runner, the runner can be started interactively ( ./run.cmd or ./run.sh ) or as a service. On start, the runner listener process loads the RSA private key (on Windows decrypting with machine key DPAPI), and asks the Token Service for an OAuth token which is signed with the RSA private key. The server then responds with an OAuth token that grants permission to access the message queue (HTTP long poll), allowing the runner to acquire the messages it will eventually run.","title":"Start and Listen"},{"location":"gh-actions/checks/auth/#run-a-workflow","text":"When a workflow is run, its labels are evaluated, it is matched to a runner and a message is placed in a queue of messages for that runner. The runner then starts listening for jobs via the message queue HTTP long poll. The message is encrypted with the runner's public key, stored during runner configuration. A workflow is queued as a result of a triggered event . Workflows can be scheduled to run at specific UTC times using POSIX cron syntax. An OAuth token is generated, granting limited access to the host in Actions Service associated with the github.com repository/organization. The lifetime of the OAuth token is the lifetime of the run or at most the job timeout (default: 6 hours) , plus 10 additional minutes.","title":"Run a workflow"},{"location":"gh-actions/checks/auth/#accessing-github-resources","text":"The job message sent to the runner contains the OAuth token to talk back to the Actions Service. The runner listener parent process will spawn a runner worker process for that job and send it the job message over IPC. The token is never persisted. Each action is run as a unique subprocess. The encrypted access token will be provided as an environment variable in each action subprocess. The token is registered with the runner as a secret and scrubbed from the logs as they are written. Authentication in a workflow run to github.com can be accomplished by using the GITHUB_TOKEN ) secret. This token expires after 60 minutes. Please note that this token is different from the OAuth token that the runner uses to talk to the Actions Service.","title":"Accessing GitHub resources"},{"location":"gh-actions/checks/auth/#hosted-runner-authentication","text":"Hosted runner authentication differs from self-hosted authentication in that runners do not undergo a registration process, but instead, the hosted runners get the OAuth token directly by reading the .credentials file. The scope of this particular token is limited for a given workflow job execution, and the token is revoked as soon as the job is finished.","title":"Hosted runner authentication"},{"location":"gh-actions/checks/git/","text":"Git Connection Check What is this check for? Make sure git can access GitHub.com or your GitHub Enterprise Server. What is checked? The test is done by executing # For GitHub.com git ls-remote --exit-code https://github.com/actions/checkout HEAD # For GitHub Enterprise Server git ls-remote --exit-code https://ghes.me/actions/checkout HEAD The test also set environment variable GIT_TRACE=1 and GIT_CURL_VERBOSE=1 before running git ls-remote , this will make git to produce debug log for better debug any potential issues. How to fix the issue? 1. Check global and system git config If you are having issues connecting to the server, check your global and system git config for any unexpected authentication headers. You might be seeing an error like: fatal: unable to access 'https://github.com/actions/checkout/': The requested URL returned error: 400 The following commands can be used to check for unexpected authentication headers: $ git config --global --list | grep extraheader http.extraheader=AUTHORIZATION: unexpected_auth_header $ git config --system --list | grep extraheader The following command can be used to remove the above value: git config --global --unset http.extraheader 2. Check the common network issue Please check the network doc 3. SSL certificate related issue If you are seeing SSL Certificate problem: in the log, it means the git can't connect to the GitHub server due to SSL handshake failure. Please check the SSL cert doc Still not working? Contact GitHub customer service or log an issue at https://github.com/actions/runner if you think it's a runner issue.","title":"Git"},{"location":"gh-actions/checks/git/#git-connection-check","text":"","title":"Git Connection Check"},{"location":"gh-actions/checks/git/#what-is-this-check-for","text":"Make sure git can access GitHub.com or your GitHub Enterprise Server.","title":"What is this check for?"},{"location":"gh-actions/checks/git/#what-is-checked","text":"The test is done by executing # For GitHub.com git ls-remote --exit-code https://github.com/actions/checkout HEAD # For GitHub Enterprise Server git ls-remote --exit-code https://ghes.me/actions/checkout HEAD The test also set environment variable GIT_TRACE=1 and GIT_CURL_VERBOSE=1 before running git ls-remote , this will make git to produce debug log for better debug any potential issues.","title":"What is checked?"},{"location":"gh-actions/checks/git/#how-to-fix-the-issue","text":"","title":"How to fix the issue?"},{"location":"gh-actions/checks/git/#1-check-global-and-system-git-config","text":"If you are having issues connecting to the server, check your global and system git config for any unexpected authentication headers. You might be seeing an error like: fatal: unable to access 'https://github.com/actions/checkout/': The requested URL returned error: 400 The following commands can be used to check for unexpected authentication headers: $ git config --global --list | grep extraheader http.extraheader=AUTHORIZATION: unexpected_auth_header $ git config --system --list | grep extraheader The following command can be used to remove the above value: git config --global --unset http.extraheader","title":"1. Check global and system git config"},{"location":"gh-actions/checks/git/#2-check-the-common-network-issue","text":"Please check the network doc","title":"2. Check the common network issue"},{"location":"gh-actions/checks/git/#3-ssl-certificate-related-issue","text":"If you are seeing SSL Certificate problem: in the log, it means the git can't connect to the GitHub server due to SSL handshake failure. Please check the SSL cert doc","title":"3. SSL certificate related issue"},{"location":"gh-actions/checks/git/#still-not-working","text":"Contact GitHub customer service or log an issue at https://github.com/actions/runner if you think it's a runner issue.","title":"Still not working?"},{"location":"gh-actions/checks/internet/","text":"Internet Connection Check What is this check for? Make sure the runner has access to https://api.github.com The runner needs to access https://api.github.com to download any actions from the marketplace. Even the runner is configured to GitHub Enterprise Server, the runner can still download actions from GitHub.com with GitHub Connect What is checked? DNS lookup for api.github.com using dotnet Ping api.github.com using dotnet Make HTTP GET to https://api.github.com using dotnet, check response headers contains X-GitHub-Request-Id How to fix the issue? 1. Check the common network issue Please check the network doc Still not working? Contact GitHub customer service or log an issue at https://github.com/actions/runner if you think it's a runner issue.","title":"Internet"},{"location":"gh-actions/checks/internet/#internet-connection-check","text":"","title":"Internet Connection Check"},{"location":"gh-actions/checks/internet/#what-is-this-check-for","text":"Make sure the runner has access to https://api.github.com The runner needs to access https://api.github.com to download any actions from the marketplace. Even the runner is configured to GitHub Enterprise Server, the runner can still download actions from GitHub.com with GitHub Connect","title":"What is this check for?"},{"location":"gh-actions/checks/internet/#what-is-checked","text":"DNS lookup for api.github.com using dotnet Ping api.github.com using dotnet Make HTTP GET to https://api.github.com using dotnet, check response headers contains X-GitHub-Request-Id","title":"What is checked?"},{"location":"gh-actions/checks/internet/#how-to-fix-the-issue","text":"","title":"How to fix the issue?"},{"location":"gh-actions/checks/internet/#1-check-the-common-network-issue","text":"Please check the network doc","title":"1. Check the common network issue"},{"location":"gh-actions/checks/internet/#still-not-working","text":"Contact GitHub customer service or log an issue at https://github.com/actions/runner if you think it's a runner issue.","title":"Still not working?"},{"location":"gh-actions/checks/network/","text":"Common Network Related Issues Common things that can cause the runner to not working properly A bug in the runner or the dotnet framework that causes the actions runner to be unable to make Http requests in a certain network environment. A Proxy or Firewall may block certain HTTP method, such as blocking all POST and PUT calls which the runner will use to upload logs. A Proxy or Firewall may only allows requests with certain user-agent to pass through and the actions runner user-agent is not in the allow list. A Proxy try to decrypt and exam HTTPS traffic for security purpose but cause the actions-runner to fail to finish SSL handshake due to the lack of trusting proxy's CA. The SSL handshake may fail if the client and server do not support the same TLS version, or the same cipher suites. A Proxy may try to modify the HTTPS request (like add or change some http headers) and causes the request become incompatible with the Actions Service (ASP.NetCore), Ex: Nginx Firewall rules that block action runner from accessing certain hosts, ex: *.github.com , *.actions.githubusercontent.com , etc Identify and solve these problems The key is to figure out where is the problem, the network environment, or the actions runner? Use a 3rd party tool to make the same requests as the runner did would be a good start point. Use nslookup to check DNS Use ping to check Ping Use traceroute , tracepath , or tracert to check the network route between the runner and the Actions service Use curl -v to check the network stack, good for verifying default certificate/proxy settings. Use Invoke-WebRequest from pwsh ( PowerShell Core ) to check the dotnet network stack, good for verifying bugs in the dotnet framework. If the 3rd party tool is also experiencing the same error as the runner does, then you might want to contact your network administrator for help. Otherwise, contact GitHub customer support or log an issue at https://github.com/actions/runner Troubleshooting: Why can't I configure a runner? If you are having trouble connecting, try these steps: Validate you can reach our endpoints from your web browser. If not, double check your local network connection For hosted Github: https://api.github.com/ https://vstoken.actions.githubusercontent.com/_apis/health https://pipelines.actions.githubusercontent.com/_apis/health For GHES/GHAE https://myGHES.com/_services/vstoken/_apis/health https://myGHES.com/_services/pipelines/_apis/health https://myGHES.com/api/v3 Validate you can reach those endpoints in powershell core The runner runs on .net core, lets validate the local settings for that stack Open up pwsh Run the command using the urls above Invoke-WebRequest {url} If not, get a packet trace using a tool like wireshark and start looking at the TLS handshake. If you see a Client Hello followed by a Server RST: You may need to configure your TLS settings to use the correct version You should support TLS version 1.2 or later You may need to configure your TLS settings to have up to date cipher suites, this may be solved by system updates and patches. Most notably, on windows server 2012 make sure the tls cipher suite update is installed Your firewall, proxy or network configuration may be blocking the connection You will want to reach out to whoever is in charge of your network with these pcap files to further troubleshoot If you see a failure later in the handshake: Try the fix in the SSLCert Fix","title":"Network"},{"location":"gh-actions/checks/network/#common-network-related-issues","text":"","title":"Common Network Related Issues"},{"location":"gh-actions/checks/network/#common-things-that-can-cause-the-runner-to-not-working-properly","text":"A bug in the runner or the dotnet framework that causes the actions runner to be unable to make Http requests in a certain network environment. A Proxy or Firewall may block certain HTTP method, such as blocking all POST and PUT calls which the runner will use to upload logs. A Proxy or Firewall may only allows requests with certain user-agent to pass through and the actions runner user-agent is not in the allow list. A Proxy try to decrypt and exam HTTPS traffic for security purpose but cause the actions-runner to fail to finish SSL handshake due to the lack of trusting proxy's CA. The SSL handshake may fail if the client and server do not support the same TLS version, or the same cipher suites. A Proxy may try to modify the HTTPS request (like add or change some http headers) and causes the request become incompatible with the Actions Service (ASP.NetCore), Ex: Nginx Firewall rules that block action runner from accessing certain hosts, ex: *.github.com , *.actions.githubusercontent.com , etc","title":"Common things that can cause the runner to not working properly"},{"location":"gh-actions/checks/network/#identify-and-solve-these-problems","text":"The key is to figure out where is the problem, the network environment, or the actions runner? Use a 3rd party tool to make the same requests as the runner did would be a good start point. Use nslookup to check DNS Use ping to check Ping Use traceroute , tracepath , or tracert to check the network route between the runner and the Actions service Use curl -v to check the network stack, good for verifying default certificate/proxy settings. Use Invoke-WebRequest from pwsh ( PowerShell Core ) to check the dotnet network stack, good for verifying bugs in the dotnet framework. If the 3rd party tool is also experiencing the same error as the runner does, then you might want to contact your network administrator for help. Otherwise, contact GitHub customer support or log an issue at https://github.com/actions/runner","title":"Identify and solve these problems"},{"location":"gh-actions/checks/network/#troubleshooting-why-cant-i-configure-a-runner","text":"If you are having trouble connecting, try these steps: Validate you can reach our endpoints from your web browser. If not, double check your local network connection For hosted Github: https://api.github.com/ https://vstoken.actions.githubusercontent.com/_apis/health https://pipelines.actions.githubusercontent.com/_apis/health For GHES/GHAE https://myGHES.com/_services/vstoken/_apis/health https://myGHES.com/_services/pipelines/_apis/health https://myGHES.com/api/v3 Validate you can reach those endpoints in powershell core The runner runs on .net core, lets validate the local settings for that stack Open up pwsh Run the command using the urls above Invoke-WebRequest {url} If not, get a packet trace using a tool like wireshark and start looking at the TLS handshake. If you see a Client Hello followed by a Server RST: You may need to configure your TLS settings to use the correct version You should support TLS version 1.2 or later You may need to configure your TLS settings to have up to date cipher suites, this may be solved by system updates and patches. Most notably, on windows server 2012 make sure the tls cipher suite update is installed Your firewall, proxy or network configuration may be blocking the connection You will want to reach out to whoever is in charge of your network with these pcap files to further troubleshoot If you see a failure later in the handshake: Try the fix in the SSLCert Fix","title":"Troubleshooting: Why can't I configure a runner?"},{"location":"gh-actions/checks/nodejs/","text":"Node.js Connection Check What is this check for? Make sure the built-in node.js has access to GitHub.com or GitHub Enterprise Server. The runner carries its own copy of node.js executable under <runner_root>/externals/node16/ . All javascript base Actions will get executed by the built-in node at <runner_root>/externals/node16/ . Not the node from $PATH What is checked? Make HTTPS GET to https://api.github.com or https://myGHES.com/api/v3 using node.js, make sure it gets 200 response code. How to fix the issue? 1. Check the common network issue Please check the network doc 2. SSL certificate related issue If you are seeing Https request failed due to SSL cert issue in the log, it means the node.js can't connect to the GitHub server due to SSL handshake failure. Please check the SSL cert doc Still not working? Contact GitHub customer service or log an issue at https://github.com/actions/runner if you think it's a runner issue.","title":"Nodejs"},{"location":"gh-actions/checks/nodejs/#nodejs-connection-check","text":"","title":"Node.js Connection Check"},{"location":"gh-actions/checks/nodejs/#what-is-this-check-for","text":"Make sure the built-in node.js has access to GitHub.com or GitHub Enterprise Server. The runner carries its own copy of node.js executable under <runner_root>/externals/node16/ . All javascript base Actions will get executed by the built-in node at <runner_root>/externals/node16/ . Not the node from $PATH","title":"What is this check for?"},{"location":"gh-actions/checks/nodejs/#what-is-checked","text":"Make HTTPS GET to https://api.github.com or https://myGHES.com/api/v3 using node.js, make sure it gets 200 response code.","title":"What is checked?"},{"location":"gh-actions/checks/nodejs/#how-to-fix-the-issue","text":"","title":"How to fix the issue?"},{"location":"gh-actions/checks/nodejs/#1-check-the-common-network-issue","text":"Please check the network doc","title":"1. Check the common network issue"},{"location":"gh-actions/checks/nodejs/#2-ssl-certificate-related-issue","text":"If you are seeing Https request failed due to SSL cert issue in the log, it means the node.js can't connect to the GitHub server due to SSL handshake failure. Please check the SSL cert doc","title":"2. SSL certificate related issue"},{"location":"gh-actions/checks/nodejs/#still-not-working","text":"Contact GitHub customer service or log an issue at https://github.com/actions/runner if you think it's a runner issue.","title":"Still not working?"},{"location":"gh-actions/checks/ssl_certs/","text":"SSL Certificate Related Issues You might run into an SSL certificate error when your GitHub Enterprise Server is using a self-signed SSL server certificate or a web proxy within your network is decrypting HTTPS traffic for a security audit. As long as your certificate is generated properly, most of the issues should be fixed after your trust the certificate properly on the runner machine. github repo actions/runner Different OS might have extra requirements on SSL certificate, Ex: macOS requires ExtendedKeyUsage https://support.apple.com/en-us/HT210176 Don't skip SSL cert validation !!! DO NOT SKIP SSL CERT VALIDATION !!! !!! IT IS A BAD SECURITY PRACTICE !!! Download SSL certificate chain Depends on how your SSL server certificate gets configured, you might need to download the whole certificate chain from a machine that has trusted the SSL certificate's CA. Approach 1: Download certificate chain using a browser (Chrome, Firefox, IT), you can google for more example, here is what I found Approach 2: Download certificate chain using OpenSSL, you can google for more example, here is what I found Approach 3: Ask your network administrator or the owner of the CA certificate to send you a copy of it Trust CA certificate for the Runner The actions runner is a dotnet core application which will follow how dotnet load SSL CA certificates on each OS. You can get full details documentation at here In short: - Windows: Load from Windows certificate store. - Linux: Load from OpenSSL CA cert bundle. - macOS: Load from macOS KeyChain. To let the runner trusts your CA certificate, you will need to: 1. Save your SSL certificate chain which includes the root CA and all intermediate CAs into a .pem file. 2. Use OpenSSL to convert .pem file to a proper format for different OS, here is some doc with sample commands 3. Trust CA on different OS: - Windows: https://docs.microsoft.com/en-us/skype-sdk/sdn/articles/installing-the-trusted-root-certificate - macOS: - Linux: Refer to the distribution documentation 1. RedHat: https://www.redhat.com/sysadmin/ca-certificates-cli 2. Ubuntu: http://manpages.ubuntu.com/manpages/focal/man8/update-ca-certificates.8.html 3. Google search: \"trust ca certificate on [linux distribution]\" 4. If all approaches failed, set environment variable SSL_CERT_FILE to the CA bundle .pem file we get. > To verify cert gets installed properly on Linux, you can try use curl -v https://sitewithsslissue.com and pwsh -Command \\\"Invoke-WebRequest -Uri https://sitewithsslissue.com\\\" Trust CA certificate for Git CLI Git uses various CA bundle file depends on your operation system. - Git packaged the CA bundle file within the Git installation on Windows - Git use OpenSSL certificate CA bundle file on Linux and macOS You can check where Git check CA file by running: export GIT_CURL_VERBOSE=1 git ls-remote https://github.com/actions/runner HEAD You should see something like: * Couldn't find host github.com in the .netrc file; using defaults * Trying 140.82.114.4... * TCP_NODELAY set * Connected to github.com (140.82.114.4) port 443 (#0) * ALPN, offering h2 * ALPN, offering http/1.1 * successfully set certificate verify locations: * CAfile: /etc/ssl/cert.pem CApath: none * SSL connection using TLSv1.2 / ECDHE-RSA-AES128-GCM-SHA256 This tells me /etc/ssl/cert.pem is where it read trusted CA certificates. To let Git trusts your CA certificate, you will need to: 1. Save your SSL certificate chain which includes the root CA and all intermediate CAs into a .pem file. 2. Set http.sslCAInfo Git config or GIT_SSL_CAINFO environment variable to the full path of the .pem file Git Doc I would recommend using http.sslCAInfo since it can be scope to certain hosts that need the extra trusted CA. Ex: git config --global http.https://myghes.com/.sslCAInfo /extra/ca/cert.pem This will make Git use the /extra/ca/cert.pem only when communicates with https://myghes.com and keep using the default CA bundle with others. Trust CA certificate for Node.js Node.js has compiled a snapshot of the Mozilla CA store that is fixed at each version of Node.js' release time. To let Node.js trusts your CA certificate, you will need to: 1. Save your SSL certificate chain which includes the root CA and all intermediate CAs into a .pem file. 2. Set environment variable NODE_EXTRA_CA_CERTS which point to the file. ex: export NODE_EXTRA_CA_CERTS=/full/path/to/cacert.pem or set NODE_EXTRA_CA_CERTS=C:\\full\\path\\to\\cacert.pem","title":"SSL Certs"},{"location":"gh-actions/checks/ssl_certs/#ssl-certificate-related-issues","text":"You might run into an SSL certificate error when your GitHub Enterprise Server is using a self-signed SSL server certificate or a web proxy within your network is decrypting HTTPS traffic for a security audit. As long as your certificate is generated properly, most of the issues should be fixed after your trust the certificate properly on the runner machine. github repo actions/runner Different OS might have extra requirements on SSL certificate, Ex: macOS requires ExtendedKeyUsage https://support.apple.com/en-us/HT210176","title":"SSL Certificate Related Issues"},{"location":"gh-actions/checks/ssl_certs/#dont-skip-ssl-cert-validation","text":"!!! DO NOT SKIP SSL CERT VALIDATION !!! !!! IT IS A BAD SECURITY PRACTICE !!!","title":"Don't skip SSL cert validation"},{"location":"gh-actions/checks/ssl_certs/#download-ssl-certificate-chain","text":"Depends on how your SSL server certificate gets configured, you might need to download the whole certificate chain from a machine that has trusted the SSL certificate's CA. Approach 1: Download certificate chain using a browser (Chrome, Firefox, IT), you can google for more example, here is what I found Approach 2: Download certificate chain using OpenSSL, you can google for more example, here is what I found Approach 3: Ask your network administrator or the owner of the CA certificate to send you a copy of it","title":"Download SSL certificate chain"},{"location":"gh-actions/checks/ssl_certs/#trust-ca-certificate-for-the-runner","text":"The actions runner is a dotnet core application which will follow how dotnet load SSL CA certificates on each OS. You can get full details documentation at here In short: - Windows: Load from Windows certificate store. - Linux: Load from OpenSSL CA cert bundle. - macOS: Load from macOS KeyChain. To let the runner trusts your CA certificate, you will need to: 1. Save your SSL certificate chain which includes the root CA and all intermediate CAs into a .pem file. 2. Use OpenSSL to convert .pem file to a proper format for different OS, here is some doc with sample commands 3. Trust CA on different OS: - Windows: https://docs.microsoft.com/en-us/skype-sdk/sdn/articles/installing-the-trusted-root-certificate - macOS: - Linux: Refer to the distribution documentation 1. RedHat: https://www.redhat.com/sysadmin/ca-certificates-cli 2. Ubuntu: http://manpages.ubuntu.com/manpages/focal/man8/update-ca-certificates.8.html 3. Google search: \"trust ca certificate on [linux distribution]\" 4. If all approaches failed, set environment variable SSL_CERT_FILE to the CA bundle .pem file we get. > To verify cert gets installed properly on Linux, you can try use curl -v https://sitewithsslissue.com and pwsh -Command \\\"Invoke-WebRequest -Uri https://sitewithsslissue.com\\\"","title":"Trust CA certificate for the Runner"},{"location":"gh-actions/checks/ssl_certs/#trust-ca-certificate-for-git-cli","text":"Git uses various CA bundle file depends on your operation system. - Git packaged the CA bundle file within the Git installation on Windows - Git use OpenSSL certificate CA bundle file on Linux and macOS You can check where Git check CA file by running: export GIT_CURL_VERBOSE=1 git ls-remote https://github.com/actions/runner HEAD You should see something like: * Couldn't find host github.com in the .netrc file; using defaults * Trying 140.82.114.4... * TCP_NODELAY set * Connected to github.com (140.82.114.4) port 443 (#0) * ALPN, offering h2 * ALPN, offering http/1.1 * successfully set certificate verify locations: * CAfile: /etc/ssl/cert.pem CApath: none * SSL connection using TLSv1.2 / ECDHE-RSA-AES128-GCM-SHA256 This tells me /etc/ssl/cert.pem is where it read trusted CA certificates. To let Git trusts your CA certificate, you will need to: 1. Save your SSL certificate chain which includes the root CA and all intermediate CAs into a .pem file. 2. Set http.sslCAInfo Git config or GIT_SSL_CAINFO environment variable to the full path of the .pem file Git Doc I would recommend using http.sslCAInfo since it can be scope to certain hosts that need the extra trusted CA. Ex: git config --global http.https://myghes.com/.sslCAInfo /extra/ca/cert.pem This will make Git use the /extra/ca/cert.pem only when communicates with https://myghes.com and keep using the default CA bundle with others.","title":"Trust CA certificate for Git CLI"},{"location":"gh-actions/checks/ssl_certs/#trust-ca-certificate-for-nodejs","text":"Node.js has compiled a snapshot of the Mozilla CA store that is fixed at each version of Node.js' release time. To let Node.js trusts your CA certificate, you will need to: 1. Save your SSL certificate chain which includes the root CA and all intermediate CAs into a .pem file. 2. Set environment variable NODE_EXTRA_CA_CERTS which point to the file. ex: export NODE_EXTRA_CA_CERTS=/full/path/to/cacert.pem or set NODE_EXTRA_CA_CERTS=C:\\full\\path\\to\\cacert.pem","title":"Trust CA certificate for Node.js"},{"location":"hosting-and-deployment/enable_github_pages/","text":"Enable Github Pages Github pages requires permissions to be enabled but the process is pretty easy. Create a public repository Navigate to Settings > Pages Configure the branch and build directory. \u2757 github.io requires using the owner name to make the web site visible.","title":"Enable GH Pages"},{"location":"hosting-and-deployment/enable_github_pages/#enable-github-pages","text":"Github pages requires permissions to be enabled but the process is pretty easy. Create a public repository Navigate to Settings > Pages Configure the branch and build directory. \u2757 github.io requires using the owner name to make the web site visible.","title":"Enable Github Pages"},{"location":"hosting-and-deployment/gh-pages-private-repo/","text":"Github Pages Private Source Code Github pages are great, it provides a free static page hosting, but the only caveat is the repository has to be public repository. And, if you want to keep your source private, you will have to opt for premium plans to host pages from private repository. Here is what I have done, github allows unlimited private repositories, so I created a new private repo where I kept my source code and another repo where my site is hosted. Create Personal Token Create a personal access token. Navigate to Settings and create a PAT. Select Developer settings Select Personal access tokens Now generate a new token, with repo permissions. Once you are done copy the generated token, we will need to set this token during our build. Create secret in private repository Go to your private repo and click the settings: Create github action in private repoPermalink This is where the magic begins, we will build a github action in our private repo. You will need to create a file at .github/workflows/ci.yml name: Build & Publish on: push: branches: [ gh_pages ] pull_request: branches: [ gh_pages ] jobs: build: runs-on: ubuntu-latest steps: - name: Checkout local code uses: actions/checkout@v3 with: path: code token: ${{ secrets.GH_PAGES}} ref: gh_pages # - name: Show Directory Files # run : | # cd code # ls -la - name: python uses: actions/setup-python@v4 with: python-version: \"3.10\" - name: Checkout public repo site uses: actions/checkout@v3 with: token: ${{ secrets.GH_PAGES}} repository: np-completed/np-completed.github.io ref: gh_pages path: site - name: Install dependencies run: python3 -m pip install -r code/requirements.txt - name: Build website run: mkdocs build --config-file code/mkdocs.yml - name: Clean Website run: | pushd site git rm -rf . popd - name: Copy website run : | pushd site # cp -rvf ../code/build/* . cp -rvf ../code/site/* . popd ls -la site/ # ls -la code/site/ - name: Deploy and Publish run: | git config --global user.email \"${GITHUB_ACTOR}@users.noreply.github.com\" git config --global user.name \"github-actions\" pushd site git add . git commit -m \"mkdocs build from Action ${GITHUB_SHA}\" git push origin gh_pages popd Each push changes to the private repository triggers the github action. Next, the GH action job will be executed, to build and publish the site to the public repo without exposing your source code.","title":"Private Repo with GH Pages"},{"location":"hosting-and-deployment/gh-pages-private-repo/#github-pages-private-source-code","text":"Github pages are great, it provides a free static page hosting, but the only caveat is the repository has to be public repository. And, if you want to keep your source private, you will have to opt for premium plans to host pages from private repository. Here is what I have done, github allows unlimited private repositories, so I created a new private repo where I kept my source code and another repo where my site is hosted.","title":"Github Pages Private Source Code"},{"location":"hosting-and-deployment/gh-pages-private-repo/#create-personal-token","text":"Create a personal access token. Navigate to Settings and create a PAT. Select Developer settings Select Personal access tokens Now generate a new token, with repo permissions. Once you are done copy the generated token, we will need to set this token during our build.","title":"Create Personal Token"},{"location":"hosting-and-deployment/gh-pages-private-repo/#create-secret-in-private-repository","text":"Go to your private repo and click the settings:","title":"Create secret in private repository"},{"location":"hosting-and-deployment/gh-pages-private-repo/#create-github-action-in-private-repopermalink","text":"This is where the magic begins, we will build a github action in our private repo. You will need to create a file at .github/workflows/ci.yml name: Build & Publish on: push: branches: [ gh_pages ] pull_request: branches: [ gh_pages ] jobs: build: runs-on: ubuntu-latest steps: - name: Checkout local code uses: actions/checkout@v3 with: path: code token: ${{ secrets.GH_PAGES}} ref: gh_pages # - name: Show Directory Files # run : | # cd code # ls -la - name: python uses: actions/setup-python@v4 with: python-version: \"3.10\" - name: Checkout public repo site uses: actions/checkout@v3 with: token: ${{ secrets.GH_PAGES}} repository: np-completed/np-completed.github.io ref: gh_pages path: site - name: Install dependencies run: python3 -m pip install -r code/requirements.txt - name: Build website run: mkdocs build --config-file code/mkdocs.yml - name: Clean Website run: | pushd site git rm -rf . popd - name: Copy website run : | pushd site # cp -rvf ../code/build/* . cp -rvf ../code/site/* . popd ls -la site/ # ls -la code/site/ - name: Deploy and Publish run: | git config --global user.email \"${GITHUB_ACTOR}@users.noreply.github.com\" git config --global user.name \"github-actions\" pushd site git add . git commit -m \"mkdocs build from Action ${GITHUB_SHA}\" git push origin gh_pages popd Each push changes to the private repository triggers the github action. Next, the GH action job will be executed, to build and publish the site to the public repo without exposing your source code.","title":"Create github action in private repoPermalink"},{"location":"hosting-and-deployment/gh-pages/","text":"Host on GitHub Pages Demo site on GitHub Pages (build & deploy with GitHub Actions) Build and deploy with GitHub Actions peaceiris/actions-gh-pages: GitHub Actions for deploying to GitHub Pages with Static Site Generators Go to the repository and read the latest README.md for more details. Build and deploy with mkdocs gh-deploy pipenv pipenv run deploy # OR pipenv shell mkdocs gh-deploy # OR pipenv run mkdocs gh-deploy","title":"Github Pages"},{"location":"hosting-and-deployment/gh-pages/#host-on-github-pages","text":"Demo site on GitHub Pages (build & deploy with GitHub Actions)","title":"Host on GitHub Pages"},{"location":"hosting-and-deployment/gh-pages/#build-and-deploy-with-github-actions","text":"peaceiris/actions-gh-pages: GitHub Actions for deploying to GitHub Pages with Static Site Generators Go to the repository and read the latest README.md for more details.","title":"Build and deploy with GitHub Actions"},{"location":"hosting-and-deployment/gh-pages/#build-and-deploy-with-mkdocs-gh-deploy","text":"","title":"Build and deploy with mkdocs gh-deploy"},{"location":"hosting-and-deployment/gh-pages/#pipenv","text":"pipenv run deploy # OR pipenv shell mkdocs gh-deploy # OR pipenv run mkdocs gh-deploy","title":"pipenv"},{"location":"sagemaker/secure-sagemaker-network-firewall-summary/","text":"Securing Amazon SageMaker Studio internet traffic using AWS Network Firewall Building secure machine learning environments with Amazon SageMaker Securing Amazon SageMaker Studio connectivity using a private VPC. Configuring Amazon SageMaker Studio for teams and groups with complete resource isolation Securing Amazon SageMaker Studio connectivity using a private VPC Background Amazon SageMaker Studio is a web-based fully integrated development environment (IDE) where you can perform end-to-end machine learning (ML) development to prepare data and build, train, and deploy models. One of these fundamental security features allows you to launch Studio in your own Amazon Virtual Private Cloud (Amazon VPC). This allows you to control, monitor, and inspect network traffic within and outside your VPC using standard AWS networking and security capabilities. For more information, see Securing Amazon SageMaker Studio connectivity using a private VPC. SM Studio users may want to provide internet access but also have some controls such as domain name or URL filtering and allow access to only specific public repositories and websites, possibly packet inspection, or other network traffic-related security controls. For these cases, AWS Network Firewall and NAT gateway-based deployment may provide a suitable use case. In this post, I outline the use of network firewall to build a secure and compliant environment by restricting and monitoring internet access, inspecting traffic, and using stateless and stateful firewall engine rules to control the network flow between Studio notebooks and the internet. Depending on your security, compliance, and governance rules, you may not need to or cannot completely block internet access from Studio and your AI and ML workloads. You may have requirements beyond the scope of network security controls implemented by security groups and network access control lists (ACLs), such as application protocol protection, deep packet inspection, domain name filtering, and intrusion prevention system (IPS). Your network traffic controls may also require many more rules compared to what is currently supported in security groups and network ACLs. In these scenarios, you can use Network Firewall\u2014a managed network firewall and IPS for your VPC. Solution overview Sagemaker Studio deployed in a VPC, provides internet access control, using the parameter AppNetworkAccessType (via the Amazon SageMaker API ) or by selecting your preference on the console when you create a Studio domain. If you select Public internet Only ( PublicInternetOnly ), all the ingress and egress internet traffic from Amazon SageMaker notebooks flows through an AWS managed internet gateway attached to a VPC in your SageMaker account. The following diagram shows this network configuration. Studio provides public internet egress through a platform-managed VPC for data scientists to download notebooks, packages, and datasets. Traffic to the attached Amazon Elastic File System (Amazon EFS) volume always goes through the customer VPC and never through the public internet egress. To use your own control flow for the internet traffic, like a NAT or internet gateway, you must set the AppNetworkAccessType parameter to VpcOnly (or select VPC Only on the console). When you launch your app, this creates an elastic network interface in the specified subnets in your VPC. You can apply all available layers of security control\u2014 security groups , network ACLs , VPC endpoints , AWS PrivateLink , or Network Firewall endpoints \u2014to the internal network and internet traffic to exercise fine-grained control of network access in Studio. The following diagram shows the VpcOnly network configuration. In this mode, the direct internet access to or from notebooks is completely disabled, and all traffic is routed through an elastic network interface in your private VPC. This also includes traffic from Studio UI widgets and interfaces, such as Experiments , Autopilot , and Model Monitor , to their respective backend SageMaker APIs. VPC only option The solution in this post uses the VpcOnly option and deploys the Studio domain into a VPC with three subnets: SageMaker subnet \u2013 Hosts all Studio workloads. All ingress and egress network flow is controlled by a security group. NAT subnet \u2013 Contains a NAT gateway. We use the NAT gateway to access the internet without exposing any private IP addresses from our private network. Network Firewall subnet \u2013 Contains a Network Firewall endpoint. The route tables are configured so that all inbound and outbound external network traffic is routed via Network Firewall. You can configure stateful and stateless Network Firewall policies to inspect, monitor, and control the traffic. The following diagram shows the overview of the solution architecture and the deployed components. SageMaker resources Create a SageMaker domain and user profile. The solution uses only one Availability Zone and is not highly available. A best practice is to use a Multi-AZ configuration for any production deployment. We create an allow domain list rule to allow internet access to the specified network domains only and block traffic to any domain not on the allow list. AWS CloudFormation resources The source code and AWS CloudFormation template for solution deployment are provided in the GitHub repository . Network Firewall is a Regional service; for more information on Region availability, see the AWS Region Table . To start experimenting with the Network Firewall and stateful rules, you need first to deploy the provided CloudFormation template to the AWS account. Clone the GitHub repository: Create an S3 bucket in the Region where you deploy the solution: aws s3 mb s3://<your s3 bucket name> You can skip this step if you already have an S3 bucket. Deploy the CloudFormation stack: make deploy CFN_ARTEFACT_S3_BUCKET=<your s3 bucket name> The deployment procedure packages the CloudFormation template and copies it to the S3 bucket your provided. Then the CloudFormation template is deployed from the S3 bucket to your AWS account. The stack deploys all the needed resources like VPC, network devices, route tables, security groups, S3 buckets, IAM policies and roles, and VPC endpoints, and also creates a new Studio domain and user profile. When the deployment is complete, you can see the full list of stack output values by running the following command in terminal: aws cloudformation describe-stacks \\ --stack-name sagemaker-studio-demo \\ --output table \\ --query \"Stacks[0].Outputs[*].[OutputKey, OutputValue]\" Launch Studio via the SageMaker console. Experiment with Network Firewall Now you can learn how to control the internet inbound and outbound access with Network Firewall. In this section, we discuss the initial setup, accessing resources not on the allow list, adding domains to the allow list, configuring logging, and additional firewall rules. Initial setup The solution deploys a Network Firewall policy with a stateful rule group with an allow domain list. This policy is attached to the Network Firewall. All inbound and outbound internet traffic is blocked now, except for the .kaggle.com domain, which is on the allow list. Let\u2019s try to access https://kaggle.com by opening a new notebook in Studio and attempting to download the front page from kaggle.com : !wget https://kaggle.com The following screenshot shows that the request succeeds because the domain is allowed by the firewall policy. Users can connect to this and only to this domain from any Studio notebook. Access resources not on the allowed domain list In the Studio notebook, try to clone any public GitHub repository, such as the following: !git clone https://github.com/pytorch/examples.git This operation times out after 5 minutes because any internet traffic except to and from the .kaggle.com domain isn\u2019t allowed and is dropped by the firewall. Add a domain to the allowed domain list To be able to run the git clone command, you must allow internet traffic to the .github.com domain. On the Amazon VPC console, choose Firewall policies. Choose the policy network-firewall-policy- . In the Stateful rule groups section, select the group rule domain-allow-sagemaker- . You can see the domain .kaggle.com on the allow list. Choose Add domain. Enter .github.com . Choose Save. You now have two names on the allow domain list. Firewall policy is propagated in real time to Network Firewall and your changes take effect immediately. Any inbound or outbound traffic from or to these domains is now allowed by the firewall and all other traffic is dropped. To validate the new configuration, go to your Studio notebook and try to clone the same GitHub repository again: !git clone https://github.com/aws-samples/amazon-sagemaker-studio-vpc-networkfirewall.git The operation succeeds this time\u2014Network Firewall allows access to the .github.com domain. Network Firewall logging In this section, you configure Network Firewall logging for your firewall\u2019s stateful engine. Logging gives you detailed information about network traffic, including the time that the stateful engine received a packet, detailed information about the packet, and any stateful rule action taken against the packet. The logs are published to the log destination that you configured, where you can retrieve and view them. On the Amazon VPC console, choose Firewalls . Choose your firewall. Choose the Firewall details tab. In the Logging section, choose Edit . Configure your firewall logging by selecting what log types you want to capture and providing the log destination. For this post, select Alert log type, set Log destination for alerts to CloudWatch Log group, and provide an existing or a new log group where the firewall logs are delivered. Choose Save . To check your settings, go back to Studio and try to access pypi.org to install a Python package: !pip install -U scikit-learn This command fails with ReadTimeoutError because Network Firewall drops any traffic to any domain not on the allow list (which contains only two domains: .github.com and .kaggle.com ). On the Amazon CloudWatch console , navigate to the log group and browse through the recent log streams. The pipy.org domain shows the blocked action. The log event also provides additional details such as various timestamps, protocol, port and IP details, event type, availability zone, and the firewall name. You can continue experimenting with Network Firewall by adding .pypi.org and .pythonhosted.org domains to the allowed domain list. Then validate your access to them via your Studio notebook. Additional firewall rules You can create any other stateless or stateful firewall rules and implement traffic filtering based on a standard stateful 5-tuple rule for network traffic inspection (protocol, source IP, source port, destination IP, destination port). Network Firewall also supports industry standard stateful Suricata compatible IPS rule groups. You can implement protocol-based rules to detect and block any non-standard or promiscuous usage or activity. For more information about creating and managing Network Firewall rule groups, see Rule groups in AWS Network Firewall. Additional security controls with Network Firewall In the previous section, we looked at one feature of the Network Firewall: filtering network traffic based on the domain name. In addition to stateless or stateful firewall rules, Network Firewall provides several tools and features for further security controls and monitoring: Central firewall management and visibility in AWS Firewall Manager . You can centrally manage security policies and automatically enforce mandatory security policies across existing and newly created accounts and VPCs. Network Firewall logging for the firewall\u2019s stateful engine. You can record flow and alert logs, and use the same or different logging destinations for each log type. Stateless rules to filter network traffic based on protocol, source IP addresses, ranges, source port ranges, destination IP addresses and ranges, and TCP flags. Integration into a broader set of AWS security components. For an example, see Automatically block suspicious traffic with AWS Network Firewall and Amazon GuardDuty. Integration in a diverse ecosystem of Network Firewall Partners that complement Network Firewall, enabling the deployment of a comprehensive security architecture. For example use cases, see Full VPC traffic visibility with AWS Network Firewall and Sumo Logic and Splunk Named Launch Partner of AWS Network Firewall. Build secure ML environments A robust security design normally includes multi-layer security controls for the system. For SageMaker environments and workloads, you can use the following AWS security services and concepts to secure, control, and monitor your environment: VPC and private subnets to perform secure API calls to other AWS services and restrict internet access for downloading packages. S3 bucket policies that restrict access to specific VPC endpoints. Encryption of ML model artifacts and other system artifacts that are either in transit or at rest. Requests to the SageMaker API and console are made over a Secure Sockets Layer (SSL) connection. Restricted IAM roles and policies for SageMaker runs and notebook access based on resource tags and project ID. Restricted access to Amazon public services, such as Amazon Elastic Container Registry (Amazon ECR) to VPC endpoints only. For a reference deployment architecture and ready-to-use deployable constructs for your environment, see Amazon SageMaker with Guardrails on AWS. Conclusion In this post, we showed how you can secure, log, and monitor internet ingress and egress traffic in Studio notebooks for your sensitive ML workloads using managed Network Firewall. You can use the provided CloudFormation templates to automate SageMaker deployment as part of your Infrastructure as Code (IaC) strategy. For more information about other possibilities to secure your SageMaker deployments and ML workloads, see Building secure machine learning environments with Amazon SageMaker.","title":"Secure Network Summary"},{"location":"sagemaker/secure-sagemaker-network-firewall-summary/#securing-amazon-sagemaker-studio-internet-traffic-using-aws-network-firewall","text":"Building secure machine learning environments with Amazon SageMaker Securing Amazon SageMaker Studio connectivity using a private VPC. Configuring Amazon SageMaker Studio for teams and groups with complete resource isolation Securing Amazon SageMaker Studio connectivity using a private VPC","title":"Securing Amazon SageMaker Studio internet traffic using AWS Network Firewall"},{"location":"sagemaker/secure-sagemaker-network-firewall-summary/#background","text":"Amazon SageMaker Studio is a web-based fully integrated development environment (IDE) where you can perform end-to-end machine learning (ML) development to prepare data and build, train, and deploy models. One of these fundamental security features allows you to launch Studio in your own Amazon Virtual Private Cloud (Amazon VPC). This allows you to control, monitor, and inspect network traffic within and outside your VPC using standard AWS networking and security capabilities. For more information, see Securing Amazon SageMaker Studio connectivity using a private VPC. SM Studio users may want to provide internet access but also have some controls such as domain name or URL filtering and allow access to only specific public repositories and websites, possibly packet inspection, or other network traffic-related security controls. For these cases, AWS Network Firewall and NAT gateway-based deployment may provide a suitable use case. In this post, I outline the use of network firewall to build a secure and compliant environment by restricting and monitoring internet access, inspecting traffic, and using stateless and stateful firewall engine rules to control the network flow between Studio notebooks and the internet. Depending on your security, compliance, and governance rules, you may not need to or cannot completely block internet access from Studio and your AI and ML workloads. You may have requirements beyond the scope of network security controls implemented by security groups and network access control lists (ACLs), such as application protocol protection, deep packet inspection, domain name filtering, and intrusion prevention system (IPS). Your network traffic controls may also require many more rules compared to what is currently supported in security groups and network ACLs. In these scenarios, you can use Network Firewall\u2014a managed network firewall and IPS for your VPC.","title":"Background"},{"location":"sagemaker/secure-sagemaker-network-firewall-summary/#solution-overview","text":"Sagemaker Studio deployed in a VPC, provides internet access control, using the parameter AppNetworkAccessType (via the Amazon SageMaker API ) or by selecting your preference on the console when you create a Studio domain. If you select Public internet Only ( PublicInternetOnly ), all the ingress and egress internet traffic from Amazon SageMaker notebooks flows through an AWS managed internet gateway attached to a VPC in your SageMaker account. The following diagram shows this network configuration. Studio provides public internet egress through a platform-managed VPC for data scientists to download notebooks, packages, and datasets. Traffic to the attached Amazon Elastic File System (Amazon EFS) volume always goes through the customer VPC and never through the public internet egress. To use your own control flow for the internet traffic, like a NAT or internet gateway, you must set the AppNetworkAccessType parameter to VpcOnly (or select VPC Only on the console). When you launch your app, this creates an elastic network interface in the specified subnets in your VPC. You can apply all available layers of security control\u2014 security groups , network ACLs , VPC endpoints , AWS PrivateLink , or Network Firewall endpoints \u2014to the internal network and internet traffic to exercise fine-grained control of network access in Studio. The following diagram shows the VpcOnly network configuration. In this mode, the direct internet access to or from notebooks is completely disabled, and all traffic is routed through an elastic network interface in your private VPC. This also includes traffic from Studio UI widgets and interfaces, such as Experiments , Autopilot , and Model Monitor , to their respective backend SageMaker APIs.","title":"Solution overview"},{"location":"sagemaker/secure-sagemaker-network-firewall-summary/#vpc-only-option","text":"The solution in this post uses the VpcOnly option and deploys the Studio domain into a VPC with three subnets: SageMaker subnet \u2013 Hosts all Studio workloads. All ingress and egress network flow is controlled by a security group. NAT subnet \u2013 Contains a NAT gateway. We use the NAT gateway to access the internet without exposing any private IP addresses from our private network. Network Firewall subnet \u2013 Contains a Network Firewall endpoint. The route tables are configured so that all inbound and outbound external network traffic is routed via Network Firewall. You can configure stateful and stateless Network Firewall policies to inspect, monitor, and control the traffic. The following diagram shows the overview of the solution architecture and the deployed components.","title":"VPC only option"},{"location":"sagemaker/secure-sagemaker-network-firewall-summary/#sagemaker-resources","text":"Create a SageMaker domain and user profile. The solution uses only one Availability Zone and is not highly available. A best practice is to use a Multi-AZ configuration for any production deployment. We create an allow domain list rule to allow internet access to the specified network domains only and block traffic to any domain not on the allow list.","title":"SageMaker resources"},{"location":"sagemaker/secure-sagemaker-network-firewall-summary/#aws-cloudformation-resources","text":"The source code and AWS CloudFormation template for solution deployment are provided in the GitHub repository . Network Firewall is a Regional service; for more information on Region availability, see the AWS Region Table . To start experimenting with the Network Firewall and stateful rules, you need first to deploy the provided CloudFormation template to the AWS account. Clone the GitHub repository: Create an S3 bucket in the Region where you deploy the solution: aws s3 mb s3://<your s3 bucket name> You can skip this step if you already have an S3 bucket. Deploy the CloudFormation stack: make deploy CFN_ARTEFACT_S3_BUCKET=<your s3 bucket name> The deployment procedure packages the CloudFormation template and copies it to the S3 bucket your provided. Then the CloudFormation template is deployed from the S3 bucket to your AWS account. The stack deploys all the needed resources like VPC, network devices, route tables, security groups, S3 buckets, IAM policies and roles, and VPC endpoints, and also creates a new Studio domain and user profile. When the deployment is complete, you can see the full list of stack output values by running the following command in terminal: aws cloudformation describe-stacks \\ --stack-name sagemaker-studio-demo \\ --output table \\ --query \"Stacks[0].Outputs[*].[OutputKey, OutputValue]\" Launch Studio via the SageMaker console.","title":"AWS CloudFormation resources"},{"location":"sagemaker/secure-sagemaker-network-firewall-summary/#experiment-with-network-firewall","text":"Now you can learn how to control the internet inbound and outbound access with Network Firewall. In this section, we discuss the initial setup, accessing resources not on the allow list, adding domains to the allow list, configuring logging, and additional firewall rules.","title":"Experiment with Network Firewall"},{"location":"sagemaker/secure-sagemaker-network-firewall-summary/#initial-setup","text":"The solution deploys a Network Firewall policy with a stateful rule group with an allow domain list. This policy is attached to the Network Firewall. All inbound and outbound internet traffic is blocked now, except for the .kaggle.com domain, which is on the allow list. Let\u2019s try to access https://kaggle.com by opening a new notebook in Studio and attempting to download the front page from kaggle.com : !wget https://kaggle.com The following screenshot shows that the request succeeds because the domain is allowed by the firewall policy. Users can connect to this and only to this domain from any Studio notebook.","title":"Initial setup"},{"location":"sagemaker/secure-sagemaker-network-firewall-summary/#access-resources-not-on-the-allowed-domain-list","text":"In the Studio notebook, try to clone any public GitHub repository, such as the following: !git clone https://github.com/pytorch/examples.git This operation times out after 5 minutes because any internet traffic except to and from the .kaggle.com domain isn\u2019t allowed and is dropped by the firewall.","title":"Access resources not on the allowed domain list"},{"location":"sagemaker/secure-sagemaker-network-firewall-summary/#add-a-domain-to-the-allowed-domain-list","text":"To be able to run the git clone command, you must allow internet traffic to the .github.com domain. On the Amazon VPC console, choose Firewall policies. Choose the policy network-firewall-policy- . In the Stateful rule groups section, select the group rule domain-allow-sagemaker- . You can see the domain .kaggle.com on the allow list. Choose Add domain. Enter .github.com . Choose Save. You now have two names on the allow domain list. Firewall policy is propagated in real time to Network Firewall and your changes take effect immediately. Any inbound or outbound traffic from or to these domains is now allowed by the firewall and all other traffic is dropped. To validate the new configuration, go to your Studio notebook and try to clone the same GitHub repository again: !git clone https://github.com/aws-samples/amazon-sagemaker-studio-vpc-networkfirewall.git The operation succeeds this time\u2014Network Firewall allows access to the .github.com domain.","title":"Add a domain to the allowed domain list"},{"location":"sagemaker/secure-sagemaker-network-firewall-summary/#network-firewall-logging","text":"In this section, you configure Network Firewall logging for your firewall\u2019s stateful engine. Logging gives you detailed information about network traffic, including the time that the stateful engine received a packet, detailed information about the packet, and any stateful rule action taken against the packet. The logs are published to the log destination that you configured, where you can retrieve and view them. On the Amazon VPC console, choose Firewalls . Choose your firewall. Choose the Firewall details tab. In the Logging section, choose Edit . Configure your firewall logging by selecting what log types you want to capture and providing the log destination. For this post, select Alert log type, set Log destination for alerts to CloudWatch Log group, and provide an existing or a new log group where the firewall logs are delivered. Choose Save . To check your settings, go back to Studio and try to access pypi.org to install a Python package: !pip install -U scikit-learn This command fails with ReadTimeoutError because Network Firewall drops any traffic to any domain not on the allow list (which contains only two domains: .github.com and .kaggle.com ). On the Amazon CloudWatch console , navigate to the log group and browse through the recent log streams. The pipy.org domain shows the blocked action. The log event also provides additional details such as various timestamps, protocol, port and IP details, event type, availability zone, and the firewall name. You can continue experimenting with Network Firewall by adding .pypi.org and .pythonhosted.org domains to the allowed domain list. Then validate your access to them via your Studio notebook.","title":"Network Firewall logging"},{"location":"sagemaker/secure-sagemaker-network-firewall-summary/#additional-firewall-rules","text":"You can create any other stateless or stateful firewall rules and implement traffic filtering based on a standard stateful 5-tuple rule for network traffic inspection (protocol, source IP, source port, destination IP, destination port). Network Firewall also supports industry standard stateful Suricata compatible IPS rule groups. You can implement protocol-based rules to detect and block any non-standard or promiscuous usage or activity. For more information about creating and managing Network Firewall rule groups, see Rule groups in AWS Network Firewall.","title":"Additional firewall rules"},{"location":"sagemaker/secure-sagemaker-network-firewall-summary/#additional-security-controls-with-network-firewall","text":"In the previous section, we looked at one feature of the Network Firewall: filtering network traffic based on the domain name. In addition to stateless or stateful firewall rules, Network Firewall provides several tools and features for further security controls and monitoring: Central firewall management and visibility in AWS Firewall Manager . You can centrally manage security policies and automatically enforce mandatory security policies across existing and newly created accounts and VPCs. Network Firewall logging for the firewall\u2019s stateful engine. You can record flow and alert logs, and use the same or different logging destinations for each log type. Stateless rules to filter network traffic based on protocol, source IP addresses, ranges, source port ranges, destination IP addresses and ranges, and TCP flags. Integration into a broader set of AWS security components. For an example, see Automatically block suspicious traffic with AWS Network Firewall and Amazon GuardDuty. Integration in a diverse ecosystem of Network Firewall Partners that complement Network Firewall, enabling the deployment of a comprehensive security architecture. For example use cases, see Full VPC traffic visibility with AWS Network Firewall and Sumo Logic and Splunk Named Launch Partner of AWS Network Firewall.","title":"Additional security controls with Network Firewall"},{"location":"sagemaker/secure-sagemaker-network-firewall-summary/#build-secure-ml-environments","text":"A robust security design normally includes multi-layer security controls for the system. For SageMaker environments and workloads, you can use the following AWS security services and concepts to secure, control, and monitor your environment: VPC and private subnets to perform secure API calls to other AWS services and restrict internet access for downloading packages. S3 bucket policies that restrict access to specific VPC endpoints. Encryption of ML model artifacts and other system artifacts that are either in transit or at rest. Requests to the SageMaker API and console are made over a Secure Sockets Layer (SSL) connection. Restricted IAM roles and policies for SageMaker runs and notebook access based on resource tags and project ID. Restricted access to Amazon public services, such as Amazon Elastic Container Registry (Amazon ECR) to VPC endpoints only. For a reference deployment architecture and ready-to-use deployable constructs for your environment, see Amazon SageMaker with Guardrails on AWS.","title":"Build secure ML environments"},{"location":"sagemaker/secure-sagemaker-network-firewall-summary/#conclusion","text":"In this post, we showed how you can secure, log, and monitor internet ingress and egress traffic in Studio notebooks for your sensitive ML workloads using managed Network Firewall. You can use the provided CloudFormation templates to automate SageMaker deployment as part of your Infrastructure as Code (IaC) strategy. For more information about other possibilities to secure your SageMaker deployments and ML workloads, see Building secure machine learning environments with Amazon SageMaker.","title":"Conclusion"},{"location":"sagemaker/secure-sagemaker-network-firewall/","text":"Securing Amazon SageMaker Studio internet traffic using AWS Network Firewall Full github repo with code The work in this document complements previous work: - Building secure machine learning environments with Amazon SageMaker Securing Amazon SageMaker Studio connectivity using a private VPC. Background Amazon SageMaker Studio is a web-based fully integrated development environment (IDE) where you can perform end-to-end machine learning (ML) development to prepare data and build, train, and deploy models. Like other AWS services, Studio supports a rich set of security-related features that allow you to build highly secure and compliant environments. One of these fundamental security features allows you to launch Studio in your own Amazon Virtual Private Cloud (Amazon VPC). This allows you to control, monitor, and inspect network traffic within and outside your VPC using standard AWS networking and security capabilities. For more information, see Securing Amazon SageMaker Studio connectivity using a private VPC. Customers in regulated industries, such as financial services, often don\u2019t allow any internet access in ML environments. They often use only VPC endpoints for AWS services, and connect only to private source code repositories in which all libraries have been vetted both in terms of security and licensing. Customers may want to provide internet access but also have some controls such as domain name or URL filtering and allow access to only specific public repositories and websites, possibly packet inspection, or other network traffic-related security controls. For these cases, AWS Network Firewall and NAT gateway-based deployment may provide a suitable use case. In this post, we show how you can use Network Firewall to build a secure and compliant environment by restricting and monitoring internet access, inspecting traffic, and using stateless and stateful firewall engine rules to control the network flow between Studio notebooks and the internet. Depending on your security, compliance, and governance rules, you may not need to or cannot completely block internet access from Studio and your AI and ML workloads. You may have requirements beyond the scope of network security controls implemented by security groups and network access control lists (ACLs), such as application protocol protection, deep packet inspection, domain name filtering, and intrusion prevention system (IPS). Your network traffic controls may also require many more rules compared to what is currently supported in security groups and network ACLs. In these scenarios, you can use Network Firewall\u2014a managed network firewall and IPS for your VPC. Solution overview When you deploy Studio in your VPC, you control how Studio accesses the internet with the parameter AppNetworkAccessType (via the Amazon SageMaker API ) or by selecting your preference on the console when you create a Studio domain. If you select Public internet Only ( PublicInternetOnly ), all the ingress and egress internet traffic from Amazon SageMaker notebooks flows through an AWS managed internet gateway attached to a VPC in your SageMaker account. The following diagram shows this network configuration. Studio provides public internet egress through a platform-managed VPC for data scientists to download notebooks, packages, and datasets. Traffic to the attached Amazon Elastic File System (Amazon EFS) volume always goes through the customer VPC and never through the public internet egress. To use your own control flow for the internet traffic, like a NAT or internet gateway, you must set the AppNetworkAccessType parameter to VpcOnly (or select VPC Only on the console). When you launch your app, this creates an elastic network interface in the specified subnets in your VPC. You can apply all available layers of security control\u2014 security groups , network ACLs , VPC endpoints , AWS PrivateLink , or Network Firewall endpoints \u2014to the internal network and internet traffic to exercise fine-grained control of network access in Studio. The following diagram shows the VpcOnly network configuration. In this mode, the direct internet access to or from notebooks is completely disabled, and all traffic is routed through an elastic network interface in your private VPC. This also includes traffic from Studio UI widgets and interfaces, such as Experiments , Autopilot , and Model Monitor , to their respective backend SageMaker APIs. For more information about network access parameters when creating a domain, see CreateDomain . The solution in this post uses the VpcOnly option and deploys the Studio domain into a VPC with three subnets: SageMaker subnet \u2013 Hosts all Studio workloads. All ingress and egress network flow is controlled by a security group. NAT subnet \u2013 Contains a NAT gateway. We use the NAT gateway to access the internet without exposing any private IP addresses from our private network. Network Firewall subnet \u2013 Contains a Network Firewall endpoint. The route tables are configured so that all inbound and outbound external network traffic is routed via Network Firewall. You can configure stateful and stateless Network Firewall policies to inspect, monitor, and control the traffic. The following diagram shows the overview of the solution architecture and the deployed components. VPC resources The solution deploys the following resources in your account: A VPC with a specified Classless Inter-Domain Routing (CIDR) block Three private subnets with specified CIDRs Internet gateway, NAT gateway, Network Firewall, and a Network Firewall endpoint in the Network Firewall subnet A Network Firewall policy and stateful domain list group with an allow domain list Elastic IP allocated to the NAT gateway Two security groups for SageMaker workloads and VPC endpoints, respectively Four route tables with configured routes An Amazon S3 VPC endpoint (type Gateway) AWS service access VPC endpoints (type Interface) for various AWS services that need to be accessed from Studio The solution also creates an AWS Identity and Access Management (IAM) execution role for SageMaker notebooks and Studio with preconfigured IAM policies. Network routing for targets outside the VPC is configured in such a way that all ingress and egress internet traffic goes via the Network Firewall and NAT gateway. For details and reference network architectures with Network Firewall and NAT gateway, see Architecture with an internet gateway and a NAT gateway , Deployment models for AWS Network Firewall , and Enforce your AWS Network Firewall protections at scale with AWS Firewall Manager . The AWS re:Invent 2020 video Which inspection architecture is right for you? discusses which inspection architecture is right for your use case. SageMaker resources The solution creates a SageMaker domain and user profile. The solution uses only one Availability Zone and is not highly available. A best practice is to use a Multi-AZ configuration for any production deployment. You can implement the highly available solution by duplicating the Single-AZ setup\u2014subnets, NAT gateway, and Network Firewall endpoints\u2014to additional Availability Zones. You use Network Firewall and its policies to control entry and exit of the internet traffic in your VPC. You create an allow domain list rule to allow internet access to the specified network domains only and block traffic to any domain not on the allow list. AWS CloudFormation resources The source code and AWS CloudFormation template for solution deployment are provided in the GitHub repository . To deploy the solution on your account, you need: An AWS account and the AWS Command Line Interface (AWS CLI) configured with administrator permissions An Amazon Simple Storage Service (Amazon S3) bucket in your account in the same Region where you deploy the solution Network Firewall is a Regional service; for more information on Region availability, see the AWS Region Table . Your CloudFormation stack doesn\u2019t have any required parameters. You may want to change the DomainName or *CIDR parameters to avoid naming conflicts with the existing resources and your VPC CIDR allocations. Otherwise, use the following default values: ProjectName \u2013 sagemaker-studio-vpc-firewall DomainName \u2013 sagemaker-anfw-domain UserProfileName \u2013 anfw-user-profile VPCCIDR \u2013 10.2.0.0/16 FirewallSubnetCIDR \u2013 10.2.1.0/24 NATGatewaySubnetCIDR \u2013 10.2.2.0/24 SageMakerStudioSubnetCIDR \u2013 10.2.3.0/24 Deploy the CloudFormation template To start experimenting with the Network Firewall and stateful rules, you need first to deploy the provided CloudFormation template to your AWS account. Clone the GitHub repository: git clone https://github.com/aws-samples/amazon-sagemaker-studio-vpc-networkfirewall.git cd amazon-sagemaker-studio-vpc-networkfirewall Create an S3 bucket in the Region where you deploy the solution: aws s3 mb s3://<your s3 bucket name> You can skip this step if you already have an S3 bucket. Deploy the CloudFormation stack: make deploy CFN_ARTEFACT_S3_BUCKET=<your s3 bucket name> The deployment procedure packages the CloudFormation template and copies it to the S3 bucket your provided. Then the CloudFormation template is deployed from the S3 bucket to your AWS account. The stack deploys all the needed resources like VPC, network devices, route tables, security groups, S3 buckets, IAM policies and roles, and VPC endpoints, and also creates a new Studio domain and user profile. When the deployment is complete, you can see the full list of stack output values by running the following command in terminal: aws cloudformation describe-stacks \\ --stack-name sagemaker-studio-demo \\ --output table \\ --query \"Stacks[0].Outputs[*].[OutputKey, OutputValue]\" Launch Studio via the SageMaker console. Experiment with Network Firewall Now you can learn how to control the internet inbound and outbound access with Network Firewall. In this section, we discuss the initial setup, accessing resources not on the allow list, adding domains to the allow list, configuring logging, and additional firewall rules. Initial setup The solution deploys a Network Firewall policy with a stateful rule group with an allow domain list. This policy is attached to the Network Firewall. All inbound and outbound internet traffic is blocked now, except for the .kaggle.com domain, which is on the allow list. Let\u2019s try to access https://kaggle.com by opening a new notebook in Studio and attempting to download the front page from kaggle.com : !wget https://kaggle.com The following screenshot shows that the request succeeds because the domain is allowed by the firewall policy. Users can connect to this and only to this domain from any Studio notebook. Access resources not on the allowed domain list In the Studio notebook, try to clone any public GitHub repository, such as the following: !git clone https://github.com/aws-samples/amazon-sagemaker-studio-vpc-networkfirewall.git This operation times out after 5 minutes because any internet traffic except to and from the .kaggle.com domain isn\u2019t allowed and is dropped by Network Firewall. Add a domain to the allowed domain list To be able to run the git clone command, you must allow internet traffic to the .github.com domain. On the Amazon VPC console, choose Firewall policies. Choose the policy network-firewall-policy- . In the Stateful rule groups section, select the group rule domain-allow-sagemaker- . You can see the domain .kaggle.com on the allow list. Choose Add domain. Enter .github.com . Choose Save. You now have two names on the allow domain list. Firewall policy is propagated in real time to Network Firewall and your changes take effect immediately. Any inbound or outbound traffic from or to these domains is now allowed by the firewall and all other traffic is dropped. To validate the new configuration, go to your Studio notebook and try to clone the same GitHub repository again: !git clone https://github.com/aws-samples/amazon-sagemaker-studio-vpc-networkfirewall.git The operation succeeds this time\u2014Network Firewall allows access to the .github.com domain. Network Firewall logging In this section, you configure Network Firewall logging for your firewall\u2019s stateful engine. Logging gives you detailed information about network traffic, including the time that the stateful engine received a packet, detailed information about the packet, and any stateful rule action taken against the packet. The logs are published to the log destination that you configured, where you can retrieve and view them. On the Amazon VPC console, choose Firewalls . Choose your firewall. Choose the Firewall details tab. In the Logging section, choose Edit . Configure your firewall logging by selecting what log types you want to capture and providing the log destination. For this post, select Alert log type, set Log destination for alerts to CloudWatch Log group, and provide an existing or a new log group where the firewall logs are delivered. Choose Save . To check your settings, go back to Studio and try to access pypi.org to install a Python package: !pip install -U scikit-learn This command fails with ReadTimeoutError because Network Firewall drops any traffic to any domain not on the allow list (which contains only two domains: .github.com and .kaggle.com ). On the Amazon CloudWatch console , navigate to the log group and browse through the recent log streams. The pipy.org domain shows the blocked action. The log event also provides additional details such as various timestamps, protocol, port and IP details, event type, availability zone, and the firewall name. You can continue experimenting with Network Firewall by adding .pypi.org and .pythonhosted.org domains to the allowed domain list. Then validate your access to them via your Studio notebook. Additional firewall rules You can create any other stateless or stateful firewall rules and implement traffic filtering based on a standard stateful 5-tuple rule for network traffic inspection (protocol, source IP, source port, destination IP, destination port). Network Firewall also supports industry standard stateful Suricata compatible IPS rule groups. You can implement protocol-based rules to detect and block any non-standard or promiscuous usage or activity. For more information about creating and managing Network Firewall rule groups, see Rule groups in AWS Network Firewall. Additional security controls with Network Firewall In the previous section, we looked at one feature of the Network Firewall: filtering network traffic based on the domain name. In addition to stateless or stateful firewall rules, Network Firewall provides several tools and features for further security controls and monitoring: Central firewall management and visibility in AWS Firewall Manager . You can centrally manage security policies and automatically enforce mandatory security policies across existing and newly created accounts and VPCs. Network Firewall logging for the firewall\u2019s stateful engine. You can record flow and alert logs, and use the same or different logging destinations for each log type. Stateless rules to filter network traffic based on protocol, source IP addresses, ranges, source port ranges, destination IP addresses and ranges, and TCP flags. Integration into a broader set of AWS security components. For an example, see Automatically block suspicious traffic with AWS Network Firewall and Amazon GuardDuty. Integration in a diverse ecosystem of Network Firewall Partners that complement Network Firewall, enabling the deployment of a comprehensive security architecture. For example use cases, see Full VPC traffic visibility with AWS Network Firewall and Sumo Logic and Splunk Named Launch Partner of AWS Network Firewall. Build secure ML environments A robust security design normally includes multi-layer security controls for the system. For SageMaker environments and workloads, you can use the following AWS security services and concepts to secure, control, and monitor your environment: VPC and private subnets to perform secure API calls to other AWS services and restrict internet access for downloading packages. S3 bucket policies that restrict access to specific VPC endpoints. Encryption of ML model artifacts and other system artifacts that are either in transit or at rest. Requests to the SageMaker API and console are made over a Secure Sockets Layer (SSL) connection. Restricted IAM roles and policies for SageMaker runs and notebook access based on resource tags and project ID. Restricted access to Amazon public services, such as Amazon Elastic Container Registry (Amazon ECR) to VPC endpoints only. For a reference deployment architecture and ready-to-use deployable constructs for your environment, see Amazon SageMaker with Guardrails on AWS. Conclusion In this post, we showed how you can secure, log, and monitor internet ingress and egress traffic in Studio notebooks for your sensitive ML workloads using managed Network Firewall. You can use the provided CloudFormation templates to automate SageMaker deployment as part of your Infrastructure as Code (IaC) strategy. For more information about other possibilities to secure your SageMaker deployments and ML workloads, see Building secure machine learning environments with Amazon SageMaker.","title":"Secure Network"},{"location":"sagemaker/secure-sagemaker-network-firewall/#securing-amazon-sagemaker-studio-internet-traffic-using-aws-network-firewall","text":"Full github repo with code The work in this document complements previous work: - Building secure machine learning environments with Amazon SageMaker Securing Amazon SageMaker Studio connectivity using a private VPC.","title":"Securing Amazon SageMaker Studio internet traffic using AWS Network Firewall"},{"location":"sagemaker/secure-sagemaker-network-firewall/#background","text":"Amazon SageMaker Studio is a web-based fully integrated development environment (IDE) where you can perform end-to-end machine learning (ML) development to prepare data and build, train, and deploy models. Like other AWS services, Studio supports a rich set of security-related features that allow you to build highly secure and compliant environments. One of these fundamental security features allows you to launch Studio in your own Amazon Virtual Private Cloud (Amazon VPC). This allows you to control, monitor, and inspect network traffic within and outside your VPC using standard AWS networking and security capabilities. For more information, see Securing Amazon SageMaker Studio connectivity using a private VPC. Customers in regulated industries, such as financial services, often don\u2019t allow any internet access in ML environments. They often use only VPC endpoints for AWS services, and connect only to private source code repositories in which all libraries have been vetted both in terms of security and licensing. Customers may want to provide internet access but also have some controls such as domain name or URL filtering and allow access to only specific public repositories and websites, possibly packet inspection, or other network traffic-related security controls. For these cases, AWS Network Firewall and NAT gateway-based deployment may provide a suitable use case. In this post, we show how you can use Network Firewall to build a secure and compliant environment by restricting and monitoring internet access, inspecting traffic, and using stateless and stateful firewall engine rules to control the network flow between Studio notebooks and the internet. Depending on your security, compliance, and governance rules, you may not need to or cannot completely block internet access from Studio and your AI and ML workloads. You may have requirements beyond the scope of network security controls implemented by security groups and network access control lists (ACLs), such as application protocol protection, deep packet inspection, domain name filtering, and intrusion prevention system (IPS). Your network traffic controls may also require many more rules compared to what is currently supported in security groups and network ACLs. In these scenarios, you can use Network Firewall\u2014a managed network firewall and IPS for your VPC.","title":"Background"},{"location":"sagemaker/secure-sagemaker-network-firewall/#solution-overview","text":"When you deploy Studio in your VPC, you control how Studio accesses the internet with the parameter AppNetworkAccessType (via the Amazon SageMaker API ) or by selecting your preference on the console when you create a Studio domain. If you select Public internet Only ( PublicInternetOnly ), all the ingress and egress internet traffic from Amazon SageMaker notebooks flows through an AWS managed internet gateway attached to a VPC in your SageMaker account. The following diagram shows this network configuration. Studio provides public internet egress through a platform-managed VPC for data scientists to download notebooks, packages, and datasets. Traffic to the attached Amazon Elastic File System (Amazon EFS) volume always goes through the customer VPC and never through the public internet egress. To use your own control flow for the internet traffic, like a NAT or internet gateway, you must set the AppNetworkAccessType parameter to VpcOnly (or select VPC Only on the console). When you launch your app, this creates an elastic network interface in the specified subnets in your VPC. You can apply all available layers of security control\u2014 security groups , network ACLs , VPC endpoints , AWS PrivateLink , or Network Firewall endpoints \u2014to the internal network and internet traffic to exercise fine-grained control of network access in Studio. The following diagram shows the VpcOnly network configuration. In this mode, the direct internet access to or from notebooks is completely disabled, and all traffic is routed through an elastic network interface in your private VPC. This also includes traffic from Studio UI widgets and interfaces, such as Experiments , Autopilot , and Model Monitor , to their respective backend SageMaker APIs. For more information about network access parameters when creating a domain, see CreateDomain . The solution in this post uses the VpcOnly option and deploys the Studio domain into a VPC with three subnets: SageMaker subnet \u2013 Hosts all Studio workloads. All ingress and egress network flow is controlled by a security group. NAT subnet \u2013 Contains a NAT gateway. We use the NAT gateway to access the internet without exposing any private IP addresses from our private network. Network Firewall subnet \u2013 Contains a Network Firewall endpoint. The route tables are configured so that all inbound and outbound external network traffic is routed via Network Firewall. You can configure stateful and stateless Network Firewall policies to inspect, monitor, and control the traffic. The following diagram shows the overview of the solution architecture and the deployed components.","title":"Solution overview"},{"location":"sagemaker/secure-sagemaker-network-firewall/#vpc-resources","text":"The solution deploys the following resources in your account: A VPC with a specified Classless Inter-Domain Routing (CIDR) block Three private subnets with specified CIDRs Internet gateway, NAT gateway, Network Firewall, and a Network Firewall endpoint in the Network Firewall subnet A Network Firewall policy and stateful domain list group with an allow domain list Elastic IP allocated to the NAT gateway Two security groups for SageMaker workloads and VPC endpoints, respectively Four route tables with configured routes An Amazon S3 VPC endpoint (type Gateway) AWS service access VPC endpoints (type Interface) for various AWS services that need to be accessed from Studio The solution also creates an AWS Identity and Access Management (IAM) execution role for SageMaker notebooks and Studio with preconfigured IAM policies. Network routing for targets outside the VPC is configured in such a way that all ingress and egress internet traffic goes via the Network Firewall and NAT gateway. For details and reference network architectures with Network Firewall and NAT gateway, see Architecture with an internet gateway and a NAT gateway , Deployment models for AWS Network Firewall , and Enforce your AWS Network Firewall protections at scale with AWS Firewall Manager . The AWS re:Invent 2020 video Which inspection architecture is right for you? discusses which inspection architecture is right for your use case.","title":"VPC resources"},{"location":"sagemaker/secure-sagemaker-network-firewall/#sagemaker-resources","text":"The solution creates a SageMaker domain and user profile. The solution uses only one Availability Zone and is not highly available. A best practice is to use a Multi-AZ configuration for any production deployment. You can implement the highly available solution by duplicating the Single-AZ setup\u2014subnets, NAT gateway, and Network Firewall endpoints\u2014to additional Availability Zones. You use Network Firewall and its policies to control entry and exit of the internet traffic in your VPC. You create an allow domain list rule to allow internet access to the specified network domains only and block traffic to any domain not on the allow list.","title":"SageMaker resources"},{"location":"sagemaker/secure-sagemaker-network-firewall/#aws-cloudformation-resources","text":"The source code and AWS CloudFormation template for solution deployment are provided in the GitHub repository . To deploy the solution on your account, you need: An AWS account and the AWS Command Line Interface (AWS CLI) configured with administrator permissions An Amazon Simple Storage Service (Amazon S3) bucket in your account in the same Region where you deploy the solution Network Firewall is a Regional service; for more information on Region availability, see the AWS Region Table . Your CloudFormation stack doesn\u2019t have any required parameters. You may want to change the DomainName or *CIDR parameters to avoid naming conflicts with the existing resources and your VPC CIDR allocations. Otherwise, use the following default values: ProjectName \u2013 sagemaker-studio-vpc-firewall DomainName \u2013 sagemaker-anfw-domain UserProfileName \u2013 anfw-user-profile VPCCIDR \u2013 10.2.0.0/16 FirewallSubnetCIDR \u2013 10.2.1.0/24 NATGatewaySubnetCIDR \u2013 10.2.2.0/24 SageMakerStudioSubnetCIDR \u2013 10.2.3.0/24 Deploy the CloudFormation template To start experimenting with the Network Firewall and stateful rules, you need first to deploy the provided CloudFormation template to your AWS account. Clone the GitHub repository: git clone https://github.com/aws-samples/amazon-sagemaker-studio-vpc-networkfirewall.git cd amazon-sagemaker-studio-vpc-networkfirewall Create an S3 bucket in the Region where you deploy the solution: aws s3 mb s3://<your s3 bucket name> You can skip this step if you already have an S3 bucket. Deploy the CloudFormation stack: make deploy CFN_ARTEFACT_S3_BUCKET=<your s3 bucket name> The deployment procedure packages the CloudFormation template and copies it to the S3 bucket your provided. Then the CloudFormation template is deployed from the S3 bucket to your AWS account. The stack deploys all the needed resources like VPC, network devices, route tables, security groups, S3 buckets, IAM policies and roles, and VPC endpoints, and also creates a new Studio domain and user profile. When the deployment is complete, you can see the full list of stack output values by running the following command in terminal: aws cloudformation describe-stacks \\ --stack-name sagemaker-studio-demo \\ --output table \\ --query \"Stacks[0].Outputs[*].[OutputKey, OutputValue]\" Launch Studio via the SageMaker console.","title":"AWS CloudFormation resources"},{"location":"sagemaker/secure-sagemaker-network-firewall/#experiment-with-network-firewall","text":"Now you can learn how to control the internet inbound and outbound access with Network Firewall. In this section, we discuss the initial setup, accessing resources not on the allow list, adding domains to the allow list, configuring logging, and additional firewall rules.","title":"Experiment with Network Firewall"},{"location":"sagemaker/secure-sagemaker-network-firewall/#initial-setup","text":"The solution deploys a Network Firewall policy with a stateful rule group with an allow domain list. This policy is attached to the Network Firewall. All inbound and outbound internet traffic is blocked now, except for the .kaggle.com domain, which is on the allow list. Let\u2019s try to access https://kaggle.com by opening a new notebook in Studio and attempting to download the front page from kaggle.com : !wget https://kaggle.com The following screenshot shows that the request succeeds because the domain is allowed by the firewall policy. Users can connect to this and only to this domain from any Studio notebook.","title":"Initial setup"},{"location":"sagemaker/secure-sagemaker-network-firewall/#access-resources-not-on-the-allowed-domain-list","text":"In the Studio notebook, try to clone any public GitHub repository, such as the following: !git clone https://github.com/aws-samples/amazon-sagemaker-studio-vpc-networkfirewall.git This operation times out after 5 minutes because any internet traffic except to and from the .kaggle.com domain isn\u2019t allowed and is dropped by Network Firewall.","title":"Access resources not on the allowed domain list"},{"location":"sagemaker/secure-sagemaker-network-firewall/#add-a-domain-to-the-allowed-domain-list","text":"To be able to run the git clone command, you must allow internet traffic to the .github.com domain. On the Amazon VPC console, choose Firewall policies. Choose the policy network-firewall-policy- . In the Stateful rule groups section, select the group rule domain-allow-sagemaker- . You can see the domain .kaggle.com on the allow list. Choose Add domain. Enter .github.com . Choose Save. You now have two names on the allow domain list. Firewall policy is propagated in real time to Network Firewall and your changes take effect immediately. Any inbound or outbound traffic from or to these domains is now allowed by the firewall and all other traffic is dropped. To validate the new configuration, go to your Studio notebook and try to clone the same GitHub repository again: !git clone https://github.com/aws-samples/amazon-sagemaker-studio-vpc-networkfirewall.git The operation succeeds this time\u2014Network Firewall allows access to the .github.com domain.","title":"Add a domain to the allowed domain list"},{"location":"sagemaker/secure-sagemaker-network-firewall/#network-firewall-logging","text":"In this section, you configure Network Firewall logging for your firewall\u2019s stateful engine. Logging gives you detailed information about network traffic, including the time that the stateful engine received a packet, detailed information about the packet, and any stateful rule action taken against the packet. The logs are published to the log destination that you configured, where you can retrieve and view them. On the Amazon VPC console, choose Firewalls . Choose your firewall. Choose the Firewall details tab. In the Logging section, choose Edit . Configure your firewall logging by selecting what log types you want to capture and providing the log destination. For this post, select Alert log type, set Log destination for alerts to CloudWatch Log group, and provide an existing or a new log group where the firewall logs are delivered. Choose Save . To check your settings, go back to Studio and try to access pypi.org to install a Python package: !pip install -U scikit-learn This command fails with ReadTimeoutError because Network Firewall drops any traffic to any domain not on the allow list (which contains only two domains: .github.com and .kaggle.com ). On the Amazon CloudWatch console , navigate to the log group and browse through the recent log streams. The pipy.org domain shows the blocked action. The log event also provides additional details such as various timestamps, protocol, port and IP details, event type, availability zone, and the firewall name. You can continue experimenting with Network Firewall by adding .pypi.org and .pythonhosted.org domains to the allowed domain list. Then validate your access to them via your Studio notebook.","title":"Network Firewall logging"},{"location":"sagemaker/secure-sagemaker-network-firewall/#additional-firewall-rules","text":"You can create any other stateless or stateful firewall rules and implement traffic filtering based on a standard stateful 5-tuple rule for network traffic inspection (protocol, source IP, source port, destination IP, destination port). Network Firewall also supports industry standard stateful Suricata compatible IPS rule groups. You can implement protocol-based rules to detect and block any non-standard or promiscuous usage or activity. For more information about creating and managing Network Firewall rule groups, see Rule groups in AWS Network Firewall.","title":"Additional firewall rules"},{"location":"sagemaker/secure-sagemaker-network-firewall/#additional-security-controls-with-network-firewall","text":"In the previous section, we looked at one feature of the Network Firewall: filtering network traffic based on the domain name. In addition to stateless or stateful firewall rules, Network Firewall provides several tools and features for further security controls and monitoring: Central firewall management and visibility in AWS Firewall Manager . You can centrally manage security policies and automatically enforce mandatory security policies across existing and newly created accounts and VPCs. Network Firewall logging for the firewall\u2019s stateful engine. You can record flow and alert logs, and use the same or different logging destinations for each log type. Stateless rules to filter network traffic based on protocol, source IP addresses, ranges, source port ranges, destination IP addresses and ranges, and TCP flags. Integration into a broader set of AWS security components. For an example, see Automatically block suspicious traffic with AWS Network Firewall and Amazon GuardDuty. Integration in a diverse ecosystem of Network Firewall Partners that complement Network Firewall, enabling the deployment of a comprehensive security architecture. For example use cases, see Full VPC traffic visibility with AWS Network Firewall and Sumo Logic and Splunk Named Launch Partner of AWS Network Firewall.","title":"Additional security controls with Network Firewall"},{"location":"sagemaker/secure-sagemaker-network-firewall/#build-secure-ml-environments","text":"A robust security design normally includes multi-layer security controls for the system. For SageMaker environments and workloads, you can use the following AWS security services and concepts to secure, control, and monitor your environment: VPC and private subnets to perform secure API calls to other AWS services and restrict internet access for downloading packages. S3 bucket policies that restrict access to specific VPC endpoints. Encryption of ML model artifacts and other system artifacts that are either in transit or at rest. Requests to the SageMaker API and console are made over a Secure Sockets Layer (SSL) connection. Restricted IAM roles and policies for SageMaker runs and notebook access based on resource tags and project ID. Restricted access to Amazon public services, such as Amazon Elastic Container Registry (Amazon ECR) to VPC endpoints only. For a reference deployment architecture and ready-to-use deployable constructs for your environment, see Amazon SageMaker with Guardrails on AWS.","title":"Build secure ML environments"},{"location":"sagemaker/secure-sagemaker-network-firewall/#conclusion","text":"In this post, we showed how you can secure, log, and monitor internet ingress and egress traffic in Studio notebooks for your sensitive ML workloads using managed Network Firewall. You can use the provided CloudFormation templates to automate SageMaker deployment as part of your Infrastructure as Code (IaC) strategy. For more information about other possibilities to secure your SageMaker deployments and ML workloads, see Building secure machine learning environments with Amazon SageMaker.","title":"Conclusion"}]}