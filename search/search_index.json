{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"So much emptyness","title":"Home"},{"location":"aws/IAM_summary/access_control/","text":"Access Control \u00b6 question A company is configuring IAM for its new AWS account. There are 5 departments with between 5 to 10 users in each department. How can they efficiently apply access permissions for each of these departments and simplify management of these users? answer Create policies for each department that define the permissions needed. Create an IAM group for each department and attach the policy to each group. Add each department's members to their respective IAM group. By creating an IAM group, all like users can be managed all at one time. Once the permissions are defined within the policy, it can be attached to the IAM group, allowing them access to the resources/services stated within the policy.","title":"Access Control"},{"location":"aws/IAM_summary/access_control/#access-control","text":"question A company is configuring IAM for its new AWS account. There are 5 departments with between 5 to 10 users in each department. How can they efficiently apply access permissions for each of these departments and simplify management of these users? answer Create policies for each department that define the permissions needed. Create an IAM group for each department and attach the policy to each group. Add each department's members to their respective IAM group. By creating an IAM group, all like users can be managed all at one time. Once the permissions are defined within the policy, it can be attached to the IAM group, allowing them access to the resources/services stated within the policy.","title":"Access Control"},{"location":"aws/IAM_summary/controlling_user_actions_with_IAM_policy_documents/","text":"Controlling User Actions with IAM Policy Documents \u00b6 Determining whether a request is allowed or denied within an account Overview of JSON policies AWS IAM FAQs TODO write policy for S3 { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : \"*\" , \"Resource\" : \"*\" } ] } End \u00b6","title":"Controlling User Actions with IAM Policy Documents"},{"location":"aws/IAM_summary/controlling_user_actions_with_IAM_policy_documents/#controlling-user-actions-with-iam-policy-documents","text":"Determining whether a request is allowed or denied within an account Overview of JSON policies AWS IAM FAQs TODO write policy for S3 { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : \"*\" , \"Resource\" : \"*\" } ] }","title":"Controlling User Actions with IAM Policy Documents"},{"location":"aws/IAM_summary/controlling_user_actions_with_IAM_policy_documents/#end","text":"","title":"End"},{"location":"aws/IAM_summary/iam_overview/","text":"IAM overview \u00b6 IAM is used to create and manage access keys for users that need to access AWS services from the AWS Command Line Interface (CLI) IAM policies are written using JSON. Identities define who can access resources Access define what resources they can access Best Practices \u00b6 Create individual users instead of using root. Enable MFA for privileged users IAM users \u00b6 Create individual IAM users. You should always create IAM users for individual users, since the root account should never be used for actual work. https://docs.aws.amazon.com/IAM/latest/UserGuide/id_root-user.html https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-user.html IAM User valid access types \u00b6 AWS Software Develeopers Kit Programmatic access via the command line AWS Management Console access IAM Groups \u00b6 Use groups to delegate access to IAM users. Groups should be used to delegate permissions to the users you create, instead of individual assigning policies to IAM users, since it makes administration easier. https://docs.aws.amazon.com/IAM/latest/UserGuide/id_root-user.html https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-user.html IAM Roles \u00b6 Roles define access permissions and are temporarily assumed by an IAM user or service. The recommended method to assign permissions to apps running in EC2 is to use IAM roles. https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html IAM Roles - Use Cases \u00b6 Assume a role to perform a task in a single session Assumed by any user or service that needs it Access is assigned using policies Grant users in one AWS account to a resource in another AWS account Give your applications running in EC2 permission to other AWS resources. IAM Credential Report \u00b6 IAM provides a downloadable credential report that lists all users in your account and the status of their various credentials, including passwords, access keys, and MFA devices. passwords access keys MFA devices Root Account \u00b6 Activate Multi-factor Authentication (MFA) on your root account. The root account should have MFA enabled, due to its unlimited access to an account. https://docs.aws.amazon.com/IAM/latest/UserGuide/id_root-user.html https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-user.html End \u00b6","title":"IAM overview"},{"location":"aws/IAM_summary/iam_overview/#iam-overview","text":"IAM is used to create and manage access keys for users that need to access AWS services from the AWS Command Line Interface (CLI) IAM policies are written using JSON. Identities define who can access resources Access define what resources they can access","title":"IAM overview"},{"location":"aws/IAM_summary/iam_overview/#best-practices","text":"Create individual users instead of using root. Enable MFA for privileged users","title":"Best Practices"},{"location":"aws/IAM_summary/iam_overview/#iam-users","text":"Create individual IAM users. You should always create IAM users for individual users, since the root account should never be used for actual work. https://docs.aws.amazon.com/IAM/latest/UserGuide/id_root-user.html https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-user.html","title":"IAM users"},{"location":"aws/IAM_summary/iam_overview/#iam-user-valid-access-types","text":"AWS Software Develeopers Kit Programmatic access via the command line AWS Management Console access","title":"IAM User valid access types"},{"location":"aws/IAM_summary/iam_overview/#iam-groups","text":"Use groups to delegate access to IAM users. Groups should be used to delegate permissions to the users you create, instead of individual assigning policies to IAM users, since it makes administration easier. https://docs.aws.amazon.com/IAM/latest/UserGuide/id_root-user.html https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-user.html","title":"IAM Groups"},{"location":"aws/IAM_summary/iam_overview/#iam-roles","text":"Roles define access permissions and are temporarily assumed by an IAM user or service. The recommended method to assign permissions to apps running in EC2 is to use IAM roles. https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html","title":"IAM Roles"},{"location":"aws/IAM_summary/iam_overview/#iam-roles-use-cases","text":"Assume a role to perform a task in a single session Assumed by any user or service that needs it Access is assigned using policies Grant users in one AWS account to a resource in another AWS account Give your applications running in EC2 permission to other AWS resources.","title":"IAM Roles - Use Cases"},{"location":"aws/IAM_summary/iam_overview/#iam-credential-report","text":"IAM provides a downloadable credential report that lists all users in your account and the status of their various credentials, including passwords, access keys, and MFA devices. passwords access keys MFA devices","title":"IAM Credential Report"},{"location":"aws/IAM_summary/iam_overview/#root-account","text":"Activate Multi-factor Authentication (MFA) on your root account. The root account should have MFA enabled, due to its unlimited access to an account. https://docs.aws.amazon.com/IAM/latest/UserGuide/id_root-user.html https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-user.html","title":"Root Account"},{"location":"aws/IAM_summary/iam_overview/#end","text":"","title":"End"},{"location":"aws/VPC_summary/IP_and_CIDR/","text":"VPC IP range and CIDR Blocks \u00b6 CIDR Wikipedia When you create a VPC, AWS asks you to associate a CIDR block with the VPC. CIDR is an acronym that stands for Classless Inter-Domain Routing . In simpler terms, a CIDR block is an IP address range. A VPC can accommodate two CIDR blocks, one for IPv4 and another for IPv6, but for the sake of simplicity I am going to limit my discussion to IPv4. When you create a CIDR block, you must enter it as an IP address range. In the case of an IPv4 CIDR, this means entering a network prefix and a subnet mask. The subnet mask determines how many IP addresses can be created from the CIDR block. Amazon requires that a CIDR block include a subnet mask ranging from 16 to 28. The two most commonly used subnet sizes are 16 bits and 24 bits. If you were to create a CIDR block with a 16-bit subnet, then the network portion of the IP address would contain two eight-bit numbers, followed by two zeros, each separated by periods. Here is an example of a CIDR block with a 16-bit subnet: 10.10.0.0/16 . This block would allow for the creation of up to 65,536 IP addresses. Each address would start with 10.10 , but you can enter any value between 0 and 255 into the last two positions. A CIDR block with a 24-bit address would contain three eight-bit numbers, followed by a single 0. Here is an example of what such a block would look like: 10.10.10.0/24 . This block could accommodate up to 256 IP addresses. The first three octets ( 10.10.10 ) would be common to each address, but the last digit can be populated with a number ranging between 0 and 255. IP address range \u00b6 0.0.0.0/0 means from the whole Internet 192.168.1.1/32 means only from 192.168.1.1 CIDR block 16 bit 24 bit x.x.x.x/16 x.x.x.x/24 IP addresses 65,536 256 CIDR block common octets 10.10.0.0 10.10.10.0 End \u00b6","title":"VPC IP range and CIDR Blocks"},{"location":"aws/VPC_summary/IP_and_CIDR/#vpc-ip-range-and-cidr-blocks","text":"CIDR Wikipedia When you create a VPC, AWS asks you to associate a CIDR block with the VPC. CIDR is an acronym that stands for Classless Inter-Domain Routing . In simpler terms, a CIDR block is an IP address range. A VPC can accommodate two CIDR blocks, one for IPv4 and another for IPv6, but for the sake of simplicity I am going to limit my discussion to IPv4. When you create a CIDR block, you must enter it as an IP address range. In the case of an IPv4 CIDR, this means entering a network prefix and a subnet mask. The subnet mask determines how many IP addresses can be created from the CIDR block. Amazon requires that a CIDR block include a subnet mask ranging from 16 to 28. The two most commonly used subnet sizes are 16 bits and 24 bits. If you were to create a CIDR block with a 16-bit subnet, then the network portion of the IP address would contain two eight-bit numbers, followed by two zeros, each separated by periods. Here is an example of a CIDR block with a 16-bit subnet: 10.10.0.0/16 . This block would allow for the creation of up to 65,536 IP addresses. Each address would start with 10.10 , but you can enter any value between 0 and 255 into the last two positions. A CIDR block with a 24-bit address would contain three eight-bit numbers, followed by a single 0. Here is an example of what such a block would look like: 10.10.10.0/24 . This block could accommodate up to 256 IP addresses. The first three octets ( 10.10.10 ) would be common to each address, but the last digit can be populated with a number ranging between 0 and 255.","title":"VPC IP range and CIDR Blocks"},{"location":"aws/VPC_summary/IP_and_CIDR/#ip-address-range","text":"0.0.0.0/0 means from the whole Internet 192.168.1.1/32 means only from 192.168.1.1 CIDR block 16 bit 24 bit x.x.x.x/16 x.x.x.x/24 IP addresses 65,536 256 CIDR block common octets 10.10.0.0 10.10.10.0","title":"IP address range"},{"location":"aws/VPC_summary/IP_and_CIDR/#end","text":"","title":"End"},{"location":"aws/VPC_summary/NAT_overview/","text":"NAT \u00b6 NAT device enables instances in a private subnet to connect to the Internet or other AWS services, but prevents the Internet from initiating connections with the instances. NAT Gateway does something similar to Internet Gateway (IGW), but it only works one way: Instances in a private subnet can connect to services outside your VPC but external services cannot initiate a connection with those instances. Each NAT gateway is created in a specific Availability Zone and implemented with redundancy in that zone. NAT devices do not support IPv6 traffic, use an egress-only Internet gateway instead. NAT gateway replaces the source IP address of the instances with the IP address of the NAT gateway.","title":"NAT"},{"location":"aws/VPC_summary/NAT_overview/#nat","text":"NAT device enables instances in a private subnet to connect to the Internet or other AWS services, but prevents the Internet from initiating connections with the instances. NAT Gateway does something similar to Internet Gateway (IGW), but it only works one way: Instances in a private subnet can connect to services outside your VPC but external services cannot initiate a connection with those instances. Each NAT gateway is created in a specific Availability Zone and implemented with redundancy in that zone. NAT devices do not support IPv6 traffic, use an egress-only Internet gateway instead. NAT gateway replaces the source IP address of the instances with the IP address of the NAT gateway.","title":"NAT"},{"location":"aws/VPC_summary/internet_gateway_overview/","text":"Internet Gateways - IGW \u00b6 An Internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between instances in the VPC and the Internet. Internet Gateway enables resources (like EC2 instances) in public subnets to connect to the internet. Similarly, resources on the internet can initiate a connection to resources in your subnet using the public. If a VPC does not have an Internet Gateway, then the resources in the VPC cannot be accessed from the Internet (unless the traffic flows via a Corporate Network and VPN/Direct Connect). An Internet gateway serves two purposes: To provide a target in the VPC route tables for Internet-routable traffic, To perform network address translation (NAT) for instances that have been NOT been assigned public IP addresses. Enabling Internet access to an Instance requires Attaching Internet gateway to the VPC Subnet should have route tables associated with the route pointing to the Internet gateway Instances should have a Public IP or Elastic IP address assigned Security groups and NACLs associated with the Instance should allow relevant traffic IGW imposes no availability risks or bandwidth constraints on the network traffic.","title":"Internet Gateways - IGW"},{"location":"aws/VPC_summary/internet_gateway_overview/#internet-gateways-igw","text":"An Internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between instances in the VPC and the Internet. Internet Gateway enables resources (like EC2 instances) in public subnets to connect to the internet. Similarly, resources on the internet can initiate a connection to resources in your subnet using the public. If a VPC does not have an Internet Gateway, then the resources in the VPC cannot be accessed from the Internet (unless the traffic flows via a Corporate Network and VPN/Direct Connect). An Internet gateway serves two purposes: To provide a target in the VPC route tables for Internet-routable traffic, To perform network address translation (NAT) for instances that have been NOT been assigned public IP addresses. Enabling Internet access to an Instance requires Attaching Internet gateway to the VPC Subnet should have route tables associated with the route pointing to the Internet gateway Instances should have a Public IP or Elastic IP address assigned Security groups and NACLs associated with the Instance should allow relevant traffic IGW imposes no availability risks or bandwidth constraints on the network traffic.","title":"Internet Gateways - IGW"},{"location":"aws/VPC_summary/private_link/","text":"AWS PrivateLink \u00b6 AWS PrivateLink Reference You can use Amazon VPC to define a virtual private cloud (VPC), which is a logically isolated virtual network. You can launch AWS resources in your VPC. You can allow the resources in your VPC to connect to resources outside that VPC. For example, add an internet gateway to the VPC to allow access to the internet, or add a VPN connection to allow access to your on-premises network. Alternatively, use AWS PrivateLink to allow the resources in your VPC to connect to services in other VPCs using private IP addresses, as if those services were hosted directly in your VPC.","title":"AWS PrivateLink"},{"location":"aws/VPC_summary/private_link/#aws-privatelink","text":"AWS PrivateLink Reference You can use Amazon VPC to define a virtual private cloud (VPC), which is a logically isolated virtual network. You can launch AWS resources in your VPC. You can allow the resources in your VPC to connect to resources outside that VPC. For example, add an internet gateway to the VPC to allow access to the internet, or add a VPN connection to allow access to your on-premises network. Alternatively, use AWS PrivateLink to allow the resources in your VPC to connect to services in other VPCs using private IP addresses, as if those services were hosted directly in your VPC.","title":"AWS PrivateLink"},{"location":"aws/VPC_summary/security_group_overview/","text":"Security Groups \u00b6 Default SG \u00b6 Your default VPCs and any VPCs that you create come with a default security group. With some resources, if you don't associate a security group when you create the resource, we associate the default security group. For example, if you do not specify a security group when you launch an EC2 instance, we associate the default security group. You can change the rules for a default security group. You can't delete a default security group. If you try to delete the default security group, you get the following error: Client.CannotDelete. The following table describes the default rules for a default security group. Inbound Source Protocol Port Range Description SG ID All All Allows inbound traffic from resources that are assigned to the same security group. Outbound Destination Protocol Port Range Description 0.0.0.0/0 All All Allows all outbound IPv4 traffic ::/0 All All Allows all outbound IPv6 traffic. This rule is added only if your VPC has an associated IPv6 CIDR block. Security Group Self Reference \u00b6 The MWAA CF template requires a self-referencing ingress rule. You can configure a Security Group to permit Inbound connections from itself (that is, the security group has its own ID as the Source of the inbound connection). This would enable any Amazon EC2 instance that is associated with the security group to communicate with any other Amazon EC2 instance that is associated with the same security group (on the given port). The important thing to note is that security groups are enforced at the instance level rather than traditional firewalls that work at the network level. Thus, there is no concept of multiple instances being \"inside a security group\". Rather, the security group is applied against traffic as it goes into each instance. Thus, the need to allow incoming connections from 'itself'.","title":"Security Groups"},{"location":"aws/VPC_summary/security_group_overview/#security-groups","text":"","title":"Security Groups"},{"location":"aws/VPC_summary/security_group_overview/#default-sg","text":"Your default VPCs and any VPCs that you create come with a default security group. With some resources, if you don't associate a security group when you create the resource, we associate the default security group. For example, if you do not specify a security group when you launch an EC2 instance, we associate the default security group. You can change the rules for a default security group. You can't delete a default security group. If you try to delete the default security group, you get the following error: Client.CannotDelete. The following table describes the default rules for a default security group. Inbound Source Protocol Port Range Description SG ID All All Allows inbound traffic from resources that are assigned to the same security group. Outbound Destination Protocol Port Range Description 0.0.0.0/0 All All Allows all outbound IPv4 traffic ::/0 All All Allows all outbound IPv6 traffic. This rule is added only if your VPC has an associated IPv6 CIDR block.","title":"Default SG"},{"location":"aws/VPC_summary/security_group_overview/#security-group-self-reference","text":"The MWAA CF template requires a self-referencing ingress rule. You can configure a Security Group to permit Inbound connections from itself (that is, the security group has its own ID as the Source of the inbound connection). This would enable any Amazon EC2 instance that is associated with the security group to communicate with any other Amazon EC2 instance that is associated with the same security group (on the given port). The important thing to note is that security groups are enforced at the instance level rather than traditional firewalls that work at the network level. Thus, there is no concept of multiple instances being \"inside a security group\". Rather, the security group is applied against traffic as it goes into each instance. Thus, the need to allow incoming connections from 'itself'.","title":"Security Group Self Reference"},{"location":"aws/VPC_summary/vpc_network/","text":"VPC Networking Components \u00b6 Private Subnets \u00b6 Private subnets can communicate with other private subnets, in same VPC and public subnets , in same VPC. Other private subnets in the same VPC By default, a private subnet can only communicate with other subnets in the same VPC, be they private or public. In order to communicate to the internet, a NAT gateway and internet gateway are required, and to enable communication between subnets in different VPCs, the VPCs must first be peered. https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html#vpc-subnet-basics Public subnets in the same VPC By default, a private subnet can only communicate with other subnets in the same VPC, be they private or public. https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html#vpc-subnet-basics Public Subnet \u00b6 NACL network access control list Router and Route Table define where traffic is directed Internet Gateway allows public traffic to the internet from the VPC VPC Peering \u00b6 Peering allows two VPC to connect on private network. This allows secure data transfer across the VPCs in each AWS account. End \u00b6","title":"VPC Networking Components"},{"location":"aws/VPC_summary/vpc_network/#vpc-networking-components","text":"","title":"VPC Networking Components"},{"location":"aws/VPC_summary/vpc_network/#private-subnets","text":"Private subnets can communicate with other private subnets, in same VPC and public subnets , in same VPC. Other private subnets in the same VPC By default, a private subnet can only communicate with other subnets in the same VPC, be they private or public. In order to communicate to the internet, a NAT gateway and internet gateway are required, and to enable communication between subnets in different VPCs, the VPCs must first be peered. https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html#vpc-subnet-basics Public subnets in the same VPC By default, a private subnet can only communicate with other subnets in the same VPC, be they private or public. https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html#vpc-subnet-basics","title":"Private Subnets"},{"location":"aws/VPC_summary/vpc_network/#public-subnet","text":"NACL network access control list Router and Route Table define where traffic is directed Internet Gateway allows public traffic to the internet from the VPC","title":"Public Subnet"},{"location":"aws/VPC_summary/vpc_network/#vpc-peering","text":"Peering allows two VPC to connect on private network. This allows secure data transfer across the VPCs in each AWS account.","title":"VPC Peering"},{"location":"aws/VPC_summary/vpc_network/#end","text":"","title":"End"},{"location":"aws/VPC_summary/vpc_overview/","text":"VPC Overview \u00b6 VPC is basically just a logically isolated part of the AWS cloud, where you can define your own network. It gives you complete control of virtual networks including your own IP address ranges, subnets, route tables, and network gateways. AWS VPC Reference And you can leverage multiple layers of security including security groups and network access control lists to help you control access to your EC2 instances in each subnet. So typically, you would have a three-tier architecture. So you've got your web servers and your web servers need to be public-facing, and by public-facing, we simply mean that they're Internet accessible. So people from the Internet can access those web servers. And this would be done over port 80 or port 443, so HTTP or HTTPS. We then have our application tier. And so this could be our application servers and they're doing some form of business logic, perhaps they're rendering an image or something like that. And they're in a private subnet and they can only speak to the web tier and then to the database tier. And then we have the database tier on the backend. And this is a private subnet, and they can only speak directly to the application tier which then passes information back to the web tier. So this is an example of a fully customizable network and the only front-facing servers are going to be your web servers. Calculate your IP address ranges at cidr.xyz. And there's 3 common IP address ranges which we'll cover off. use 10.0.0.0/16 - First usable ip 10.0.0.1 - last usable ip 10.0.0.254 - Count 256","title":"VPC Overview"},{"location":"aws/VPC_summary/vpc_overview/#vpc-overview","text":"VPC is basically just a logically isolated part of the AWS cloud, where you can define your own network. It gives you complete control of virtual networks including your own IP address ranges, subnets, route tables, and network gateways. AWS VPC Reference And you can leverage multiple layers of security including security groups and network access control lists to help you control access to your EC2 instances in each subnet. So typically, you would have a three-tier architecture. So you've got your web servers and your web servers need to be public-facing, and by public-facing, we simply mean that they're Internet accessible. So people from the Internet can access those web servers. And this would be done over port 80 or port 443, so HTTP or HTTPS. We then have our application tier. And so this could be our application servers and they're doing some form of business logic, perhaps they're rendering an image or something like that. And they're in a private subnet and they can only speak to the web tier and then to the database tier. And then we have the database tier on the backend. And this is a private subnet, and they can only speak directly to the application tier which then passes information back to the web tier. So this is an example of a fully customizable network and the only front-facing servers are going to be your web servers. Calculate your IP address ranges at cidr.xyz. And there's 3 common IP address ranges which we'll cover off. use 10.0.0.0/16 - First usable ip 10.0.0.1 - last usable ip 10.0.0.254 - Count 256","title":"VPC Overview"},{"location":"aws/aws_pricing/aws_budgets/","text":"AWS Budgets \u00b6 Budgets is used to predict costs BEFORE they are incured End \u00b6","title":"AWS Budgets"},{"location":"aws/aws_pricing/aws_budgets/#aws-budgets","text":"Budgets is used to predict costs BEFORE they are incured","title":"AWS Budgets"},{"location":"aws/aws_pricing/aws_budgets/#end","text":"","title":"End"},{"location":"aws/aws_pricing/aws_organizations/","text":"AWS Organizations \u00b6 AWS Organizations is an account management service that enables you to consolidate multiple AWS accounts into an organization that you create and centrally manage. AWS Organization Flavors \u00b6 Full Access Create Organizational Units and put AWS accounts behind those OUs. We apply policies to the OUs or directly to AWS accounts. Consolidated Billing Allows for volume discounts on all accounts unused reserved instances for EC2 are applied across the group Best practices for AWS Organizations \u00b6 Always enable multi-factor authentication on root account Always use a strong and complex password on root account Paying account should be used for billing purposes only. Do not deploy resources into the paying account. Service Control Policy \u00b6 AWS Organizations provides central governance and management for multiple accounts. Organization service control policies (SCPs) allow you to create permissions guardrails that apply to all accounts within a given organization. Use Case Example \u00b6 An oil and gas utility company which is highly regulated must create a Cloud governance scheme. The company is organized into multiple autonomous departments which will all be using AWS resources. These departments each sponsor independent projects that are reviewed by regulatory boards for the approval of customer price increases. The code and infrastructure for each project has production, development, and testing environments. Which of the following account strategies will maximize security and operational efficiency for the company? Create an Organizational Unit structure in AWS Organizations with separate underlying accounts for production, development, and testing environments. A multi-layered account structure will work best for this company, leveraging AWS Organizations to establish Organizational Units for each department, with separate production, development, and testing environments. While there is no physical AWS account at the department level, service control policies can be applied at the Organizational Unit level, and billing can be reported separately for each department. An account for each department \u2014 in which the department combines dev/test/prod \u2014 or a single account for the company hosting all workloads together will NOT provide segregation of production, development, and testing environments at the account level. Multiple standalone accounts for each department and environment would compromise operational efficiency in managing environments across departments, as there is no overarching AWS Organization to manage all the accounts centrally. https://docs.aws.amazon.com/wellarchitected/latest/security-pillar/wellarchitected-security-pillar.pdf https://aws.amazon.com/organizations/ End \u00b6","title":"AWS Organizations"},{"location":"aws/aws_pricing/aws_organizations/#aws-organizations","text":"AWS Organizations is an account management service that enables you to consolidate multiple AWS accounts into an organization that you create and centrally manage.","title":"AWS Organizations"},{"location":"aws/aws_pricing/aws_organizations/#aws-organization-flavors","text":"Full Access Create Organizational Units and put AWS accounts behind those OUs. We apply policies to the OUs or directly to AWS accounts. Consolidated Billing Allows for volume discounts on all accounts unused reserved instances for EC2 are applied across the group","title":"AWS Organization Flavors"},{"location":"aws/aws_pricing/aws_organizations/#best-practices-for-aws-organizations","text":"Always enable multi-factor authentication on root account Always use a strong and complex password on root account Paying account should be used for billing purposes only. Do not deploy resources into the paying account.","title":"Best practices for AWS Organizations"},{"location":"aws/aws_pricing/aws_organizations/#service-control-policy","text":"AWS Organizations provides central governance and management for multiple accounts. Organization service control policies (SCPs) allow you to create permissions guardrails that apply to all accounts within a given organization.","title":"Service Control Policy"},{"location":"aws/aws_pricing/aws_organizations/#use-case-example","text":"An oil and gas utility company which is highly regulated must create a Cloud governance scheme. The company is organized into multiple autonomous departments which will all be using AWS resources. These departments each sponsor independent projects that are reviewed by regulatory boards for the approval of customer price increases. The code and infrastructure for each project has production, development, and testing environments. Which of the following account strategies will maximize security and operational efficiency for the company? Create an Organizational Unit structure in AWS Organizations with separate underlying accounts for production, development, and testing environments. A multi-layered account structure will work best for this company, leveraging AWS Organizations to establish Organizational Units for each department, with separate production, development, and testing environments. While there is no physical AWS account at the department level, service control policies can be applied at the Organizational Unit level, and billing can be reported separately for each department. An account for each department \u2014 in which the department combines dev/test/prod \u2014 or a single account for the company hosting all workloads together will NOT provide segregation of production, development, and testing environments at the account level. Multiple standalone accounts for each department and environment would compromise operational efficiency in managing environments across departments, as there is no overarching AWS Organization to manage all the accounts centrally. https://docs.aws.amazon.com/wellarchitected/latest/security-pillar/wellarchitected-security-pillar.pdf https://aws.amazon.com/organizations/","title":"Use Case Example"},{"location":"aws/aws_pricing/aws_organizations/#end","text":"","title":"End"},{"location":"aws/aws_pricing/computing_model_types/","text":"Computing Model Types \u00b6 IaaS offers building blocks that can be rented. When you pay a web hosting fee, you're using IaaS. - IaaS Example: Pay a subscription fee to a hosting company to serve your website on an instance you manage.","title":"Computing Model Types"},{"location":"aws/aws_pricing/computing_model_types/#computing-model-types","text":"IaaS offers building blocks that can be rented. When you pay a web hosting fee, you're using IaaS. - IaaS Example: Pay a subscription fee to a hosting company to serve your website on an instance you manage.","title":"Computing Model Types"},{"location":"aws/aws_pricing/cost_explorer/","text":"AWS Cost Explorer \u00b6 Cost Explorer allows you to visualize and forecast your costs and usage over time. Cost Explorer is used to explore costs AFTER they have been incurred.","title":"AWS Cost Explorer"},{"location":"aws/aws_pricing/cost_explorer/#aws-cost-explorer","text":"Cost Explorer allows you to visualize and forecast your costs and usage over time. Cost Explorer is used to explore costs AFTER they have been incurred.","title":"AWS Cost Explorer"},{"location":"aws/aws_pricing/cost_usage_report/","text":"Cost and Usage Report \u00b6 Cost and Usage Reports to publish your AWS billing reports to an S3 bucket you own. AWS updates the report in your bucket once a day in comma-separated value (CSV) format. You can view the reports using spreadsheet software or access them from an application using the Amazon S3 API. https://docs.aws.amazon.com/cur/latest/userguide/what-is-cur.html","title":"Cost and Usage Report"},{"location":"aws/aws_pricing/cost_usage_report/#cost-and-usage-report","text":"Cost and Usage Reports to publish your AWS billing reports to an S3 bucket you own. AWS updates the report in your bucket once a day in comma-separated value (CSV) format. You can view the reports using spreadsheet software or access them from an application using the Amazon S3 API. https://docs.aws.amazon.com/cur/latest/userguide/what-is-cur.html","title":"Cost and Usage Report"},{"location":"aws/aws_pricing/exams_clf_c01/","text":"AWS Certified Cloud Practitioner (CLF-C01) \u00b6 The AWS Certified Cloud Practitioner is the starting point for non-technical people looking to understand Cloud Computing and contribute to their organization\u2019s cloud initiatives. The Cloud Practitioner exam has 65 questions. The test is pass/fail with a minimum requirement of 70% to pass. Domain % of Exam Cloud Concepts 26 Security and Compliance 25 Technology 33 Billing and Pricing 16 Exam Review \u00b6 AWS Well Architected Framework Read the AWS overview after finishing the course preparation and again the day before the exam. Overview of Amazon Web Services Overview of Amazon Web Services AWS Terminology Cheatsheet Eligible Exams \u00b6","title":"AWS Certifications"},{"location":"aws/aws_pricing/exams_clf_c01/#aws-certified-cloud-practitioner-clf-c01","text":"The AWS Certified Cloud Practitioner is the starting point for non-technical people looking to understand Cloud Computing and contribute to their organization\u2019s cloud initiatives. The Cloud Practitioner exam has 65 questions. The test is pass/fail with a minimum requirement of 70% to pass. Domain % of Exam Cloud Concepts 26 Security and Compliance 25 Technology 33 Billing and Pricing 16","title":"AWS Certified Cloud Practitioner (CLF-C01)"},{"location":"aws/aws_pricing/exams_clf_c01/#exam-review","text":"AWS Well Architected Framework Read the AWS overview after finishing the course preparation and again the day before the exam. Overview of Amazon Web Services Overview of Amazon Web Services AWS Terminology Cheatsheet","title":"Exam Review"},{"location":"aws/aws_pricing/exams_clf_c01/#eligible-exams","text":"","title":"Eligible Exams"},{"location":"aws/aws_pricing/pricing_101/","text":"AWS Pricing 101 \u00b6 CAPEX stands for capital expenditure, which is where you pay up front, and it's fix sunk costs. For example, buying a server up front, or buying a whole bunch of servers, or buying network, switches or firewalls, or load balancers, et cetera. OpEx stands for operational expenditure, which is where you pay for what you use. And I want you to think of utility billing. So things like electricity, or gas, or water. This is where you're paying for things as you use them. And that was the great thing about us, is we started out just paying for what our customers were using. We started out with a service learning management system, so we had no servers at all. We'd only incur a cost when someone actually went to our website and started watching content. How AWS Pricing Works: https://docs.aws.amazon.com/whitepapers/latest/how-aws-pricing-works/welcome.html Pay as you go, pay for what you use, pay less as you pay more an pay even less when you reserve capacity. Pricing Policies \u00b6 Key principles and best practices that are broadly applicable. So the first one of four is understand the fundamentals of pricing. We then have start early with cost optimization. We then have maximize the power of flexibility, Use the right pricing model for the job. Three fundamental drivers of cost with AWS: \u00b6 Compute Storage Data outbound Data leaving your AWS environment, not data entering in Free Services \u00b6 VPC virtual data center in the cloud. Beanstalk The resources that Beanstalks provisioned by itself is not free. CloudFormation IAM Auto Scaling (EC2 instances ) Opsworks Consolidated Billing Lambda Pricing \u00b6 Request Pricing Free tier 1 million requests per month $0.20 per 1 million thereafter Duration Pricing 400,000 GB second per month free , up to 3.2 million seconds of compute time EBS Pricing \u00b6 Volumes (per GB) Snapshots (per GB) Data Transfer S3 Pricing \u00b6 Storage Class (standard or 1A or 1AZ) Storage Requests (GET, PUT, COPY) Data Copies Glacier \u00b6 AWS data archival service. And this is basically driven by two factors. We've got storage and then data retrieval times. Snowball \u00b6 is basically a petabyte scale data transport solution that uses secure appliances to transfer large amounts of data in and out of the AWS Cloud. And I want you to think of it as a gigantic disk that you use to move your data into the AWS Cloud. So basically, when you're transferring thousands of gigs, it's much more efficient to put it on a device locally and have that device then shipped to an AWS data center. service fee per job. So Snowball comes in two different sizes, we've got the 50 terabytes it's going to be a one off of charge of $200. 80 terabyte is $250, daily charge first 10 days are free and then after that you're paying $15 data transfer: So data transfer into S3 is free, but data transfer out from S3 onto a Snowball is not. RDS \u00b6 Clock hours of server time Database characteristics Database purchase type Number of instances provisioned storage additional storage requests deployment type data transfer DynamoDB, \u00b6 write read indexed storage There are really three drivers of costs. That's provision throughput in terms of write, provision throughput in terms of read, and then the actual amount of data that you're storing inside of DynamoDB. CloudFront \u00b6 traffic distribution the number of requests then data transfer out End \u00b6","title":"AWS Pricing 101"},{"location":"aws/aws_pricing/pricing_101/#aws-pricing-101","text":"CAPEX stands for capital expenditure, which is where you pay up front, and it's fix sunk costs. For example, buying a server up front, or buying a whole bunch of servers, or buying network, switches or firewalls, or load balancers, et cetera. OpEx stands for operational expenditure, which is where you pay for what you use. And I want you to think of utility billing. So things like electricity, or gas, or water. This is where you're paying for things as you use them. And that was the great thing about us, is we started out just paying for what our customers were using. We started out with a service learning management system, so we had no servers at all. We'd only incur a cost when someone actually went to our website and started watching content. How AWS Pricing Works: https://docs.aws.amazon.com/whitepapers/latest/how-aws-pricing-works/welcome.html Pay as you go, pay for what you use, pay less as you pay more an pay even less when you reserve capacity.","title":"AWS Pricing 101"},{"location":"aws/aws_pricing/pricing_101/#pricing-policies","text":"Key principles and best practices that are broadly applicable. So the first one of four is understand the fundamentals of pricing. We then have start early with cost optimization. We then have maximize the power of flexibility, Use the right pricing model for the job.","title":"Pricing Policies"},{"location":"aws/aws_pricing/pricing_101/#three-fundamental-drivers-of-cost-with-aws","text":"Compute Storage Data outbound Data leaving your AWS environment, not data entering in","title":"Three fundamental drivers of cost with AWS:"},{"location":"aws/aws_pricing/pricing_101/#free-services","text":"VPC virtual data center in the cloud. Beanstalk The resources that Beanstalks provisioned by itself is not free. CloudFormation IAM Auto Scaling (EC2 instances ) Opsworks Consolidated Billing","title":"Free Services"},{"location":"aws/aws_pricing/pricing_101/#lambda-pricing","text":"Request Pricing Free tier 1 million requests per month $0.20 per 1 million thereafter Duration Pricing 400,000 GB second per month free , up to 3.2 million seconds of compute time","title":"Lambda Pricing"},{"location":"aws/aws_pricing/pricing_101/#ebs-pricing","text":"Volumes (per GB) Snapshots (per GB) Data Transfer","title":"EBS Pricing"},{"location":"aws/aws_pricing/pricing_101/#s3-pricing","text":"Storage Class (standard or 1A or 1AZ) Storage Requests (GET, PUT, COPY) Data Copies","title":"S3 Pricing"},{"location":"aws/aws_pricing/pricing_101/#glacier","text":"AWS data archival service. And this is basically driven by two factors. We've got storage and then data retrieval times.","title":"Glacier"},{"location":"aws/aws_pricing/pricing_101/#snowball","text":"is basically a petabyte scale data transport solution that uses secure appliances to transfer large amounts of data in and out of the AWS Cloud. And I want you to think of it as a gigantic disk that you use to move your data into the AWS Cloud. So basically, when you're transferring thousands of gigs, it's much more efficient to put it on a device locally and have that device then shipped to an AWS data center. service fee per job. So Snowball comes in two different sizes, we've got the 50 terabytes it's going to be a one off of charge of $200. 80 terabyte is $250, daily charge first 10 days are free and then after that you're paying $15 data transfer: So data transfer into S3 is free, but data transfer out from S3 onto a Snowball is not.","title":"Snowball"},{"location":"aws/aws_pricing/pricing_101/#rds","text":"Clock hours of server time Database characteristics Database purchase type Number of instances provisioned storage additional storage requests deployment type data transfer","title":"RDS"},{"location":"aws/aws_pricing/pricing_101/#dynamodb","text":"write read indexed storage There are really three drivers of costs. That's provision throughput in terms of write, provision throughput in terms of read, and then the actual amount of data that you're storing inside of DynamoDB.","title":"DynamoDB,"},{"location":"aws/aws_pricing/pricing_101/#cloudfront","text":"traffic distribution the number of requests then data transfer out","title":"CloudFront"},{"location":"aws/aws_pricing/pricing_101/#end","text":"","title":"End"},{"location":"aws/aws_pricing/regions_AZs/","text":"AWS Regions, Availability Zones and Edge Locations \u00b6 Partition \u00b6 S3 bucket names must be unique within an AWS partition. A partition is a grouping of AWS Regions. AWS currently has 3 partitions: aws (Standard Regions), aws-cn (China Regions), and aws-us-gov (AWS GovCloud [US] Regions). AWS Region \u00b6 A physical location with multiple, isolated, and physically separate AZ's within a geographic area. AWS has the concept of a Region, which is a physical location around the world where data centers are clustered. Each AWS Region consists of multiple, isolated, and physically separate AZ's within a geographic area. Reference: Regions and Availability Zones . There are more Availability Zones than Regions. Regions contain 2 or more Availability Zones, which are themselves made up of 1 or more data centers. This means there will always be more AZs than Regions. https://aws.amazon.com/about-aws/global-infrastructure/regions_az/?p=ngi&loc=2 Availability Zones \u00b6 An Availability Zone (AZ) is one or more discrete data centers with redundant power, networking, and connectivity in an AWS Region. Each group of logical data centers is called an Availability Zone. An Availability Zone (AZ) is one or more discrete data centers with redundant power, networking, and connectivity in an AWS Region. AZs give customers the ability to operate production applications and databases that are more highly available, fault tolerant, and scalable than would be possible from a single data center. All AZs in an AWS Region are interconnected with high-bandwidth, low-latency networking, over fully redundant, dedicated metro fiber providing high-throughput, low-latency networking between AZs. All traffic between AZs is encrypted. The network performance is sufficient to accomplish synchronous replication between AZs. AZs make partitioning applications for high availability easy. If an application is partitioned across AZs, companies are better isolated and protected from issues such as power outages, lightning strikes, tornadoes, earthquakes, and more. AZs are physically separated by a meaningful distance, many kilometers, from any other AZ, although all are within 100 km (60 miles) of each other. Edge Locations \u00b6 Edge locations reduce latency and speeds up delivery of applications. These are used to cache static content. Edge locations are separate from AZs and Regions, and there are more Edge Locations than Regions and Availability Zones. End \u00b6","title":"Regions and AZs"},{"location":"aws/aws_pricing/regions_AZs/#aws-regions-availability-zones-and-edge-locations","text":"","title":"AWS Regions, Availability Zones and Edge Locations"},{"location":"aws/aws_pricing/regions_AZs/#partition","text":"S3 bucket names must be unique within an AWS partition. A partition is a grouping of AWS Regions. AWS currently has 3 partitions: aws (Standard Regions), aws-cn (China Regions), and aws-us-gov (AWS GovCloud [US] Regions).","title":"Partition"},{"location":"aws/aws_pricing/regions_AZs/#aws-region","text":"A physical location with multiple, isolated, and physically separate AZ's within a geographic area. AWS has the concept of a Region, which is a physical location around the world where data centers are clustered. Each AWS Region consists of multiple, isolated, and physically separate AZ's within a geographic area. Reference: Regions and Availability Zones . There are more Availability Zones than Regions. Regions contain 2 or more Availability Zones, which are themselves made up of 1 or more data centers. This means there will always be more AZs than Regions. https://aws.amazon.com/about-aws/global-infrastructure/regions_az/?p=ngi&loc=2","title":"AWS Region"},{"location":"aws/aws_pricing/regions_AZs/#availability-zones","text":"An Availability Zone (AZ) is one or more discrete data centers with redundant power, networking, and connectivity in an AWS Region. Each group of logical data centers is called an Availability Zone. An Availability Zone (AZ) is one or more discrete data centers with redundant power, networking, and connectivity in an AWS Region. AZs give customers the ability to operate production applications and databases that are more highly available, fault tolerant, and scalable than would be possible from a single data center. All AZs in an AWS Region are interconnected with high-bandwidth, low-latency networking, over fully redundant, dedicated metro fiber providing high-throughput, low-latency networking between AZs. All traffic between AZs is encrypted. The network performance is sufficient to accomplish synchronous replication between AZs. AZs make partitioning applications for high availability easy. If an application is partitioned across AZs, companies are better isolated and protected from issues such as power outages, lightning strikes, tornadoes, earthquakes, and more. AZs are physically separated by a meaningful distance, many kilometers, from any other AZ, although all are within 100 km (60 miles) of each other.","title":"Availability Zones"},{"location":"aws/aws_pricing/regions_AZs/#edge-locations","text":"Edge locations reduce latency and speeds up delivery of applications. These are used to cache static content. Edge locations are separate from AZs and Regions, and there are more Edge Locations than Regions and Availability Zones.","title":"Edge Locations"},{"location":"aws/aws_pricing/regions_AZs/#end","text":"","title":"End"},{"location":"aws/aws_pricing/resource_groups_and_tagging/","text":"Resource Groups and Tagging \u00b6 Using Resource Groups you can apply automation to resources tagged with specific tags. For example we can stop all EC2 instances in a region. Resource groups in combination with AWS systems manager allow you to control and execute automation against entire fleets of EC2 instances , all at the push of a button. Tag editor is a global service taht allows us to discover resourcess and add additional tags to them as well. Resource Groups \u00b6 A resource group is a collection of resources that share one or more tags. Groups can be based on resource types and tag queries, or AWS CloudFormation stacks. A resource group is a collection of AWS resources that are all in the same AWS region. A resource group is a collection of AWS resources that are all in the same AWS region, and that match criteria provided in a query. You can create unlimited, single-region groups in your account, use your groups to view group-related insights, and automate tasks on group resources. Groups can be based on resource types and tag queries, or AWS CloudFormation stacks. Reference: What is AWS Resource Groups? End \u00b6","title":"Resource Groups and Tagging"},{"location":"aws/aws_pricing/resource_groups_and_tagging/#resource-groups-and-tagging","text":"Using Resource Groups you can apply automation to resources tagged with specific tags. For example we can stop all EC2 instances in a region. Resource groups in combination with AWS systems manager allow you to control and execute automation against entire fleets of EC2 instances , all at the push of a button. Tag editor is a global service taht allows us to discover resourcess and add additional tags to them as well.","title":"Resource Groups and Tagging"},{"location":"aws/aws_pricing/resource_groups_and_tagging/#resource-groups","text":"A resource group is a collection of resources that share one or more tags. Groups can be based on resource types and tag queries, or AWS CloudFormation stacks. A resource group is a collection of AWS resources that are all in the same AWS region. A resource group is a collection of AWS resources that are all in the same AWS region, and that match criteria provided in a query. You can create unlimited, single-region groups in your account, use your groups to view group-related insights, and automate tasks on group resources. Groups can be based on resource types and tag queries, or AWS CloudFormation stacks. Reference: What is AWS Resource Groups?","title":"Resource Groups"},{"location":"aws/aws_pricing/resource_groups_and_tagging/#end","text":"","title":"End"},{"location":"aws/aws_pricing/support_plans/","text":"AWS Support Plans \u00b6 AWS Support currently has five levels (1 free and 4 paid). The Basic plan is the free entitlement for all AWS Customers. The four paid support plans in order of ascending cost are Developer, Business, Enterprise On-Ramp and Enterprise. https://aws.amazon.com/premiumsupport/compare-plans/ Free Developer $29 per month Business $100 per month Business Support is the minimum plan that provides access to the full set of Trusted Advisor checks. Enterprise $15K per month Enterprise Support provides access to a Technical Account Manager (TAM) who helps coordinate access to subject matter experts among other things. For AWS Enterprise customers, the AWS Concierge is a resource dedicated to answering billing and account questions. https://www.amazonaws.cn/en/support/features/ AWS Infrastructure Event Management is a structured program available to Enterprise Support customers (and Business Support customers for an additional fee) that helps you plan for large-scale events, such as product or application launches, infrastructure migrations, and marketing events. https://aws.amazon.com/premiumsupport/programs/iem/#:~:text=AWS%20Infrastructure%20Event%20Management%20is,infrastructure%20migrations%2C%20and%20marketing%20events. Case Severity and Response Times \u00b6 Basic Developer Business Enterprise General Guidance: < 24 business hours System Impaired: < 12 business hours General Guidance: < 24 hours System Impaired: < 12 hours General Guidance: < 24 hours System Impaired: < 12 hours Production System Impaired: < 4 hours Production System Down: < 1 hour Production System Impaired: < 4 hours Production System Down: < 1 hour Business Critical System Down: < 15 minutes 24x7 Chat or Telephone Support x x Trusted Advisor Checks x x AWS Partner Network (APN) Consulting Partners \u00b6 APN Consulting Partners include professional services organizations like system integrators, strategic consultancies, agencies, managed service providers (MSPs), and value-added resellers. In this case, we would engage a Consulting Partner to help us deploy a new system to the AWS Cloud. Joining the AWS Partner Network (APN) Strengthens Your Capabilities to Better Serve Customers. End \u00b6","title":"AWS Support Plans"},{"location":"aws/aws_pricing/support_plans/#aws-support-plans","text":"AWS Support currently has five levels (1 free and 4 paid). The Basic plan is the free entitlement for all AWS Customers. The four paid support plans in order of ascending cost are Developer, Business, Enterprise On-Ramp and Enterprise. https://aws.amazon.com/premiumsupport/compare-plans/ Free Developer $29 per month Business $100 per month Business Support is the minimum plan that provides access to the full set of Trusted Advisor checks. Enterprise $15K per month Enterprise Support provides access to a Technical Account Manager (TAM) who helps coordinate access to subject matter experts among other things. For AWS Enterprise customers, the AWS Concierge is a resource dedicated to answering billing and account questions. https://www.amazonaws.cn/en/support/features/ AWS Infrastructure Event Management is a structured program available to Enterprise Support customers (and Business Support customers for an additional fee) that helps you plan for large-scale events, such as product or application launches, infrastructure migrations, and marketing events. https://aws.amazon.com/premiumsupport/programs/iem/#:~:text=AWS%20Infrastructure%20Event%20Management%20is,infrastructure%20migrations%2C%20and%20marketing%20events.","title":"AWS Support Plans"},{"location":"aws/aws_pricing/support_plans/#case-severity-and-response-times","text":"Basic Developer Business Enterprise General Guidance: < 24 business hours System Impaired: < 12 business hours General Guidance: < 24 hours System Impaired: < 12 hours General Guidance: < 24 hours System Impaired: < 12 hours Production System Impaired: < 4 hours Production System Down: < 1 hour Production System Impaired: < 4 hours Production System Down: < 1 hour Business Critical System Down: < 15 minutes 24x7 Chat or Telephone Support x x Trusted Advisor Checks x x","title":"Case Severity and Response Times"},{"location":"aws/aws_pricing/support_plans/#aws-partner-network-apn-consulting-partners","text":"APN Consulting Partners include professional services organizations like system integrators, strategic consultancies, agencies, managed service providers (MSPs), and value-added resellers. In this case, we would engage a Consulting Partner to help us deploy a new system to the AWS Cloud. Joining the AWS Partner Network (APN) Strengthens Your Capabilities to Better Serve Customers.","title":"AWS Partner Network (APN) Consulting Partners"},{"location":"aws/aws_pricing/support_plans/#end","text":"","title":"End"},{"location":"aws/cfn_diagrams/","text":"Usage install \u00b6 npm i -g @mhlabs/cfn-diagram draw.io render \u00b6 cfn-dia draw.io --template-file cloudfrontS3Cdk.yaml --output-file cloudfrontS3Cdk.drawio cfn-dia draw.io --template-file vpc_pub_priv_2az_bastion.yml --output-file vpc_pub_priv_2az_bastion.drawio html render \u00b6 cfn-dia html --template-file cloudfrontS3Cdk.yaml --output-path cloudfrontS3Cdk-html cfn-dia html --template-file vpc_pub_priv_2az_bastion.yml --output-path vpc_pub_priv_2az_bastion-html","title":"Cfn Diagrams"},{"location":"aws/cfn_diagrams/#install","text":"npm i -g @mhlabs/cfn-diagram","title":"install"},{"location":"aws/cfn_diagrams/#drawio-render","text":"cfn-dia draw.io --template-file cloudfrontS3Cdk.yaml --output-file cloudfrontS3Cdk.drawio cfn-dia draw.io --template-file vpc_pub_priv_2az_bastion.yml --output-file vpc_pub_priv_2az_bastion.drawio","title":"draw.io render"},{"location":"aws/cfn_diagrams/#html-render","text":"cfn-dia html --template-file cloudfrontS3Cdk.yaml --output-path cloudfrontS3Cdk-html cfn-dia html --template-file vpc_pub_priv_2az_bastion.yml --output-path vpc_pub_priv_2az_bastion-html","title":"html render"},{"location":"aws/cloud_formation/apache/","text":"Test Apache EC2 \u00b6 An Amazon EC2 instance in a private subnet will never be directly reachable from the Internet, even if it has a public IP address. This is because a private subnet does not have a Route Table entry that connects the subnet to an Internet Gateway. This is intentional and desired. So, your options are: Put your instance in a Public Subnet instead of a Private Subnet, or Create a VPN connection to the VPC so you can communicate with resources in the VPC, including the private subnet, or Connect to an instance in the Public Subnet and use Port Forwarding to then obtain a connection with the private instance (see below), or Use a Load Balancer or Proxy in the Public Subnet to forward traffic to the private subnet (one benefit is that it mimics the production setup) Port Forwarding is a common technique to provide private connectivity to a resource that is not directly accessible. For example: Public-Instance in the public subnet Private-Instance in the private subnet SSH into Public-Instance with port forwarding, which then establishes a connection to Private-Instance Access resources on your local machine and it will actually forward the request to Private-Instance A sample connection string would be: ssh -i pemfile ec2-user@public-instance -L 8000:private-instance:80 Any request sent to your local computer's port 8000 would be forwarded to Public-Instance, which would then forward the request to private-instance:80. This will continue as long as the SSH session is in place.","title":"Test Apache EC2"},{"location":"aws/cloud_formation/apache/#test-apache-ec2","text":"An Amazon EC2 instance in a private subnet will never be directly reachable from the Internet, even if it has a public IP address. This is because a private subnet does not have a Route Table entry that connects the subnet to an Internet Gateway. This is intentional and desired. So, your options are: Put your instance in a Public Subnet instead of a Private Subnet, or Create a VPN connection to the VPC so you can communicate with resources in the VPC, including the private subnet, or Connect to an instance in the Public Subnet and use Port Forwarding to then obtain a connection with the private instance (see below), or Use a Load Balancer or Proxy in the Public Subnet to forward traffic to the private subnet (one benefit is that it mimics the production setup) Port Forwarding is a common technique to provide private connectivity to a resource that is not directly accessible. For example: Public-Instance in the public subnet Private-Instance in the private subnet SSH into Public-Instance with port forwarding, which then establishes a connection to Private-Instance Access resources on your local machine and it will actually forward the request to Private-Instance A sample connection string would be: ssh -i pemfile ec2-user@public-instance -L 8000:private-instance:80 Any request sent to your local computer's port 8000 would be forwarded to Public-Instance, which would then forward the request to private-instance:80. This will continue as long as the SSH session is in place.","title":"Test Apache EC2"},{"location":"aws/cloud_formation/cfn_examples/","text":"CF template guide \u00b6 CF Template Parameters \u00b6 Parameters: InstanceType: Description: WebServer EC2 instance type Type: String Default: t3.small AllowedValues: [t2.micro, t2.small, t3.micro, t3.small] ConstraintDescription: must be a valid EC2 instance type. SSHLocation: Description: The IP address range that can be used to SSH to the EC2 instances Type: String MinLength: 9 MaxLength: 18 Default: 0.0.0.0/0 AllowedPattern: (\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})/(\\d{1,2}) ConstraintDescription: must be a valid IP CIDR range of the form x.x.x.x/x. LatestAmiId: Type: 'AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>' Default: '/aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2' AllowedValues add dropdown in AWS Console, for example: AllowedValues: [t2.micro, t2.small, t3.micro, t3.small] Resource Allocation \u00b6 CF Template Resources VpcId: !Ref VPC Reference logical id of VPC !Ref 'LatestAmiId' Pull AMI Image Id from SSM ImageId UserData: Provision Apache webserver and restart automatically. Note, each resource will have a Logical ID (resource logical name) and a Physical ID (id assigned by AWS after resource creation). Think of Logical IDs as being used to reference resources within AWS CloudFormation template and Physical IDs being used to identify resources outside of AWS CloudFormation templates after they have been created. Resources: VPC: Type: 'AWS::EC2::VPC' Properties: EnableDnsSupport: 'true' EnableDnsHostnames: 'true' CidrBlock: 10.0.0.0/16 PublicSubnet: Type: 'AWS::EC2::Subnet' Properties: CidrBlock: 10.0.0.0/24 AvailabilityZone: 'us-east-1a' VpcId: !Ref VPC InternetGateway: Type: 'AWS::EC2::InternetGateway' VPCGatewayAttachment: Type: 'AWS::EC2::VPCGatewayAttachment' Properties: VpcId: !Ref VPC InternetGatewayId: !Ref InternetGateway PublicRouteTable: Type: 'AWS::EC2::RouteTable' Properties: VpcId: !Ref VPC PublicRoute: Type: 'AWS::EC2::Route' DependsOn: VPCGatewayAttachment Properties: RouteTableId: !Ref PublicRouteTable DestinationCidrBlock: 0.0.0.0/0 GatewayId: !Ref InternetGateway PublicSubnetRouteTableAssociation: Type: 'AWS::EC2::SubnetRouteTableAssociation' Properties: SubnetId: !Ref PublicSubnet RouteTableId: !Ref PublicRouteTable WebServerSecurityGroup: Type: 'AWS::EC2::SecurityGroup' Properties: VpcId: !Ref VPC GroupDescription: Allow access from HTTP and SSH traffic SecurityGroupIngress: - IpProtocol: tcp FromPort: '80' ToPort: '80' CidrIp: 0.0.0.0/0 - IpProtocol: tcp FromPort: '22' ToPort: '22' CidrIp: !Ref SSHLocation #EC2 instance where users can authenticate using Instance Connect, or via cloud_user and the password provided by CFN outputs. WebServerInstance: Type: 'AWS::EC2::Instance' Properties: InstanceType: !Ref InstanceType ImageId: !Ref 'LatestAmiId' NetworkInterfaces: - GroupSet: - !Ref WebServerSecurityGroup AssociatePublicIpAddress: 'true' DeviceIndex: '0' DeleteOnTermination: 'true' SubnetId: !Ref PublicSubnet UserData: Fn::Base64: Fn::Sub: - | #!/bin/bash -xe yum install -y ec2-instance-connect adduser cloud_user echo '${Password}' | passwd cloud_user --stdin sed 's/PasswordAuthentication no/PasswordAuthentication yes/' -i /etc/ssh/sshd_config service sshd restart yum update -y yum install -y httpd service httpd start chkconfig httpd on groupadd www usermod -a -G www cloud_user echo '<html><h1>Deploying a Basic Infrastructure Using CloudFormation Templates Hands-On Lab</h1><h3>Web Server</h3><h3>Availability Zone: ' > /var/www/html/index.html curl http://169.254.169.254/latest/meta-data/placement/availability-zone >> /var/www/html/index.html echo '</h3> <h3>Instance Id: ' >> /var/www/html/index.html curl http://169.254.169.254/latest/meta-data/instance-id >> /var/www/html/index.html echo '</h3></html> ' >> /var/www/html/index.html - Password: !GetAtt \"CustomPasswordWebServer.password\" Lambda \u00b6 Use lambda so we don't need a PEM key for auth. #Lambda function to generate a password for cloud_user. Students edit nothing below this line. RoleLambdaPasswordGenerator : Type : AWS::IAM::Role Properties : AssumeRolePolicyDocument : Version : \"2012-10-17\" Statement : - Effect : Allow Principal : Service : - lambda.amazonaws.com Action : - sts:AssumeRole Path : / PolicyLambdaGeneratePassword : Type : AWS::IAM::Policy Properties : Roles : - !Ref \"RoleLambdaPasswordGenerator\" PolicyName : LambdaPasswordGeneratorPolicy PolicyDocument : Version : \"2012-10-17\" Statement : - Effect : Allow Action : - logs:* Resource : - arn:aws:logs:*:*:* LambdaGeneratePassword : Type : AWS::Lambda::Function Properties : Description : Generate a random password. Handler : index.lambda_handler Role : !GetAtt \"RoleLambdaPasswordGenerator.Arn\" Runtime : python3.7 Timeout : 10 Code : ZipFile : !Join - \"\\n\" - - import cfnresponse; - import json; - import string; - \"import random;\\n\" - \"PASSWORD_LENGTH = 12;\\n\" - \"def lambda_handler(event, context):\" - \" print('## EVENT ##')\" - \" print(event)\" - \" responseData = {};\\n\" - \" try:\" - \" chars = string.ascii_uppercase + string.ascii_lowercase + string.digits;\" - \" responseData['password'] = ''.join(random.choice(chars) for x in range(PASSWORD_LENGTH));\" - \" cfnresponse.send(event, context, cfnresponse.SUCCESS, responseData\\ \\ );\\n\" - \" except Exception as e:\" - \" print ('Error running PasswordGenerator');\" - \" print (e);\" - \" cfnresponse.send(event, context, cfnresponse.FAILED, responseData\\ \\ );\\n\" - \" return responseData;\" CustomPasswordWebServer : Type : Custom::CustomPasswordWebServer Properties : Length : 8 ServiceToken : !GetAtt \"LambdaGeneratePassword.Arn\" CF Outputs \u00b6 #Outputs to provide useful information to students via the CFN Console Outputs: URL: Value: !Join - '' - - 'http://' - !GetAtt - WebServerInstance - PublicIp Description: Newly created application URL Outputs: InstanceId: Description: InstanceId of the newly created EC2 instance Value: !Ref 'WebServerInstance' AZ: Description: Availability Zone of the newly created EC2 instance Value: !GetAtt [WebServerInstance, AvailabilityZone] PublicDNS: Description: Public DNSName of the newly created EC2 instance Value: !GetAtt [WebServerInstance, PublicDnsName] PublicIP: Description: Public IP address of the newly created EC2 instance Value: !GetAtt [WebServerInstance, PublicIp] WebServerCustomPassword: Description: Password Value: !GetAtt \"CustomPasswordWebServer.password\" Azure Lambda authorizer for use with AWS API Gateway \u00b6 AWSTemplateFormatVersion : \"2010-09-09\" Description : Azure Lambda authorizer for use with AWS API Gateway Parameters : DataClassificationTag : Description : \"Classification for the data stored in this stack\" Type : String Default : \"internal\" AllowedValues : - critical - restricted - internal - public OwnerTag : Description : \"Individual(s)/Velocity Group responsible for this stack\" ConstraintDescription : \"Can contain only alphanumeric characters, spaces, dashes, and underscores.\" AllowedPattern : \"[-_/ a-zA-Z0-9]*\" MaxLength : \"64\" \"MinLength\" : \"4\" Type : String Metadata : AWS::CloudFormation::Interface : ParameterGroups : - Label : default : AWS Tags Parameters : - DataClassificationTag - OwnerTag","title":"Cfn Examples"},{"location":"aws/cloud_formation/cfn_examples/#cf-template-guide","text":"","title":"CF template guide"},{"location":"aws/cloud_formation/cfn_examples/#cf-template-parameters","text":"Parameters: InstanceType: Description: WebServer EC2 instance type Type: String Default: t3.small AllowedValues: [t2.micro, t2.small, t3.micro, t3.small] ConstraintDescription: must be a valid EC2 instance type. SSHLocation: Description: The IP address range that can be used to SSH to the EC2 instances Type: String MinLength: 9 MaxLength: 18 Default: 0.0.0.0/0 AllowedPattern: (\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})\\.(\\d{1,3})/(\\d{1,2}) ConstraintDescription: must be a valid IP CIDR range of the form x.x.x.x/x. LatestAmiId: Type: 'AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>' Default: '/aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2' AllowedValues add dropdown in AWS Console, for example: AllowedValues: [t2.micro, t2.small, t3.micro, t3.small]","title":"CF Template Parameters"},{"location":"aws/cloud_formation/cfn_examples/#resource-allocation","text":"CF Template Resources VpcId: !Ref VPC Reference logical id of VPC !Ref 'LatestAmiId' Pull AMI Image Id from SSM ImageId UserData: Provision Apache webserver and restart automatically. Note, each resource will have a Logical ID (resource logical name) and a Physical ID (id assigned by AWS after resource creation). Think of Logical IDs as being used to reference resources within AWS CloudFormation template and Physical IDs being used to identify resources outside of AWS CloudFormation templates after they have been created. Resources: VPC: Type: 'AWS::EC2::VPC' Properties: EnableDnsSupport: 'true' EnableDnsHostnames: 'true' CidrBlock: 10.0.0.0/16 PublicSubnet: Type: 'AWS::EC2::Subnet' Properties: CidrBlock: 10.0.0.0/24 AvailabilityZone: 'us-east-1a' VpcId: !Ref VPC InternetGateway: Type: 'AWS::EC2::InternetGateway' VPCGatewayAttachment: Type: 'AWS::EC2::VPCGatewayAttachment' Properties: VpcId: !Ref VPC InternetGatewayId: !Ref InternetGateway PublicRouteTable: Type: 'AWS::EC2::RouteTable' Properties: VpcId: !Ref VPC PublicRoute: Type: 'AWS::EC2::Route' DependsOn: VPCGatewayAttachment Properties: RouteTableId: !Ref PublicRouteTable DestinationCidrBlock: 0.0.0.0/0 GatewayId: !Ref InternetGateway PublicSubnetRouteTableAssociation: Type: 'AWS::EC2::SubnetRouteTableAssociation' Properties: SubnetId: !Ref PublicSubnet RouteTableId: !Ref PublicRouteTable WebServerSecurityGroup: Type: 'AWS::EC2::SecurityGroup' Properties: VpcId: !Ref VPC GroupDescription: Allow access from HTTP and SSH traffic SecurityGroupIngress: - IpProtocol: tcp FromPort: '80' ToPort: '80' CidrIp: 0.0.0.0/0 - IpProtocol: tcp FromPort: '22' ToPort: '22' CidrIp: !Ref SSHLocation #EC2 instance where users can authenticate using Instance Connect, or via cloud_user and the password provided by CFN outputs. WebServerInstance: Type: 'AWS::EC2::Instance' Properties: InstanceType: !Ref InstanceType ImageId: !Ref 'LatestAmiId' NetworkInterfaces: - GroupSet: - !Ref WebServerSecurityGroup AssociatePublicIpAddress: 'true' DeviceIndex: '0' DeleteOnTermination: 'true' SubnetId: !Ref PublicSubnet UserData: Fn::Base64: Fn::Sub: - | #!/bin/bash -xe yum install -y ec2-instance-connect adduser cloud_user echo '${Password}' | passwd cloud_user --stdin sed 's/PasswordAuthentication no/PasswordAuthentication yes/' -i /etc/ssh/sshd_config service sshd restart yum update -y yum install -y httpd service httpd start chkconfig httpd on groupadd www usermod -a -G www cloud_user echo '<html><h1>Deploying a Basic Infrastructure Using CloudFormation Templates Hands-On Lab</h1><h3>Web Server</h3><h3>Availability Zone: ' > /var/www/html/index.html curl http://169.254.169.254/latest/meta-data/placement/availability-zone >> /var/www/html/index.html echo '</h3> <h3>Instance Id: ' >> /var/www/html/index.html curl http://169.254.169.254/latest/meta-data/instance-id >> /var/www/html/index.html echo '</h3></html> ' >> /var/www/html/index.html - Password: !GetAtt \"CustomPasswordWebServer.password\"","title":"Resource Allocation"},{"location":"aws/cloud_formation/cfn_examples/#lambda","text":"Use lambda so we don't need a PEM key for auth. #Lambda function to generate a password for cloud_user. Students edit nothing below this line. RoleLambdaPasswordGenerator : Type : AWS::IAM::Role Properties : AssumeRolePolicyDocument : Version : \"2012-10-17\" Statement : - Effect : Allow Principal : Service : - lambda.amazonaws.com Action : - sts:AssumeRole Path : / PolicyLambdaGeneratePassword : Type : AWS::IAM::Policy Properties : Roles : - !Ref \"RoleLambdaPasswordGenerator\" PolicyName : LambdaPasswordGeneratorPolicy PolicyDocument : Version : \"2012-10-17\" Statement : - Effect : Allow Action : - logs:* Resource : - arn:aws:logs:*:*:* LambdaGeneratePassword : Type : AWS::Lambda::Function Properties : Description : Generate a random password. Handler : index.lambda_handler Role : !GetAtt \"RoleLambdaPasswordGenerator.Arn\" Runtime : python3.7 Timeout : 10 Code : ZipFile : !Join - \"\\n\" - - import cfnresponse; - import json; - import string; - \"import random;\\n\" - \"PASSWORD_LENGTH = 12;\\n\" - \"def lambda_handler(event, context):\" - \" print('## EVENT ##')\" - \" print(event)\" - \" responseData = {};\\n\" - \" try:\" - \" chars = string.ascii_uppercase + string.ascii_lowercase + string.digits;\" - \" responseData['password'] = ''.join(random.choice(chars) for x in range(PASSWORD_LENGTH));\" - \" cfnresponse.send(event, context, cfnresponse.SUCCESS, responseData\\ \\ );\\n\" - \" except Exception as e:\" - \" print ('Error running PasswordGenerator');\" - \" print (e);\" - \" cfnresponse.send(event, context, cfnresponse.FAILED, responseData\\ \\ );\\n\" - \" return responseData;\" CustomPasswordWebServer : Type : Custom::CustomPasswordWebServer Properties : Length : 8 ServiceToken : !GetAtt \"LambdaGeneratePassword.Arn\"","title":"Lambda"},{"location":"aws/cloud_formation/cfn_examples/#cf-outputs","text":"#Outputs to provide useful information to students via the CFN Console Outputs: URL: Value: !Join - '' - - 'http://' - !GetAtt - WebServerInstance - PublicIp Description: Newly created application URL Outputs: InstanceId: Description: InstanceId of the newly created EC2 instance Value: !Ref 'WebServerInstance' AZ: Description: Availability Zone of the newly created EC2 instance Value: !GetAtt [WebServerInstance, AvailabilityZone] PublicDNS: Description: Public DNSName of the newly created EC2 instance Value: !GetAtt [WebServerInstance, PublicDnsName] PublicIP: Description: Public IP address of the newly created EC2 instance Value: !GetAtt [WebServerInstance, PublicIp] WebServerCustomPassword: Description: Password Value: !GetAtt \"CustomPasswordWebServer.password\"","title":"CF Outputs"},{"location":"aws/cloud_formation/cfn_examples/#azure-lambda-authorizer-for-use-with-aws-api-gateway","text":"AWSTemplateFormatVersion : \"2010-09-09\" Description : Azure Lambda authorizer for use with AWS API Gateway Parameters : DataClassificationTag : Description : \"Classification for the data stored in this stack\" Type : String Default : \"internal\" AllowedValues : - critical - restricted - internal - public OwnerTag : Description : \"Individual(s)/Velocity Group responsible for this stack\" ConstraintDescription : \"Can contain only alphanumeric characters, spaces, dashes, and underscores.\" AllowedPattern : \"[-_/ a-zA-Z0-9]*\" MaxLength : \"64\" \"MinLength\" : \"4\" Type : String Metadata : AWS::CloudFormation::Interface : ParameterGroups : - Label : default : AWS Tags Parameters : - DataClassificationTag - OwnerTag","title":"Azure Lambda authorizer for use with AWS API Gateway"},{"location":"aws/cloud_formation/deploy_basic/","text":"Deploy a Basic Infrastructure Using CloudFormation Templates \u00b6 Goal: Perform direct update of existing CloudFormation Stack. The CF template will permanently show these changes. Introduction \u00b6 Your development team has been using this template for the intern-testing program. After performing an analysis, your team has determined that the t3.small instance was more compute power than they needed. Update the template so that the default instance defined in the stack is a t3.micro. In this hands-on lab, we're going to jump into an environment that already has a CloudFormation stack deployed. We'll review the contents of the CloudFormation template, and then we'll perform direct updates to the stack itself. Update template from t3.small to t3.micro . The stack already has a VPC with a public subnet. AWS CloudFormation Templates CF Outputs: At the bottom of our CF templates we define our outputs, from the stack building process. CF Parameter: We can edit the parameter to make permanent changes to our CF template. Update Stack For testing it is possible to modify stack parameters , without modifying the CF template. Users building a new stack from the CF template would provision the original resources. Solution: Edit CF Parameter \u00b6 Log in to the AWS Management Console using the credentials provided on the lab instructions page. Make sure you're using the us-east-1 region. Using CloudFormation Designer, Configure the InstanceType Stack Parameter to T3.Micro In the search bar at the top, enter \"CloudFormation\". Select CloudFormation from the dropdown menu. Select the stack already in the CloudFormation dashboard. Review the Stack info tab Review the Events tab (the newest events will appear first). Review the Resources tab.7. Right-click and open the physical ID for the InternetGateway in a new tab. 10 Right-click and open the physical ID for the WebServerInstance in a new tab. Close out the InternetGateway and WebServerInstance tabs after reviewing.7. 8. 9. 10. Review the Outputs tab. Right-click on the PublicDNS link and open it in a new tab. (Make sure to specify http, not https). Navigate back to the CloudFormation dashboard Click on the Parameters tab Near the top of the page, next to Delete, click Update Ensure Use current template is selected and click Next. Under InstanceType, select t3.micro. Note: This only changes the current deployment of the stack. Click Cancel. Navigate back to the CloudFormation dashboard and click on the existing stack. Click Update again. Select Edit template in designer and click View in Designer. Update the CloudFormation Stack in \"designer\". Ensure YAML is selected for the template. Under Parameters, change the default size from t3.small to t3.micro by erasing \"small\" and typing \"micro\". Launch the Updated Stack and Verify the New EC2 Resource Is Reachable \u00b6 Copy the template and click on the checkmarked box icon in the top-left corner to validate the template. You should see Template is valid. Click on the cloud icon next to the checkmarked box to launch the stack. Note: This is a direct update to the existing stack. Click Next. Under Parameters, select the InstanceType dropdown box and select t3.micro. Click Next through Tags. Scroll down and select the acknowledgement under Capabilities. Click Update stack. Click on the refresh icon in the top-right corner of the CloudFormation dashboard to ensure the update is complete (this may take a few minutes). Navigate to the Resources tab and open the WebServerInstance physical ID in a new tab to verify that the InstanceType is t3.micro. Navigate back to the CloudFormation dashboard and select the Outputs tab Open the PublicDNS value in a new tab to ensure the instance is reachable via the public web. End \u00b6","title":"Deploy Basic"},{"location":"aws/cloud_formation/deploy_basic/#deploy-a-basic-infrastructure-using-cloudformation-templates","text":"Goal: Perform direct update of existing CloudFormation Stack. The CF template will permanently show these changes.","title":"Deploy a Basic Infrastructure Using CloudFormation Templates"},{"location":"aws/cloud_formation/deploy_basic/#introduction","text":"Your development team has been using this template for the intern-testing program. After performing an analysis, your team has determined that the t3.small instance was more compute power than they needed. Update the template so that the default instance defined in the stack is a t3.micro. In this hands-on lab, we're going to jump into an environment that already has a CloudFormation stack deployed. We'll review the contents of the CloudFormation template, and then we'll perform direct updates to the stack itself. Update template from t3.small to t3.micro . The stack already has a VPC with a public subnet. AWS CloudFormation Templates CF Outputs: At the bottom of our CF templates we define our outputs, from the stack building process. CF Parameter: We can edit the parameter to make permanent changes to our CF template. Update Stack For testing it is possible to modify stack parameters , without modifying the CF template. Users building a new stack from the CF template would provision the original resources.","title":"Introduction"},{"location":"aws/cloud_formation/deploy_basic/#solution-edit-cf-parameter","text":"Log in to the AWS Management Console using the credentials provided on the lab instructions page. Make sure you're using the us-east-1 region. Using CloudFormation Designer, Configure the InstanceType Stack Parameter to T3.Micro In the search bar at the top, enter \"CloudFormation\". Select CloudFormation from the dropdown menu. Select the stack already in the CloudFormation dashboard. Review the Stack info tab Review the Events tab (the newest events will appear first). Review the Resources tab.7. Right-click and open the physical ID for the InternetGateway in a new tab. 10 Right-click and open the physical ID for the WebServerInstance in a new tab. Close out the InternetGateway and WebServerInstance tabs after reviewing.7. 8. 9. 10. Review the Outputs tab. Right-click on the PublicDNS link and open it in a new tab. (Make sure to specify http, not https). Navigate back to the CloudFormation dashboard Click on the Parameters tab Near the top of the page, next to Delete, click Update Ensure Use current template is selected and click Next. Under InstanceType, select t3.micro. Note: This only changes the current deployment of the stack. Click Cancel. Navigate back to the CloudFormation dashboard and click on the existing stack. Click Update again. Select Edit template in designer and click View in Designer. Update the CloudFormation Stack in \"designer\". Ensure YAML is selected for the template. Under Parameters, change the default size from t3.small to t3.micro by erasing \"small\" and typing \"micro\".","title":"Solution: Edit CF Parameter"},{"location":"aws/cloud_formation/deploy_basic/#launch-the-updated-stack-and-verify-the-new-ec2-resource-is-reachable","text":"Copy the template and click on the checkmarked box icon in the top-left corner to validate the template. You should see Template is valid. Click on the cloud icon next to the checkmarked box to launch the stack. Note: This is a direct update to the existing stack. Click Next. Under Parameters, select the InstanceType dropdown box and select t3.micro. Click Next through Tags. Scroll down and select the acknowledgement under Capabilities. Click Update stack. Click on the refresh icon in the top-right corner of the CloudFormation dashboard to ensure the update is complete (this may take a few minutes). Navigate to the Resources tab and open the WebServerInstance physical ID in a new tab to verify that the InstanceType is t3.micro. Navigate back to the CloudFormation dashboard and select the Outputs tab Open the PublicDNS value in a new tab to ensure the instance is reachable via the public web.","title":"Launch the Updated Stack and Verify the New EC2 Resource Is Reachable"},{"location":"aws/cloud_formation/deploy_basic/#end","text":"","title":"End"},{"location":"aws/cloud_formation/EC2/security_groups/","text":"Security Groups \u00b6 Control Network access using Security Groups. AWS Security Group Inline Inbound Rule Example using CloudFormation \u00b6 This CF template is not working currently AWS::EC2::SecurityGroup Ingress adds an inbound rule to a security group. An inbound rule permits instances to receive traffic from the specified IPv4 or IPv6 CIDR address range, or from the instances associated with the specified security group. You must specify only one of the following properties: CidrIp , CidrIpv6 , SourcePrefixListId , SourceSecurityGroupId , or SourceSecurityGroupName . You specify a protocol for each rule (for example, TCP). For TCP and UDP, you must also specify a port or port range. For ICMP/ICMPv6, you must also specify the ICMP/ICMPv6 type and code. You can use -1 to mean all types or all codes. You must specify a source security group ( SourcePrefixListId , SourceSecurityGroupId , or SourceSecurityGroupName ) or a CIDR range ( CidrIp or CidrIpv6 ). If you do not specify one of these parameters, the stack will launch successfully but the rule will not be added to the security group. Rule changes are propagated to instances within the security group as quickly as possible. However, a small delay might occur. The EC2 Security Group Rule is an embedded property of the AWS::EC2::SecurityGroup type. Inbound Rule - Type HTTP - Protoclol TCP - Port Range 80 - Source: Existing SG to allow ocelot traffic - Description Ocelot Access Outbound Rules - Name - IP Version IPV4 - Type All traffic - Protocol All - Port range All - Destination 0.0.0.0/0 AWSTemplateFormatVersion: 2010-09-09 Description: | CloudFormation template for security group. Allow traffic across ocelot using private subnet. Parameters: VPCid: Type: AWS::EC2::VPC::Id SGname: Type: AWS::EC2::SecurityGroup::GroupName Description: Name of security group EnvironmentTag: Description: In Which AWS Environment are you deploying? Type: String Default: non-prod AllowedValues: - non-prod - prod BayOwnerTag: Description: Individuals responsible for the resources AllowedPattern: \"[-_a-zA-Z0-9]*\" ConstraintDescription: Can only contain alphanumeric characters, spaces, dashes and underscores MinLength: \"4\" MaxLength: \"64\" Type: String ProjectTag: Description: Project title AllowedPattern: \"[-_a-zA-Z0-9]*\" ConstraintDescription: Can only contain alphanumeric characters, spaces, dashes and underscores MinLength: \"4\" MaxLength: \"64\" Type: String Metadata: AWS::CloudFormation::Interface: ParameterGroups: - Label: default: \"Tags\" Parameters: - EnvironmentTag - OwnerTag - ProjectTag Resources: SG: Type: AWS::EC2::SecurityGroup Properties: GroupDescription: A Security Group for EC2 VpcId: !Ref VPCid SecurityGroupIngress: - IpProtocol: tcp FromPort: 80 ToPort: 80 CidrIp: !Ref SGname SecurityGroupEgress: - IpProtocol: -1 CidrIp: 0.0.0.0/0 Outputs: SecurityGroupId: Description: Security Group Id Value: !GetAtt SG.GroupId SecurityGroupName: Description: Security Group Name Value: !Ref SGname VpcId: Description: VpcId in Which SG is there Value: !GetAtt SG.GroupId StackName: Value: !Ref 'AWS::StackName' Description: Name of the resulting cloudformation stack Ingress and Egress Rule \u00b6 InstanceSecurityGroup: Type: AWS::EC2::SecurityGroup Properties: GroupDescription: Allow http to client host VpcId: Ref: myVPC SecurityGroupIngress: - IpProtocol: tcp FromPort: 80 ToPort: 80 CidrIp: 0.0.0.0/0 SecurityGroupEgress: - IpProtocol: tcp FromPort: 80 ToPort: 80 CidrIp: 0.0.0.0/0","title":"ec2 Security Groups"},{"location":"aws/cloud_formation/EC2/security_groups/#security-groups","text":"Control Network access using Security Groups.","title":"Security Groups"},{"location":"aws/cloud_formation/EC2/security_groups/#aws-security-group-inline-inbound-rule-example-using-cloudformation","text":"This CF template is not working currently AWS::EC2::SecurityGroup Ingress adds an inbound rule to a security group. An inbound rule permits instances to receive traffic from the specified IPv4 or IPv6 CIDR address range, or from the instances associated with the specified security group. You must specify only one of the following properties: CidrIp , CidrIpv6 , SourcePrefixListId , SourceSecurityGroupId , or SourceSecurityGroupName . You specify a protocol for each rule (for example, TCP). For TCP and UDP, you must also specify a port or port range. For ICMP/ICMPv6, you must also specify the ICMP/ICMPv6 type and code. You can use -1 to mean all types or all codes. You must specify a source security group ( SourcePrefixListId , SourceSecurityGroupId , or SourceSecurityGroupName ) or a CIDR range ( CidrIp or CidrIpv6 ). If you do not specify one of these parameters, the stack will launch successfully but the rule will not be added to the security group. Rule changes are propagated to instances within the security group as quickly as possible. However, a small delay might occur. The EC2 Security Group Rule is an embedded property of the AWS::EC2::SecurityGroup type. Inbound Rule - Type HTTP - Protoclol TCP - Port Range 80 - Source: Existing SG to allow ocelot traffic - Description Ocelot Access Outbound Rules - Name - IP Version IPV4 - Type All traffic - Protocol All - Port range All - Destination 0.0.0.0/0 AWSTemplateFormatVersion: 2010-09-09 Description: | CloudFormation template for security group. Allow traffic across ocelot using private subnet. Parameters: VPCid: Type: AWS::EC2::VPC::Id SGname: Type: AWS::EC2::SecurityGroup::GroupName Description: Name of security group EnvironmentTag: Description: In Which AWS Environment are you deploying? Type: String Default: non-prod AllowedValues: - non-prod - prod BayOwnerTag: Description: Individuals responsible for the resources AllowedPattern: \"[-_a-zA-Z0-9]*\" ConstraintDescription: Can only contain alphanumeric characters, spaces, dashes and underscores MinLength: \"4\" MaxLength: \"64\" Type: String ProjectTag: Description: Project title AllowedPattern: \"[-_a-zA-Z0-9]*\" ConstraintDescription: Can only contain alphanumeric characters, spaces, dashes and underscores MinLength: \"4\" MaxLength: \"64\" Type: String Metadata: AWS::CloudFormation::Interface: ParameterGroups: - Label: default: \"Tags\" Parameters: - EnvironmentTag - OwnerTag - ProjectTag Resources: SG: Type: AWS::EC2::SecurityGroup Properties: GroupDescription: A Security Group for EC2 VpcId: !Ref VPCid SecurityGroupIngress: - IpProtocol: tcp FromPort: 80 ToPort: 80 CidrIp: !Ref SGname SecurityGroupEgress: - IpProtocol: -1 CidrIp: 0.0.0.0/0 Outputs: SecurityGroupId: Description: Security Group Id Value: !GetAtt SG.GroupId SecurityGroupName: Description: Security Group Name Value: !Ref SGname VpcId: Description: VpcId in Which SG is there Value: !GetAtt SG.GroupId StackName: Value: !Ref 'AWS::StackName' Description: Name of the resulting cloudformation stack","title":"AWS Security Group Inline Inbound Rule Example using CloudFormation"},{"location":"aws/cloud_formation/EC2/security_groups/#ingress-and-egress-rule","text":"InstanceSecurityGroup: Type: AWS::EC2::SecurityGroup Properties: GroupDescription: Allow http to client host VpcId: Ref: myVPC SecurityGroupIngress: - IpProtocol: tcp FromPort: 80 ToPort: 80 CidrIp: 0.0.0.0/0 SecurityGroupEgress: - IpProtocol: tcp FromPort: 80 ToPort: 80 CidrIp: 0.0.0.0/0","title":"Ingress and Egress Rule"},{"location":"aws/cloud_formation/IAM/role_and_policy/","text":"Resources: GenerateJWTLambdaExecutionRole: Type: AWS::IAM::Role Properties: RoleName: cloudeng-jwt-token-creator-role Path: /protected/ AssumeRolePolicyDocument: Version: \"2012-10-17\" Statement: - Effect: Allow Principal: Service: - lambda.amazonaws.com AWS: - arn:aws:iam::640315046644:role/jwtcreator-role Action: - sts:AssumeRole Policies: - PolicyName: jwt-token-creator-role-policy PolicyDocument: Version: \"2012-10-17\" Statement: - Effect: Allow Action: - ssm:* Resource: \"*\" - Effect: Allow Action: - logs:CreateLogGroup - logs:CreateLogStream - logs:PutLogEvents Resource: \"*\"","title":"Role and policy"},{"location":"aws/cloud_formation/cognito_user_signin/","text":"Create sign-in page in Cognito user pool \u00b6 https://awstut.com/en/2021/12/12/create-a-sign-in-page-in-the-cognito-user-pool/ Architecture \u00b6 Requirements \u00b6 AWS CLI S3 Bucket(Here, the bucket name is cognito-4090723305377 and region is us-east-1 ) Usage \u00b6 Template File Modification \u00b6 Modify the following locations in cf-cognito-base.yaml. Parameters : TemplateBucketName : Type : String Default : cognito-4090723305377 Upload Template Files to S3 Bucket \u00b6 aws s3 cp . s3://cognito-409072330537/user-signin/ --recursive CloudFormation Stack Creation \u00b6 aws cloudformation create-stack --stack-name cf-cognito --template-url https://cognito-409072330537.s3.amazonaws.com/user-signin/user-signin.yaml --capabilities CAPABILITY_IAM Upload HTML files \u00b6 aws s3 cp ./html s3://fa-010/ --recursive Cloudformation Stack \u00b6 This time, we will build this environment with a nested stack. For more information about nested stacks, please refer to the following page. Cloudformation Nested Stacks Create User Pool \u00b6 In user-signin-user-pool.yaml, we will define the Cognito-related resources. First, let\u2019s check the user pool itself. Resources : UserPool : Type : AWS::Cognito::UserPool Properties : AutoVerifiedAttributes : - email UsernameAttributes : - email UserPoolName : !Sub ${Prefix}-UserPool Schema : - AttributeDataType : String Mutable : true Name : name Required : true The AutoVerifiedAttributes property allows you to set automatic verification for the specified attributes. Amazon Cognito can automatically verify email addresses or mobile phone numbers by sending a verification code\u2014or, for email, a verification link. For email addresses, the code or link is sent in an email message. For phone numbers, the code is sent in an SMS text message. Configuring email or phone verification You can specify \u201cemail\u201d or \u201cphone_number\u201d for this property. In this case, we will specify the former so that a verification code will be automatically sent to the registered email address upon signup. The UsernameAttributes property is a parameter to specify the attributes to be used in place of the user name (username). UsernameAttributes Determines whether email addresses or phone numbers can be specified as user names when a user signs up. Possible values: phone_number or email. AWS::Cognito::UserPool UsernameAttributes As mentioned above, you can specify \u201cemail\u201d or \u201cphone_number\u201d for this property, but we will specify the former. In this case, we will specify the former. This setting will allow users to sign in using their email address instead of their user name. One thing to note about this property is that username is considered a required attribute in Cognito. The username value is a separate attribute and not the same as the name attribute. A username is always required to register a user, and it can\u2019t be changed after a user is created. User Pool attributes However, if a username is not required, such as in the case of the application we are creating this time, an email address/phone number can be used instead of a username. In this case, the user name will be set to a randomly generated ID. If your application does not require a username, you do not need to ask users to provide one. Your app can create a unique username for users in the background. This is useful if, for example, you want users to register and sign in with an email address and password. User Pool attributes The Schema property allows you to specify the attributes to be registered when signing up. In this case, we will add one of the standard attributes, name (not username). Note that email address and password are not required to be specified in this property, as they are set to \u201cemail\u201d in the UsernameAttributes property. Create user pool app client \u00b6 Next, check the user pool app client. Resources : UserPoolClient : Type : AWS::Cognito::UserPoolClient Properties : AccessTokenValidity : 60 # (minutes) default value. AllowedOAuthFlowsUserPoolClient : true AllowedOAuthFlows : - code - implicit AllowedOAuthScopes : - aws.cognito.signin.user.admin - email - openid - phone - profile CallbackURLs : - !Sub \"${BucketWesSiteEndpointUrl}/${SigninHtml}\" ClientName : !Sub ${Prefix}-UserPoolClient EnableTokenRevocation : true # default value. ExplicitAuthFlows : - ALLOW_CUSTOM_AUTH - ALLOW_REFRESH_TOKEN_AUTH - ALLOW_USER_SRP_AUTH IdTokenValidity : 60 # (minutes) default value. LogoutURLs : - !Sub \"${BucketWesSiteEndpointUrl}/${SignoutHtml}\" PreventUserExistenceErrors : ENABLED # default value. RefreshTokenValidity : 30 # (days) default value. SupportedIdentityProviders : - COGNITO TokenValidityUnits : AccessToken : minutes IdToken : minutes RefreshToken : days UserPoolId : !Ref UserPool By creating an app client, you will be able to perform API operations such as signup. An app is an entity within a user pool that has permission to call unauthenticated API operations. Configuring a user pool app client We will configure the app client settings in a manner similar to the AWS official page Using the Amazon Cognito hosted UI for sign-up and sign-in. The parameters that are important this time are the CallbackURLs and LogoutURLs properties. Specify the URLs to be redirected to after sign-in and sign-out, respectively. Enter Callback URL(s). A callback URL indicates where the user will be redirected after a successful sign-in. Enter Sign out URL(s). A sign-out URL indicates where your user will be redirected after signing out. Using the Amazon Cognito hosted UI for sign up and sign in In this case, we will specify the URL of the HTML file to be placed in the S3 bucket as described below. Set up domain \u00b6 Then, check the domain. Resources : UserPoolDomain : Type : AWS::Cognito::UserPoolDomain Properties : Domain : !Ref Prefix UserPoolId : !Ref UserPool You need to create a domain for the sign-in/sign-up page with hosted UI. After setting up an app client, you can configure the address of your sign-up and sign-in webpages. You can use an Amazon Cognito hosted domain and choose an available domain prefix, or you can use your own web address as a custom domain. Configuring a user pool domain This time, we will use the domain prefix to set the domain, passing the prefix string to the Domain property. For the US Northeast region (us-east-1), the domain containing the prefix will be as follows [prefix].auth.us-east-1.amazoncognito.com The URL for the sign-in/sign-out page with the hosted UI is as follows URL for sign-in page https://[prefix].auth.us-east-1.amazoncognito.com/login?response_type=code&client_id=[app-client-id]&redirect_uri=[callback-url] URL for sign-out page https://[prefix].auth.us-east-1.amazoncognito.com/logout?response_type=code&client_id=[app-client-id]&logout_uri=[sign-out-url] Please note that there are restrictions on the URLs that can be specified as sign-in/sign-out destinations, and they must start with \u201chttps\u201d. Prepare sign-in/sign-out destination content with S3 static website hosting \u00b6 Define S3-related resources in user-signin-s3.yaml . Resources : S3Bucket : Type : AWS::S3::Bucket Properties : BucketName : !Ref Prefix AccessControl : Private WebsiteConfiguration : IndexDocument : index.html BucketPolicy : Type : AWS::S3::BucketPolicy Properties : Bucket : !Ref Prefix PolicyDocument : Statement : Action : - s3:GetObject Effect : Allow Resource : !Sub \"arn:aws:s3:::${S3Bucket}/*\" Principal : \"*\" We will prepare the content to be displayed after sign-in and sign-out. We will use the static website hosting feature provided by S3. You can use Amazon S3 to host a static website. On a static website, individual webpages include static content. They might also contain client-side scripts. Hosting a static website using Amazon S3 The content published using this function can be accessed via two types of endpoints (website endpoint and RESTAPI endpoint), but we will use the REST API endpoint. This has to do with the URL to be specified as the sign-in/sign-out destination as described earlier. This is because REST API endpoints are the endpoints that can be used for HTTPS communication, which is a condition of the URLs that can be specified. In the case of the US Northeast region (us-east-1), the URL to access the content using this endpoint is as follows https://s3-us-east-1.amazonaws.com/mikes-website","title":"Create sign-in page in Cognito user pool"},{"location":"aws/cloud_formation/cognito_user_signin/#create-sign-in-page-in-cognito-user-pool","text":"https://awstut.com/en/2021/12/12/create-a-sign-in-page-in-the-cognito-user-pool/","title":"Create sign-in page in Cognito user pool"},{"location":"aws/cloud_formation/cognito_user_signin/#architecture","text":"","title":"Architecture"},{"location":"aws/cloud_formation/cognito_user_signin/#requirements","text":"AWS CLI S3 Bucket(Here, the bucket name is cognito-4090723305377 and region is us-east-1 )","title":"Requirements"},{"location":"aws/cloud_formation/cognito_user_signin/#usage","text":"","title":"Usage"},{"location":"aws/cloud_formation/cognito_user_signin/#template-file-modification","text":"Modify the following locations in cf-cognito-base.yaml. Parameters : TemplateBucketName : Type : String Default : cognito-4090723305377","title":"Template File Modification"},{"location":"aws/cloud_formation/cognito_user_signin/#upload-template-files-to-s3-bucket","text":"aws s3 cp . s3://cognito-409072330537/user-signin/ --recursive","title":"Upload  Template Files to S3 Bucket"},{"location":"aws/cloud_formation/cognito_user_signin/#cloudformation-stack-creation","text":"aws cloudformation create-stack --stack-name cf-cognito --template-url https://cognito-409072330537.s3.amazonaws.com/user-signin/user-signin.yaml --capabilities CAPABILITY_IAM","title":"CloudFormation Stack Creation"},{"location":"aws/cloud_formation/cognito_user_signin/#upload-html-files","text":"aws s3 cp ./html s3://fa-010/ --recursive","title":"Upload HTML files"},{"location":"aws/cloud_formation/cognito_user_signin/#cloudformation-stack","text":"This time, we will build this environment with a nested stack. For more information about nested stacks, please refer to the following page. Cloudformation Nested Stacks","title":"Cloudformation Stack"},{"location":"aws/cloud_formation/cognito_user_signin/#create-user-pool","text":"In user-signin-user-pool.yaml, we will define the Cognito-related resources. First, let\u2019s check the user pool itself. Resources : UserPool : Type : AWS::Cognito::UserPool Properties : AutoVerifiedAttributes : - email UsernameAttributes : - email UserPoolName : !Sub ${Prefix}-UserPool Schema : - AttributeDataType : String Mutable : true Name : name Required : true The AutoVerifiedAttributes property allows you to set automatic verification for the specified attributes. Amazon Cognito can automatically verify email addresses or mobile phone numbers by sending a verification code\u2014or, for email, a verification link. For email addresses, the code or link is sent in an email message. For phone numbers, the code is sent in an SMS text message. Configuring email or phone verification You can specify \u201cemail\u201d or \u201cphone_number\u201d for this property. In this case, we will specify the former so that a verification code will be automatically sent to the registered email address upon signup. The UsernameAttributes property is a parameter to specify the attributes to be used in place of the user name (username). UsernameAttributes Determines whether email addresses or phone numbers can be specified as user names when a user signs up. Possible values: phone_number or email. AWS::Cognito::UserPool UsernameAttributes As mentioned above, you can specify \u201cemail\u201d or \u201cphone_number\u201d for this property, but we will specify the former. In this case, we will specify the former. This setting will allow users to sign in using their email address instead of their user name. One thing to note about this property is that username is considered a required attribute in Cognito. The username value is a separate attribute and not the same as the name attribute. A username is always required to register a user, and it can\u2019t be changed after a user is created. User Pool attributes However, if a username is not required, such as in the case of the application we are creating this time, an email address/phone number can be used instead of a username. In this case, the user name will be set to a randomly generated ID. If your application does not require a username, you do not need to ask users to provide one. Your app can create a unique username for users in the background. This is useful if, for example, you want users to register and sign in with an email address and password. User Pool attributes The Schema property allows you to specify the attributes to be registered when signing up. In this case, we will add one of the standard attributes, name (not username). Note that email address and password are not required to be specified in this property, as they are set to \u201cemail\u201d in the UsernameAttributes property.","title":"Create User Pool"},{"location":"aws/cloud_formation/cognito_user_signin/#create-user-pool-app-client","text":"Next, check the user pool app client. Resources : UserPoolClient : Type : AWS::Cognito::UserPoolClient Properties : AccessTokenValidity : 60 # (minutes) default value. AllowedOAuthFlowsUserPoolClient : true AllowedOAuthFlows : - code - implicit AllowedOAuthScopes : - aws.cognito.signin.user.admin - email - openid - phone - profile CallbackURLs : - !Sub \"${BucketWesSiteEndpointUrl}/${SigninHtml}\" ClientName : !Sub ${Prefix}-UserPoolClient EnableTokenRevocation : true # default value. ExplicitAuthFlows : - ALLOW_CUSTOM_AUTH - ALLOW_REFRESH_TOKEN_AUTH - ALLOW_USER_SRP_AUTH IdTokenValidity : 60 # (minutes) default value. LogoutURLs : - !Sub \"${BucketWesSiteEndpointUrl}/${SignoutHtml}\" PreventUserExistenceErrors : ENABLED # default value. RefreshTokenValidity : 30 # (days) default value. SupportedIdentityProviders : - COGNITO TokenValidityUnits : AccessToken : minutes IdToken : minutes RefreshToken : days UserPoolId : !Ref UserPool By creating an app client, you will be able to perform API operations such as signup. An app is an entity within a user pool that has permission to call unauthenticated API operations. Configuring a user pool app client We will configure the app client settings in a manner similar to the AWS official page Using the Amazon Cognito hosted UI for sign-up and sign-in. The parameters that are important this time are the CallbackURLs and LogoutURLs properties. Specify the URLs to be redirected to after sign-in and sign-out, respectively. Enter Callback URL(s). A callback URL indicates where the user will be redirected after a successful sign-in. Enter Sign out URL(s). A sign-out URL indicates where your user will be redirected after signing out. Using the Amazon Cognito hosted UI for sign up and sign in In this case, we will specify the URL of the HTML file to be placed in the S3 bucket as described below.","title":"Create user pool app client"},{"location":"aws/cloud_formation/cognito_user_signin/#set-up-domain","text":"Then, check the domain. Resources : UserPoolDomain : Type : AWS::Cognito::UserPoolDomain Properties : Domain : !Ref Prefix UserPoolId : !Ref UserPool You need to create a domain for the sign-in/sign-up page with hosted UI. After setting up an app client, you can configure the address of your sign-up and sign-in webpages. You can use an Amazon Cognito hosted domain and choose an available domain prefix, or you can use your own web address as a custom domain. Configuring a user pool domain This time, we will use the domain prefix to set the domain, passing the prefix string to the Domain property. For the US Northeast region (us-east-1), the domain containing the prefix will be as follows [prefix].auth.us-east-1.amazoncognito.com The URL for the sign-in/sign-out page with the hosted UI is as follows URL for sign-in page https://[prefix].auth.us-east-1.amazoncognito.com/login?response_type=code&client_id=[app-client-id]&redirect_uri=[callback-url] URL for sign-out page https://[prefix].auth.us-east-1.amazoncognito.com/logout?response_type=code&client_id=[app-client-id]&logout_uri=[sign-out-url] Please note that there are restrictions on the URLs that can be specified as sign-in/sign-out destinations, and they must start with \u201chttps\u201d.","title":"Set up domain"},{"location":"aws/cloud_formation/cognito_user_signin/#prepare-sign-insign-out-destination-content-with-s3-static-website-hosting","text":"Define S3-related resources in user-signin-s3.yaml . Resources : S3Bucket : Type : AWS::S3::Bucket Properties : BucketName : !Ref Prefix AccessControl : Private WebsiteConfiguration : IndexDocument : index.html BucketPolicy : Type : AWS::S3::BucketPolicy Properties : Bucket : !Ref Prefix PolicyDocument : Statement : Action : - s3:GetObject Effect : Allow Resource : !Sub \"arn:aws:s3:::${S3Bucket}/*\" Principal : \"*\" We will prepare the content to be displayed after sign-in and sign-out. We will use the static website hosting feature provided by S3. You can use Amazon S3 to host a static website. On a static website, individual webpages include static content. They might also contain client-side scripts. Hosting a static website using Amazon S3 The content published using this function can be accessed via two types of endpoints (website endpoint and RESTAPI endpoint), but we will use the REST API endpoint. This has to do with the URL to be specified as the sign-in/sign-out destination as described earlier. This is because REST API endpoints are the endpoints that can be used for HTTPS communication, which is a condition of the URLs that can be specified. In the case of the US Northeast region (us-east-1), the URL to access the content using this endpoint is as follows https://s3-us-east-1.amazonaws.com/mikes-website","title":"Prepare sign-in/sign-out destination content with S3 static website hosting"},{"location":"aws/cloud_formation/cognito_user_signin/cognito_page/","text":"Create sign-in page in Cognito user pool \u00b6 Create a sign-in page using Cognito. Cognito is an authentication system service provided by AWS. Amazon Cognito provides authentication, authorization, and user management for your web and mobile apps. Your users can sign in directly with a user name and password, or through a third party such as Facebook, Amazon, Google or Apple. What is Amazon Cognito This time, we will use the hosted UI of the Cognito user pool to create a sign-in page. The Amazon Cognito Hosted UI provides you an OAuth 2.0 compliant authorization server. It includes default implementation of end user flows such as registration and authentication. Using the Amazon Cognito hosted UI for sign-up and sign-in In this article, we will build an environment that follows the contents of the AWS official website Setting up the hosted UI with the Amazon Cognito console. Create a Cognito user pool. Use the user pool to create a sign-in/sign-up function and a page for this function. Configure the user pool to register the following information when signing up Email address Password Name * not username Sign-in will be performed using the email address and password. Create an S3 bucket, enable the static website hosting feature, and install the following three HTML files. index.html: for the top page signin.html: For the page to be displayed after signing in signout.html: For the page to be displayed after signing out Architecting \u00b6 Create a CloudFormation stacks. After checking the resources for each stack, the information for the main resource created this time is as follows Name of the S3 bucket: user-signin ID of Cognito user pool: us-east-1_cbhl69qfU Cognito user pool app client ID: 1johr8c8veqbkqrr2t1sand9av Cognito user pool domain prefix: user-signin From the above, the URL for the sign-in/sign-out page with the hosted UI mentioned above is determined as follows URL for sign-in page https://user-signin.auth.us-east-1_cbhl69qfU.amazoncognito.com/login?response_type=code&client_id=1johr8c8veqbkqrr2t1sand9av&redirect_uri=https://s3-us-east-1.amazonaws.com/user-signin/signin.html https://fa-010.auth.ap-northeast-1.amazoncognito.com/login?response_type=code&client_id=5vb5gkv9rjeunvendcc1pj4089&redirect_uri=https://s3-ap-northeast-1.amazonaws.com/fa-010/signin.html URL for sign-out page https://fa-010.auth.ap-northeast-1.amazoncognito.com/logout?response_type=code&client_id=5vb5gkv9rjeunvendcc1pj4089&logout_uri=https://s3-ap-northeast-1.amazonaws.com/fa-010/signout.html Place the HTML in the S3 bucket \u00b6 Place the HTML in the S3 bucket and prepare the contents after sign-in/sign-out. Put the sign-in/sign-out URL described above in index.html, and then install it using the AWS CLI. $ aws s3 cp ./html s3://user-signin/ --recursive upload: ./html/index.html to s3://user-signin/index.html upload: ./html/signout.html to s3://user-signin/signout.html upload: ./html/signin.html to s3://user-signin/signin.html Accessing the hosted UI: Sign up \u00b6 Now that we are ready, let\u2019s actually access the hosted UI. First, access the following URL https://user-signin.s3.amazonaws.com/index.html https://s3-us-east-1.amazonaws.com/user-signin/index.html The root page, index.html, will be displayed.","title":"Create sign-in page in Cognito user pool"},{"location":"aws/cloud_formation/cognito_user_signin/cognito_page/#create-sign-in-page-in-cognito-user-pool","text":"Create a sign-in page using Cognito. Cognito is an authentication system service provided by AWS. Amazon Cognito provides authentication, authorization, and user management for your web and mobile apps. Your users can sign in directly with a user name and password, or through a third party such as Facebook, Amazon, Google or Apple. What is Amazon Cognito This time, we will use the hosted UI of the Cognito user pool to create a sign-in page. The Amazon Cognito Hosted UI provides you an OAuth 2.0 compliant authorization server. It includes default implementation of end user flows such as registration and authentication. Using the Amazon Cognito hosted UI for sign-up and sign-in In this article, we will build an environment that follows the contents of the AWS official website Setting up the hosted UI with the Amazon Cognito console. Create a Cognito user pool. Use the user pool to create a sign-in/sign-up function and a page for this function. Configure the user pool to register the following information when signing up Email address Password Name * not username Sign-in will be performed using the email address and password. Create an S3 bucket, enable the static website hosting feature, and install the following three HTML files. index.html: for the top page signin.html: For the page to be displayed after signing in signout.html: For the page to be displayed after signing out","title":"Create sign-in page in Cognito user pool"},{"location":"aws/cloud_formation/cognito_user_signin/cognito_page/#architecting","text":"Create a CloudFormation stacks. After checking the resources for each stack, the information for the main resource created this time is as follows Name of the S3 bucket: user-signin ID of Cognito user pool: us-east-1_cbhl69qfU Cognito user pool app client ID: 1johr8c8veqbkqrr2t1sand9av Cognito user pool domain prefix: user-signin From the above, the URL for the sign-in/sign-out page with the hosted UI mentioned above is determined as follows URL for sign-in page https://user-signin.auth.us-east-1_cbhl69qfU.amazoncognito.com/login?response_type=code&client_id=1johr8c8veqbkqrr2t1sand9av&redirect_uri=https://s3-us-east-1.amazonaws.com/user-signin/signin.html https://fa-010.auth.ap-northeast-1.amazoncognito.com/login?response_type=code&client_id=5vb5gkv9rjeunvendcc1pj4089&redirect_uri=https://s3-ap-northeast-1.amazonaws.com/fa-010/signin.html URL for sign-out page https://fa-010.auth.ap-northeast-1.amazoncognito.com/logout?response_type=code&client_id=5vb5gkv9rjeunvendcc1pj4089&logout_uri=https://s3-ap-northeast-1.amazonaws.com/fa-010/signout.html","title":"Architecting"},{"location":"aws/cloud_formation/cognito_user_signin/cognito_page/#place-the-html-in-the-s3-bucket","text":"Place the HTML in the S3 bucket and prepare the contents after sign-in/sign-out. Put the sign-in/sign-out URL described above in index.html, and then install it using the AWS CLI. $ aws s3 cp ./html s3://user-signin/ --recursive upload: ./html/index.html to s3://user-signin/index.html upload: ./html/signout.html to s3://user-signin/signout.html upload: ./html/signin.html to s3://user-signin/signin.html","title":"Place the HTML in the S3 bucket"},{"location":"aws/cloud_formation/cognito_user_signin/cognito_page/#accessing-the-hosted-ui-sign-up","text":"Now that we are ready, let\u2019s actually access the hosted UI. First, access the following URL https://user-signin.s3.amazonaws.com/index.html https://s3-us-east-1.amazonaws.com/user-signin/index.html The root page, index.html, will be displayed.","title":"Accessing the hosted UI: Sign up"},{"location":"aws/cloud_formation/lambda/lambda_api_gw/","text":"AWSTemplateFormatVersion : \"2010-09-09\" Description : Azure Lambda authorizer for use with AWS API Gateway Parameters : DataClassificationTag : Description : \"Classification for the data stored in this stack\" Type : String Default : \"internal\" AllowedValues : - critical - restricted - internal - public RegulatedTag : Description : \"Should the resources deployed with this stack be tracked as regulated\" Type : String Default : 'no' AllowedValues : - 'no' - 'gxp' - 'sox' ProjectTag : Description : \"Name of the project this resource is for\" Type : String ConstraintDescription : \"Can contain only alphanumeric characters, dashes, and underscores.\" AllowedPattern : \"[-_ a-zA-Z0-9]*\" MaxLength : \"64\" MinLength : \"3\" OwnerTag : Description : \"Individual(s)/Velocity Group responsible for this stack\" ConstraintDescription : \"Can contain only alphanumeric characters, spaces, dashes, and underscores.\" AllowedPattern : \"[-_/ a-zA-Z0-9]*\" MaxLength : \"64\" \"MinLength\" : \"4\" Type : String CostCenterTag : Type : String Description : \"Cost center associated with the owner of this resource. List of available options here: https://help.platforms.engineering/aws-tagging/\" Environment : Description : \"Which AWS Environment you are deploying to\" Type : String AllowedValues : - non-prod - prod Default : non-prod Metadata : AWS::CloudFormation::Interface : ParameterGroups : - Label : default : AWS Tags Parameters : - DataClassificationTag - RegulatedTag - ProjectTag - OwnerTag - CostCenterTag - Environment Conditions : CreateIAMResource : !Equals [ !Ref \"AWS::Region\" , 'us-east-1' ] Resources : LambdaFunction : Type : \"AWS::Lambda::Function\" Properties : Code : S3Bucket : !Sub mon-deploy-${AWS::Region} S3Key : \"lambda-authorizer/lambda-azureV2.0.zip\" Description : Azure Lambda authorizer for use with AWS API Gateway version 2 FunctionName : protected-cloudeng-azure-api-gateway-authorizer-V2 Handler : index.handler MemorySize : 128 Role : !If [ CreateIAMResource , !GetAtt LambdaExecutionRole.Arn , !Sub \"arn:aws:iam::${AWS::AccountId}:role/protected/cloudeng-lambda-azure-authorizer-v2-role\" ] Runtime : nodejs14.x Timeout : 30 Environment : Variables : Environment : !Ref Environment Tags : - Key : mon:env Value : !Ref Environment - Key : mon:owner Value : !Ref OwnerTag - Key : mon:cost-center Value : !Ref CostCenterTag - Key : mon:regulated Value : !Ref RegulatedTag - Key : mon:project Value : !Ref ProjectTag - Key : mon:data-classification Value : !Ref DataClassificationTag LambdaExecutionRole : Type : AWS::IAM::Role Condition : CreateIAMResource Properties : Path : /protected/ RoleName : cloudeng-lambda-azure-authorizer-v2-role ManagedPolicyArns : - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole AssumeRolePolicyDocument : Version : 2012-10-17 Statement : - Action : sts:AssumeRole Effect : Allow Principal : Service : lambda.amazonaws.com - Effect : Allow Principal : AWS : !Sub arn:aws:iam::${AWS::AccountId}:role/cloudops Action : sts:AssumeRole LambdaInvokePermission : Type : AWS::Lambda::Permission Properties : Action : lambda:InvokeFunction FunctionName : !GetAtt [ LambdaFunction , Arn ] Principal : apigateway.amazonaws.com","title":"Lambda api gw"},{"location":"aws/cloud_formation/lambda/lambda_github_oidc/","text":"lambda github oidc \u00b6 --- AWSTemplateFormatVersion : '2010-09-09' Description : Lambda for github OIDC Provider Resources : GithubOIDCProviderFunction : Type : AWS::Lambda::Function Properties : Runtime : python3.9 Handler : index.lambda_handler MemorySize : 128 Role : !Sub \"arn:aws:iam::${AWS::AccountId}:role/protected/cloudeng-github-oidc-provider-Role\" FunctionName : protected-github-custom-provider Timeout : 30 Code : S3Bucket : !Sub mon-deploy-${AWS::Region} S3Key : \"oidc-provider/github_oidc.zip\"","title":"lambda github oidc"},{"location":"aws/cloud_formation/lambda/lambda_github_oidc/#lambda-github-oidc","text":"--- AWSTemplateFormatVersion : '2010-09-09' Description : Lambda for github OIDC Provider Resources : GithubOIDCProviderFunction : Type : AWS::Lambda::Function Properties : Runtime : python3.9 Handler : index.lambda_handler MemorySize : 128 Role : !Sub \"arn:aws:iam::${AWS::AccountId}:role/protected/cloudeng-github-oidc-provider-Role\" FunctionName : protected-github-custom-provider Timeout : 30 Code : S3Bucket : !Sub mon-deploy-${AWS::Region} S3Key : \"oidc-provider/github_oidc.zip\"","title":"lambda github oidc"},{"location":"aws/cloud_formation/lambda/lambda_oidc_iam/","text":"AWSTemplateFormatVersion : '2010-09-09' Description : IAM Role and lambda for oidc provider Conditions : CreateIAMResource : !Equals [ !Ref \"AWS::Region\" , 'us-east-1' ] Resources : ClusterOIDCProviderFunction : Type : AWS::Lambda::Function Properties : Runtime : python3.7 Handler : index.lambda_handler MemorySize : 128 Role : !If [ CreateIAMResource , !GetAtt ClusterOIDCLambdaExecutionRole.Arn , !Sub \"arn:aws:iam::${AWS::AccountId}:role/protected/cloudeng-eks-oidc-provider-Role\" ] FunctionName : protected-cloudeng-eks-oidc-creator Timeout : 30 Code : S3Bucket : !Sub mon-deploy-${AWS::Region} S3Key : \"oidc-provider/oidc-provider-lambda.zip\" ClusterOIDCLambdaExecutionRole : Type : AWS::IAM::Role Condition : CreateIAMResource Properties : RoleName : cloudeng-eks-oidc-provider-Role Path : /protected/ Tags : - Key : bayer:privileged:iam:oidc Value : true AssumeRolePolicyDocument : Version : \"2012-10-17\" Statement : - Effect : Allow Principal : Service : - lambda.amazonaws.com Action : - sts:AssumeRole Policies : - PolicyName : eks-oidc-provider-policy PolicyDocument : Version : \"2012-10-17\" Statement : - Effect : Allow Action : - eks:DescribeCluster Resource : !Sub \"arn:aws:eks:*:${AWS::AccountId}:cluster/*\" - Effect : Allow Action : - iam:CreateOpenIDConnectProvider - iam:DeleteOpenIDConnectProvider Resource : \"*\" - Effect : Allow Action : - logs:CreateLogGroup - logs:CreateLogStream - logs:PutLogEvents Resource : \"*\"","title":"Lambda oidc iam"},{"location":"aws/cloud_formation/sagemaker/","text":"Sagemaker Studio Domain \u00b6 Standard Setup \u00b6 Control all aspects of account configuration, including permissions, integrations, and encryption. Advanced network security, and data encryption SageMaker Studio, and RStudio integration SageMaker Studio Projects, and Jumpstart configurable SageMaker Canvas, and Amazon services integrations IAM, or IAM Identity Center (successor to AWS SSO)","title":"Sagemaker Studio Domain"},{"location":"aws/cloud_formation/sagemaker/#sagemaker-studio-domain","text":"","title":"Sagemaker Studio Domain"},{"location":"aws/cloud_formation/sagemaker/#standard-setup","text":"Control all aspects of account configuration, including permissions, integrations, and encryption. Advanced network security, and data encryption SageMaker Studio, and RStudio integration SageMaker Studio Projects, and Jumpstart configurable SageMaker Canvas, and Amazon services integrations IAM, or IAM Identity Center (successor to AWS SSO)","title":"Standard Setup"},{"location":"aws/cloud_formation/vpc/self_reference_security_groups/","text":"Self Reference Security Groups \u00b6 Security groups are not real groups; instances that have the same security group do not really share a rule set but they each have an identical copy of the same rule set. In other words; two instances are not sitting together behind a perimeter but rather each of them sits behind an identical version of the same perimeter. As a result if you want two instances that have the same security group to talk to each other you need to make sure that your security group accepts connections from itself. It needs to be self-referencing because it is applied separately for each of those two instances. This problem was hidden to us before because our first two services were smaller and sit on the same machine. The calls between them never left the EC2 instance they are on and therefore never passed through a security group. Solution \u00b6 It is very simple to add a self-referencing ingress rule to a security group either with the API or through the web console. However if you try to add it in a CloudFormation template as you create the security group itself you\u2019ll most likely fail due to a circular dependency. You should use an SecurityGroupIngress resource rather than an inline security group ingress rule. SecurityGroup : Type : AWS::EC2::SecurityGroup Properties : GroupName : security-group VpcId : !Ref VPC #adding this to avoid circular dependency SelfReferenceRule : Type : AWS::EC2::SecurityGroupIngress Properties : IpProtocol : \"-1\" #-1 used for all traffic SourceSecurityGroupId : !Ref SecurityGroup GroupId : !Ref SecurityGroup","title":"Self Reference SGs"},{"location":"aws/cloud_formation/vpc/self_reference_security_groups/#self-reference-security-groups","text":"Security groups are not real groups; instances that have the same security group do not really share a rule set but they each have an identical copy of the same rule set. In other words; two instances are not sitting together behind a perimeter but rather each of them sits behind an identical version of the same perimeter. As a result if you want two instances that have the same security group to talk to each other you need to make sure that your security group accepts connections from itself. It needs to be self-referencing because it is applied separately for each of those two instances. This problem was hidden to us before because our first two services were smaller and sit on the same machine. The calls between them never left the EC2 instance they are on and therefore never passed through a security group.","title":"Self Reference Security Groups"},{"location":"aws/cloud_formation/vpc/self_reference_security_groups/#solution","text":"It is very simple to add a self-referencing ingress rule to a security group either with the API or through the web console. However if you try to add it in a CloudFormation template as you create the security group itself you\u2019ll most likely fail due to a circular dependency. You should use an SecurityGroupIngress resource rather than an inline security group ingress rule. SecurityGroup : Type : AWS::EC2::SecurityGroup Properties : GroupName : security-group VpcId : !Ref VPC #adding this to avoid circular dependency SelfReferenceRule : Type : AWS::EC2::SecurityGroupIngress Properties : IpProtocol : \"-1\" #-1 used for all traffic SourceSecurityGroupId : !Ref SecurityGroup GroupId : !Ref SecurityGroup","title":"Solution"},{"location":"aws/cloud_formation/vpc/vpc_private_subnets/","text":"VPC with only private subnets \u00b6 Provision a VPC with two private subnets using CloudFormation. This CF template can be modified to fit general use cases or used as is. AWS CloudFormation is the most reliable method to create and manage a Virtual Private Cloud (VPC), complete with subnets, NATting, route tables, etc. The emphasis is on use of CF and Infrastructure as Code to build and manage resources in AWS, less about the issues of VPC design. A private subnet does not route traffic to an internet gateway. Resources in a private subnet do not have public IP addresses, and can only communicate with resources in other subnets in the same VPC. One or more subnets can be placed in a VPC. It is possible to mix and match public and private subnets in a VPC, allowing some resources in the VPC to access the internet, and some to only access other resources in the VPC. A VPC with public subnets allows instances to access the internet. If those instances have public IP addresses, they can also be access from the internet. Create VPC with private subnets \u00b6 A VPC with private subnets is the easiest to configure. This CF template provisions the VPC with two private subnets. CloudFormation Template: VPC with private subnets Parameters : Tag : Type : String Resources : VPC : Type : \"AWS::EC2::VPC\" Properties : CidrBlock : \"10.0.0.0/16\" Tags : - Key : \"Name\" Value : !Ref \"Tag\" SubnetA : Type : \"AWS::EC2::Subnet\" Properties : AvailabilityZone : !Select - 0 - !GetAZs Ref : 'AWS::Region' VpcId : !Ref \"VPC\" CidrBlock : \"10.0.0.0/24\" SubnetB : Type : \"AWS::EC2::Subnet\" Properties : AvailabilityZone : !Select - 1 - !GetAZs Ref : 'AWS::Region' VpcId : !Ref \"VPC\" CidrBlock : \"10.0.1.0/24\" RouteTable : Type : \"AWS::EC2::RouteTable\" Properties : VpcId : !Ref \"VPC\" Outputs : VpcId : Description : The VPC ID Value : !Ref VPC The name of the VPC is defined by the Tag parameter: Tag : Type : String The VPC is defined as a AWSEC2VPC resource. The CidrBlock property defines the Classless Inter-Domain Routing IP block defining the range of IP addresses available to the subnets associated with the VPC. 10.0.0.0/16 defines a block of IP addresses that all start with 10.0 . Note the VPC has a tag called Name. The value of this tag is displayed in the AWS web console: VPC : Type : \"AWS::EC2::VPC\" Properties : CidrBlock : \"10.0.0.0/16\" Tags : - Key : \"Name\" Value : !Ref \"Tag\" Next, you define two subnets with AWSEC2Subnet resources. Subnets are placed in availability zones (AZs), which are isolated locations in a region. AZs have codes like us-east-1 or ap-southeast-2, which are based on the region that the AZs are located in. Rather than hard code these AZ names, you can use the Select intrinsic function to return items from the GetAZs array, which will return an available AZ for the region in which your VPC is being created. Each subnet has its own unique CIDR block. The first subnet defines the block 10.0.0.0/24 , which means all resources in this subnet have IP addresses that start with 10.0.0 . The second subnet defines the block 10.0.1.0/24 , which means all resources in the second subnet have IP addresses starting with 10.0.1 : SubnetA : Type : \"AWS::EC2::Subnet\" Properties : AvailabilityZone : !Select - 0 - !GetAZs Ref : 'AWS::Region' VpcId : !Ref \"VPC\" CidrBlock : \"10.0.0.0/24\" SubnetB : Type : \"AWS::EC2::Subnet\" Properties : AvailabilityZone : !Select - 1 - !GetAZs Ref : 'AWS::Region' VpcId : !Ref \"VPC\" CidrBlock : \"10.0.1.0/24\" Network connectivity between subnets is defined by a route table, created by the AWSEC2RouteTable resource. The default route table allows connectivity between instances in each subnet, so you don't specify any additional routes here: RouteTable : Type : \"AWS::EC2::RouteTable\" Properties : VpcId : !Ref \"VPC\"","title":"VPC with private subnets"},{"location":"aws/cloud_formation/vpc/vpc_private_subnets/#vpc-with-only-private-subnets","text":"Provision a VPC with two private subnets using CloudFormation. This CF template can be modified to fit general use cases or used as is. AWS CloudFormation is the most reliable method to create and manage a Virtual Private Cloud (VPC), complete with subnets, NATting, route tables, etc. The emphasis is on use of CF and Infrastructure as Code to build and manage resources in AWS, less about the issues of VPC design. A private subnet does not route traffic to an internet gateway. Resources in a private subnet do not have public IP addresses, and can only communicate with resources in other subnets in the same VPC. One or more subnets can be placed in a VPC. It is possible to mix and match public and private subnets in a VPC, allowing some resources in the VPC to access the internet, and some to only access other resources in the VPC. A VPC with public subnets allows instances to access the internet. If those instances have public IP addresses, they can also be access from the internet.","title":"VPC with only private subnets"},{"location":"aws/cloud_formation/vpc/vpc_private_subnets/#create-vpc-with-private-subnets","text":"A VPC with private subnets is the easiest to configure. This CF template provisions the VPC with two private subnets. CloudFormation Template: VPC with private subnets Parameters : Tag : Type : String Resources : VPC : Type : \"AWS::EC2::VPC\" Properties : CidrBlock : \"10.0.0.0/16\" Tags : - Key : \"Name\" Value : !Ref \"Tag\" SubnetA : Type : \"AWS::EC2::Subnet\" Properties : AvailabilityZone : !Select - 0 - !GetAZs Ref : 'AWS::Region' VpcId : !Ref \"VPC\" CidrBlock : \"10.0.0.0/24\" SubnetB : Type : \"AWS::EC2::Subnet\" Properties : AvailabilityZone : !Select - 1 - !GetAZs Ref : 'AWS::Region' VpcId : !Ref \"VPC\" CidrBlock : \"10.0.1.0/24\" RouteTable : Type : \"AWS::EC2::RouteTable\" Properties : VpcId : !Ref \"VPC\" Outputs : VpcId : Description : The VPC ID Value : !Ref VPC The name of the VPC is defined by the Tag parameter: Tag : Type : String The VPC is defined as a AWSEC2VPC resource. The CidrBlock property defines the Classless Inter-Domain Routing IP block defining the range of IP addresses available to the subnets associated with the VPC. 10.0.0.0/16 defines a block of IP addresses that all start with 10.0 . Note the VPC has a tag called Name. The value of this tag is displayed in the AWS web console: VPC : Type : \"AWS::EC2::VPC\" Properties : CidrBlock : \"10.0.0.0/16\" Tags : - Key : \"Name\" Value : !Ref \"Tag\" Next, you define two subnets with AWSEC2Subnet resources. Subnets are placed in availability zones (AZs), which are isolated locations in a region. AZs have codes like us-east-1 or ap-southeast-2, which are based on the region that the AZs are located in. Rather than hard code these AZ names, you can use the Select intrinsic function to return items from the GetAZs array, which will return an available AZ for the region in which your VPC is being created. Each subnet has its own unique CIDR block. The first subnet defines the block 10.0.0.0/24 , which means all resources in this subnet have IP addresses that start with 10.0.0 . The second subnet defines the block 10.0.1.0/24 , which means all resources in the second subnet have IP addresses starting with 10.0.1 : SubnetA : Type : \"AWS::EC2::Subnet\" Properties : AvailabilityZone : !Select - 0 - !GetAZs Ref : 'AWS::Region' VpcId : !Ref \"VPC\" CidrBlock : \"10.0.0.0/24\" SubnetB : Type : \"AWS::EC2::Subnet\" Properties : AvailabilityZone : !Select - 1 - !GetAZs Ref : 'AWS::Region' VpcId : !Ref \"VPC\" CidrBlock : \"10.0.1.0/24\" Network connectivity between subnets is defined by a route table, created by the AWSEC2RouteTable resource. The default route table allows connectivity between instances in each subnet, so you don't specify any additional routes here: RouteTable : Type : \"AWS::EC2::RouteTable\" Properties : VpcId : !Ref \"VPC\"","title":"Create VPC with private subnets"},{"location":"aws/cloud_formation/vpc/vpc_public_private_subnets/","text":"VPC with Public and Private Subnets \u00b6 Provision a VPC with public and private subnets. CF Template Anatomy \u00b6 Step 1. Create VPC and Internet Gateway \u00b6 Firstly, we start by creating a VPC with a size /16 IPv4 CIDR block. This provides up to 65,536 private IPv4 addresses. We might hard code the CIDR block for VPC but it\u2019s better to define it as an input parameter so a user can either customize the IP ranges or use a default value. Parameters : paramVpcCIDR : Description : Enter the IP range (CIDR notation) for VPC Type : String Default : 10.192.0.0/16 Resources : # a) Create a VPC myVPC : Type : AWS::EC2::VPC Properties : CidrBlock : !Ref paramVpcCIDR EnableDnsSupport : true EnableDnsHostnames : true Tags : - Key : CloudFormationLab Value : !Ref paramUniqueName CidrBlock - the IP address range available to our VPC EnableDnsSupport - if set to true, AWS will resolve DNS hostnames to any instance within the VPC\u2019s IP address EnableDnsHostnames - if set to true, instances get allocated DNS hostnames by default Internet Gateway \u00b6 Secondly, we need to create an Internet Gateway. An Internet Gateway is a logical connection between a VPC and the Internet. If there is no Internet Gateway, then there is no connection between the VPC and the Internet. # b) Create a Internet Gateway myInternetGateway : Type : AWS::EC2::InternetGateway Properties : Tags : - Key : CloudFormationLab Value : !Ref paramUniqueName An internet gateway enables resources in your public subnets (such as EC2 instances) to connect to the internet if the resource has a public IPv4 address or an IPv6 address. Similarly, resources on the internet can initiate a connection to resources in your subnet using the public IPv4 address or IPv6 address. For example, an internet gateway enables you to connect to an EC2 instance in AWS using your local computer. An internet gateway provides a target in your VPC route tables for internet-routable traffic. For communication using IPv4, the internet gateway also performs network address translation (NAT). For communication using IPv6, NAT is not needed because IPv6 addresses are public. For more information, see AWS Reference: IP addresses and NAT. Public and private subnets \u00b6 If a subnet is associated with a route table that has a route to an internet gateway, it's known as a public subnet. If a subnet is associated with a route table that does not have a route to an internet gateway, it's known as a private subnet. In your public subnet's route table, you can specify a route for the internet gateway to all destinations not explicitly known to the route table ( 0.0.0.0/0 for IPv4 or ::/0 for IPv6). Alternatively, you can scope the route to a narrower range of IP addresses; for example, the public IPv4 addresses of your company\u2019s public endpoints outside of AWS, or the Elastic IP addresses of other Amazon EC2 instances outside your VPC. IP addresses and NAT \u00b6 To enable communication over the internet for IPv4, your instance must have a public IPv4 address. You can either configure your VPC to automatically assign public IPv4 addresses to your instances, or you can assign Elastic IP addresses to your instances. Your instance is only aware of the private (internal) IP address space defined within the VPC and subnet. The internet gateway logically provides the one-to-one NAT on behalf of your instance, so that when traffic leaves your VPC subnet and goes to the internet, the reply address field is set to the public IPv4 address or Elastic IP address of your instance, and not its private IP address. Conversely, traffic that's destined for the public IPv4 address or Elastic IP address of your instance has its destination address translated into the instance's private IPv4 address before the traffic is delivered to the VPC. Attach Internet Gateway \u00b6 And finally, let\u2019s attach the Internet Gateway to the VPC # c) Attach the Internet Gateway to the VPC myVPCGatewayAttachment : Type : AWS::EC2::VPCGatewayAttachment Properties : VpcId : !Ref myVPC InternetGatewayId : !Ref myInternetGateway With that, we have already built a very basic VPC. Step 2. Create a public route table and public subnets across two AZs \u00b6 Placing our instances in multiple AZs increase the availability of our resources and improves the fault tolerance in our applications. If one AZ experiences an outage, you might want to redirect the traffic to the other AZ. Having said that, we are going to follow AWS best practices to create subnets - each in a different AZ. Firstly, let\u2019s create a custom route table for all public subnets and name it public route table. We need it to control the routing for the public subnets which we are about to create. # a) Create a public route table for the VPC (will be public once it is associated with the Internet Gateway) myPublicRouteTable : Type : AWS::EC2::RouteTable Properties : VpcId : !Ref myVPC Tags : - Key : Name Value : !Ref paramUniqueName Secondly, we need to add a new route to the public route table that points all traffic (0.0.0.0/0) to the Internet Gateway. # b) Associate the public route table with the Internet Gateway myPublicRoute : Type : AWS::EC2::Route DependsOn : myVPCGatewayAttachment Properties : RouteTableId : !Ref myPublicRouteTable DestinationCidrBlock : 0.0.0.0/0 GatewayId : !Ref myInternetGateway DestinationCidrBlock - it specifies which traffic we want this route to be applied to. In this case, we apply it to all traffic using the 0.0.0.0/0 CIDR block GatewayId - it specifies where traffic matching the CIDR block should be directed Note, when you add a DependsOn attribute to a resource, that resource is created only after the creation of the resource specified in the DependsOn attribute. Thirdly, once we are done with the public route table, time to create two public subnets with a size /24 IPv4 CIDR block in each of two AZs. This provides up to 256 addresses per subnet, a few of which are reserved for AWS use. It\u2019s better to define CIDR blocks for both subnets as input parameters so a user can either customize the IP ranges or use a default range. Parameters : # etc paramPublicSubnet1CIDR : Description : Enter the IP range (CIDR notation) for the public subnet in AZ A Type : String Default : 10.192.10.0/24 paramPublicSubnet2CIDR : Description : Enter the IP range (CIDR notation) for the public subnet in AZ B Type : String Default : 10.192.11.0/24 Important! The CIDR blocks of two subnets must not overlap with the CIDR block that's associated with the VPC. Subnets must exist within a VPC, so this is how we associate these two subnets within our VPC: # c) Create a public subnet in AZ 1 (will be public once it is associated with public route table) myPublicSubnet1 : Type : AWS::EC2::Subnet Properties : VpcId : !Ref myVPC AvailabilityZone : !Select [ 0 , !GetAZs '' ] # AZ 1 CidrBlock : !Ref paramPublicSubnet1CIDR MapPublicIpOnLaunch : true Tags : - Key : Name Value : !Ref paramUniqueName # Create a public subnet in AZ 2 (will be public once it is associated with public route table) myPublicSubnet2 : Type : AWS::EC2::Subnet Properties : VpcId : !Ref myVPC AvailabilityZone : !Select [ 1 , !GetAZs '' ] # AZ 2 CidrBlock : !Ref paramPublicSubnet2CIDR MapPublicIpOnLaunch : true Tags : - Key : Name Value : !Ref paramUniqueName By setting MapPublicIpOnLaunch to true instances launched into the subnet will be allocated a public IP address by default. This means that any instances in this subnet will be reachable from the Internet via the Internet Gateway attached to the VPC. And finally, it\u2019s important to associate the route table with both public subnets: # d) Associate the public route table with the public subnet in AZ 1 myPublicSubnet1RouteTableAssociation : Type : AWS::EC2::SubnetRouteTableAssociation Properties : RouteTableId : !Ref myPublicRouteTable SubnetId : !Ref myPublicSubnet1 # Associate the public route table with the public subnet in AZ 2 myPublicSubnet2RouteTableAssociation : Type : AWS::EC2::SubnetRouteTableAssociation Properties : RouteTableId : !Ref myPublicRouteTable SubnetId : !Ref myPublicSubnet2 At that point, we are done with public subnets, now let\u2019s create private subnets Step 3. Create NAT Gateways, private route tables and private subnets across two AZs \u00b6 We don't want instances within our private subnets to be reachable from the public internet. But we do want these instances to be able to initiate outbound connections. Also we want them to be able to do this without having public IP addresses. That is when NAT gateway comes in handy. NAT gateway enables instances in a private subnet to connect to the internet or other AWS services, but prevent the internet from initiating a connection with those instances. It will have a public IP address and will be associated with a public subnet. Private instances in private subnets will be able to use this to initiate outbound connections. But the NAT will not allow the reverse, a party on the public internet cannot use the NAT to connect to our private instances. Thus, we need an Elastic IP (EIP) address and a NAT gateway. Not even one, but two! Because a single NAT Gateway in a single AZ has redundancy within that AZ only, so if there are any zonal issues then instances in other AZs would have no route to the internet. To remain highly available, we need a NAT Gateway in each AZ and a different route table for each AZ. # a) Specify an Elastic IP (EIP) address for a NAT Gateway in AZ 1 myEIPforNatGateway1 : Type : AWS::EC2::EIP DependsOn : myVPCGatewayAttachment Properties : Domain : vpc # Specify an Elastic IP (EIP) address for a NAT Gateway in AZ 2 myEIPforNatGateway2 : Type : AWS::EC2::EIP DependsOn : myVPCGatewayAttachment Properties : Domain : vpc # b) Create a NAT Gateway in the public subnet for AZ 1 myNatGateway1 : Type : AWS::EC2::NatGateway Properties : AllocationId : !GetAtt myEIPforNatGateway1.AllocationId SubnetId : !Ref myPublicSubnet1 # Create a NAT Gateway in the public subnet for AZ 2 myNatGateway2 : Type : AWS::EC2::NatGateway Properties : AllocationId : !GetAtt myEIPforNatGateway2.AllocationId SubnetId : !Ref myPublicSubnet2 # c) Create a private route table for AZ 1 myPrivateRouteTable1 : Type : AWS::EC2::RouteTable Properties : VpcId : !Ref myVPC Tags : - Key : Name Value : !Ref paramUniqueName # Create a private route table for AZ 2 myPrivateRouteTable2 : Type : AWS::EC2::RouteTable Properties : VpcId : !Ref myVPC Tags : - Key : Name Value : !Ref paramUniqueName # d) Associate the private route table with the Nat Gateway in AZ 1 myPrivateRouteForAz1 : Type : AWS::EC2::Route DependsOn : myVPCGatewayAttachment Properties : RouteTableId : !Ref myPrivateRouteTable1 DestinationCidrBlock : 0.0.0.0/0 NatGatewayId : !Ref myNatGateway1 # Associate the private route table with the Nat Gateway in AZ 2 myPrivateRouteForAz2 : Type : AWS::EC2::Route DependsOn : myVPCGatewayAttachment Properties : RouteTableId : !Ref myPrivateRouteTable2 DestinationCidrBlock : 0.0.0.0/0 NatGatewayId : !Ref myNatGateway2 In the sample above, we created and specified an Elastic IP address to associate with the NAT gateway for each AZ. After creating a NAT gateway, we created and updated the route table associated with each private subnet to point internet-bound traffic to the NAT gateway. This enables instances in our private subnets to communicate with the internet. And our final step is to create two private subnets with a size /24 IPv4 CIDR block in each of two AZs. And then associate the private route table with the private subnet for each AZ. Parameters : # etc paramPrivateSubnet1CIDR : Description : Enter the IP range (CIDR notation) for the private subnet in AZ A Type : String Default : 10.192.20.0/24 paramPrivateSubnet2CIDR : Description : Enter the IP range (CIDR notation) for the private subnet in AZ B Type : String Default : 10.192.21.0/24 # e) Create a private subnet in AZ 1 myPrivateSubnet1 : Type : AWS::EC2::Subnet Properties : VpcId : !Ref myVPC AvailabilityZone : !Select [ 0 , !GetAZs '' ] # AZ 1 CidrBlock : !Ref paramPrivateSubnet1CIDR MapPublicIpOnLaunch : true Tags : - Key : Name Value : !Ref paramUniqueName # Create a private subnet in AZ 2 myPrivateSubnet2 : Type : AWS::EC2::Subnet Properties : VpcId : !Ref myVPC AvailabilityZone : !Select [ 1 , !GetAZs '' ] # AZ 2 CidrBlock : !Ref paramPrivateSubnet2CIDR MapPublicIpOnLaunch : true Tags : - Key : Name Value : !Ref paramUniqueName # f) Associate the private route table with the private subnet in AZ 1 myPrivateSubnet1RouteTableAssociation : Type : AWS::EC2::SubnetRouteTableAssociation Properties : RouteTableId : !Ref myPrivateRouteTable1 SubnetId : !Ref myPrivateSubnet1 # Associate the private route table with the private subnet in AZ 2 myPrivateSubnet2RouteTableAssociation : Type : AWS::EC2::SubnetRouteTableAssociation Properties : RouteTableId : !Ref myPrivateRouteTable2 SubnetId : !Ref myPrivateSubnet2 Optionally, we can add an Output section to get the list of newly created private and public subnets Outputs : outputVPC : Description : A reference to the created VPC Value : !Ref myVPC outputPublicSubnets : Description : A list of the public subnets Value : !Join [ \",\" , [ !Ref myPublicSubnet1 , !Ref myPublicSubnet2 ]] outputPrivateSubnets : Description : A list of the private subnets Value : !Join [ \",\" , [ !Ref myPrivateSubnet1 , !Ref myPrivateSubnet2 ]] CF Template \u00b6 ## =================== VERSION =================== # AWSTemplateFormatVersion : 2010-09-09 ## =================== DESCRIPTION =================== # Description : >- AWS CloudFormation sample template. Create a custom VPC with a pair of public and private subnets spread across two AZs ## =================== PARAMETERS =================== # Parameters : paramVpcCIDR : Description : Enter the IP range (CIDR notation) for VPC Type : String Default : 10.192.0.0/16 paramPublicSubnet1CIDR : Description : Enter the IP range (CIDR notation) for the public subnet in AZ A Type : String Default : 10.192.10.0/24 paramPublicSubnet2CIDR : Description : Enter the IP range (CIDR notation) for the public subnet in AZ B Type : String Default : 10.192.11.0/24 paramPrivateSubnet1CIDR : Description : Enter the IP range (CIDR notation) for the private subnet in AZ A Type : String Default : 10.192.20.0/24 paramPrivateSubnet2CIDR : Description : Enter the IP range (CIDR notation) for the private subnet in AZ B Type : String Default : 10.192.21.0/24 paramUniqueName : Description : Give a unique name for \"CloudFormationLab\" tag value Type : String Default : DaenerysTargaryen ## =================== RESOURCES =================== # Resources : # ---------------- Step 1 ---------------- # a) Create a VPC myVPC : Type : AWS::EC2::VPC Properties : CidrBlock : !Ref paramVpcCIDR EnableDnsSupport : true # let instances in the VPC get DNS hostnames EnableDnsHostnames : true # allow DNS resolution Tags : - Key : MasteringCF Value : !Ref paramUniqueName # b) Create a Internet Gateway myInternetGateway : Type : AWS::EC2::InternetGateway Properties : Tags : - Key : MasteringCF Value : !Ref paramUniqueName # c) Attach the Internet Gateway to the VPC myVPCGatewayAttachment : Type : AWS::EC2::VPCGatewayAttachment Properties : VpcId : !Ref myVPC InternetGatewayId : !Ref myInternetGateway # ---------------- Step 2 ---------------- # a) Create a public route table for the VPC (will be public once it is associated with the Internet Gateway) myPublicRouteTable : Type : AWS::EC2::RouteTable Properties : VpcId : !Ref myVPC Tags : - Key : MasteringCF Value : !Ref paramUniqueName # b) Associate the public route table with the Internet Gateway myPublicRoute : Type : AWS::EC2::Route DependsOn : myVPCGatewayAttachment Properties : RouteTableId : !Ref myPublicRouteTable DestinationCidrBlock : 0.0.0.0/0 GatewayId : !Ref myInternetGateway # c) Create a public subnet in AZ 1 (will be public once it is associated with public route table) myPublicSubnet1 : Type : AWS::EC2::Subnet Properties : VpcId : !Ref myVPC AvailabilityZone : !Select [ 0 , !GetAZs '' ] # AZ 1 CidrBlock : !Ref paramPublicSubnet1CIDR MapPublicIpOnLaunch : true # allow instances launched in this subnet receive a public IPv4 address Tags : - Key : MasteringCF Value : !Ref paramUniqueName # Create a public subnet in AZ 2 (will be public once it is associated with public route table) myPublicSubnet2 : Type : AWS::EC2::Subnet Properties : VpcId : !Ref myVPC AvailabilityZone : !Select [ 1 , !GetAZs '' ] # AZ 2 CidrBlock : !Ref paramPublicSubnet2CIDR MapPublicIpOnLaunch : true # allow instances launched in this subnet receive a public IPv4 address Tags : - Key : MasteringCF Value : !Ref paramUniqueName # d) Associate the public route table with the public subnet in AZ 1 myPublicSubnet1RouteTableAssociation : Type : AWS::EC2::SubnetRouteTableAssociation Properties : RouteTableId : !Ref myPublicRouteTable SubnetId : !Ref myPublicSubnet1 # Associate the public route table with the public subnet in AZ 2 myPublicSubnet2RouteTableAssociation : Type : AWS::EC2::SubnetRouteTableAssociation Properties : RouteTableId : !Ref myPublicRouteTable SubnetId : !Ref myPublicSubnet2 # ---------------- Step 3 ---------------- # a) Specify an Elastic IP (EIP) address for a NAT Gateway in AZ 1 myEIPforNatGateway1 : Type : AWS::EC2::EIP DependsOn : myVPCGatewayAttachment Properties : Domain : vpc # if the region supports EC2-Classic, the default is \"standard\", otherwise - \"vpc\" Tags : - Key : MasteringCF Value : !Ref paramUniqueName # Specify an Elastic IP (EIP) address for a NAT Gateway in AZ 2 myEIPforNatGateway2 : Type : AWS::EC2::EIP DependsOn : myVPCGatewayAttachment Properties : Domain : vpc # if the region supports EC2-Classic, the default is \"standard\", otherwise - \"vpc\" Tags : - Key : MasteringCF Value : !Ref paramUniqueName # b) Create a NAT Gateway in the public subnet for AZ 1 myNatGateway1 : Type : AWS::EC2::NatGateway Properties : AllocationId : !GetAtt myEIPforNatGateway1.AllocationId SubnetId : !Ref myPublicSubnet1 Tags : - Key : MasteringCF Value : !Ref paramUniqueName # Create a NAT Gateway in the public subnet for AZ 2 myNatGateway2 : Type : AWS::EC2::NatGateway Properties : AllocationId : !GetAtt myEIPforNatGateway2.AllocationId SubnetId : !Ref myPublicSubnet2 Tags : - Key : MasteringCF Value : !Ref paramUniqueName # c) Create a private route table for AZ 1 myPrivateRouteTable1 : Type : AWS::EC2::RouteTable Properties : VpcId : !Ref myVPC Tags : - Key : MasteringCF Value : !Ref paramUniqueName # Create a private route table for AZ 2 myPrivateRouteTable2 : Type : AWS::EC2::RouteTable Properties : VpcId : !Ref myVPC Tags : - Key : MasteringCF Value : !Ref paramUniqueName # d) Associate the private route table with the Nat Gateway in AZ 1 myPrivateRouteForAz1 : Type : AWS::EC2::Route DependsOn : myVPCGatewayAttachment Properties : RouteTableId : !Ref myPrivateRouteTable1 DestinationCidrBlock : 0.0.0.0/0 NatGatewayId : !Ref myNatGateway1 # Associate the private route table with the Nat Gateway in AZ 2 myPrivateRouteForAz2 : Type : AWS::EC2::Route DependsOn : myVPCGatewayAttachment Properties : RouteTableId : !Ref myPrivateRouteTable2 DestinationCidrBlock : 0.0.0.0/0 NatGatewayId : !Ref myNatGateway2 # e) Create a private subnet in AZ 1 myPrivateSubnet1 : Type : AWS::EC2::Subnet Properties : VpcId : !Ref myVPC AvailabilityZone : !Select [ 0 , !GetAZs '' ] # AZ 1 CidrBlock : !Ref paramPrivateSubnet1CIDR MapPublicIpOnLaunch : false # private subnet doesn't need public IP Tags : - Key : MasteringCF Value : !Ref paramUniqueName # Create a private subnet in AZ 2 myPrivateSubnet2 : Type : AWS::EC2::Subnet Properties : VpcId : !Ref myVPC AvailabilityZone : !Select [ 1 , !GetAZs '' ] # AZ 2 CidrBlock : !Ref paramPrivateSubnet2CIDR MapPublicIpOnLaunch : false # private subnet doesn't need public IP Tags : - Key : MasteringCF Value : !Ref paramUniqueName # f) Associate the private route table with the private subnet in AZ 1 myPrivateSubnet1RouteTableAssociation : Type : AWS::EC2::SubnetRouteTableAssociation Properties : RouteTableId : !Ref myPrivateRouteTable1 SubnetId : !Ref myPrivateSubnet1 # Associate the private route table with the private subnet in AZ 2 myPrivateSubnet2RouteTableAssociation : Type : AWS::EC2::SubnetRouteTableAssociation Properties : RouteTableId : !Ref myPrivateRouteTable2 SubnetId : !Ref myPrivateSubnet2 ## =================== OUTPUTS =================== # Outputs : outputVPC : Description : A reference to the created VPC Value : !Ref myVPC outputPublicSubnets : Description : A list of the public subnets Value : !Join [ \",\" , [ !Ref myPublicSubnet1 , !Ref myPublicSubnet2 ]] outputPrivateSubnets : Description : A list of the private subnets Value : !Join [ \",\" , [ !Ref myPrivateSubnet1 , !Ref myPrivateSubnet2 ]]","title":"VPC public and priv subnets"},{"location":"aws/cloud_formation/vpc/vpc_public_private_subnets/#vpc-with-public-and-private-subnets","text":"Provision a VPC with public and private subnets.","title":"VPC with Public and Private Subnets"},{"location":"aws/cloud_formation/vpc/vpc_public_private_subnets/#cf-template-anatomy","text":"","title":"CF Template Anatomy"},{"location":"aws/cloud_formation/vpc/vpc_public_private_subnets/#step-1-create-vpc-and-internet-gateway","text":"Firstly, we start by creating a VPC with a size /16 IPv4 CIDR block. This provides up to 65,536 private IPv4 addresses. We might hard code the CIDR block for VPC but it\u2019s better to define it as an input parameter so a user can either customize the IP ranges or use a default value. Parameters : paramVpcCIDR : Description : Enter the IP range (CIDR notation) for VPC Type : String Default : 10.192.0.0/16 Resources : # a) Create a VPC myVPC : Type : AWS::EC2::VPC Properties : CidrBlock : !Ref paramVpcCIDR EnableDnsSupport : true EnableDnsHostnames : true Tags : - Key : CloudFormationLab Value : !Ref paramUniqueName CidrBlock - the IP address range available to our VPC EnableDnsSupport - if set to true, AWS will resolve DNS hostnames to any instance within the VPC\u2019s IP address EnableDnsHostnames - if set to true, instances get allocated DNS hostnames by default","title":"Step 1. Create VPC and Internet Gateway"},{"location":"aws/cloud_formation/vpc/vpc_public_private_subnets/#internet-gateway","text":"Secondly, we need to create an Internet Gateway. An Internet Gateway is a logical connection between a VPC and the Internet. If there is no Internet Gateway, then there is no connection between the VPC and the Internet. # b) Create a Internet Gateway myInternetGateway : Type : AWS::EC2::InternetGateway Properties : Tags : - Key : CloudFormationLab Value : !Ref paramUniqueName An internet gateway enables resources in your public subnets (such as EC2 instances) to connect to the internet if the resource has a public IPv4 address or an IPv6 address. Similarly, resources on the internet can initiate a connection to resources in your subnet using the public IPv4 address or IPv6 address. For example, an internet gateway enables you to connect to an EC2 instance in AWS using your local computer. An internet gateway provides a target in your VPC route tables for internet-routable traffic. For communication using IPv4, the internet gateway also performs network address translation (NAT). For communication using IPv6, NAT is not needed because IPv6 addresses are public. For more information, see AWS Reference: IP addresses and NAT.","title":"Internet Gateway"},{"location":"aws/cloud_formation/vpc/vpc_public_private_subnets/#public-and-private-subnets","text":"If a subnet is associated with a route table that has a route to an internet gateway, it's known as a public subnet. If a subnet is associated with a route table that does not have a route to an internet gateway, it's known as a private subnet. In your public subnet's route table, you can specify a route for the internet gateway to all destinations not explicitly known to the route table ( 0.0.0.0/0 for IPv4 or ::/0 for IPv6). Alternatively, you can scope the route to a narrower range of IP addresses; for example, the public IPv4 addresses of your company\u2019s public endpoints outside of AWS, or the Elastic IP addresses of other Amazon EC2 instances outside your VPC.","title":"Public and private subnets"},{"location":"aws/cloud_formation/vpc/vpc_public_private_subnets/#ip-addresses-and-nat","text":"To enable communication over the internet for IPv4, your instance must have a public IPv4 address. You can either configure your VPC to automatically assign public IPv4 addresses to your instances, or you can assign Elastic IP addresses to your instances. Your instance is only aware of the private (internal) IP address space defined within the VPC and subnet. The internet gateway logically provides the one-to-one NAT on behalf of your instance, so that when traffic leaves your VPC subnet and goes to the internet, the reply address field is set to the public IPv4 address or Elastic IP address of your instance, and not its private IP address. Conversely, traffic that's destined for the public IPv4 address or Elastic IP address of your instance has its destination address translated into the instance's private IPv4 address before the traffic is delivered to the VPC.","title":"IP addresses and NAT"},{"location":"aws/cloud_formation/vpc/vpc_public_private_subnets/#attach-internet-gateway","text":"And finally, let\u2019s attach the Internet Gateway to the VPC # c) Attach the Internet Gateway to the VPC myVPCGatewayAttachment : Type : AWS::EC2::VPCGatewayAttachment Properties : VpcId : !Ref myVPC InternetGatewayId : !Ref myInternetGateway With that, we have already built a very basic VPC.","title":"Attach Internet Gateway"},{"location":"aws/cloud_formation/vpc/vpc_public_private_subnets/#step-2-create-a-public-route-table-and-public-subnets-across-two-azs","text":"Placing our instances in multiple AZs increase the availability of our resources and improves the fault tolerance in our applications. If one AZ experiences an outage, you might want to redirect the traffic to the other AZ. Having said that, we are going to follow AWS best practices to create subnets - each in a different AZ. Firstly, let\u2019s create a custom route table for all public subnets and name it public route table. We need it to control the routing for the public subnets which we are about to create. # a) Create a public route table for the VPC (will be public once it is associated with the Internet Gateway) myPublicRouteTable : Type : AWS::EC2::RouteTable Properties : VpcId : !Ref myVPC Tags : - Key : Name Value : !Ref paramUniqueName Secondly, we need to add a new route to the public route table that points all traffic (0.0.0.0/0) to the Internet Gateway. # b) Associate the public route table with the Internet Gateway myPublicRoute : Type : AWS::EC2::Route DependsOn : myVPCGatewayAttachment Properties : RouteTableId : !Ref myPublicRouteTable DestinationCidrBlock : 0.0.0.0/0 GatewayId : !Ref myInternetGateway DestinationCidrBlock - it specifies which traffic we want this route to be applied to. In this case, we apply it to all traffic using the 0.0.0.0/0 CIDR block GatewayId - it specifies where traffic matching the CIDR block should be directed Note, when you add a DependsOn attribute to a resource, that resource is created only after the creation of the resource specified in the DependsOn attribute. Thirdly, once we are done with the public route table, time to create two public subnets with a size /24 IPv4 CIDR block in each of two AZs. This provides up to 256 addresses per subnet, a few of which are reserved for AWS use. It\u2019s better to define CIDR blocks for both subnets as input parameters so a user can either customize the IP ranges or use a default range. Parameters : # etc paramPublicSubnet1CIDR : Description : Enter the IP range (CIDR notation) for the public subnet in AZ A Type : String Default : 10.192.10.0/24 paramPublicSubnet2CIDR : Description : Enter the IP range (CIDR notation) for the public subnet in AZ B Type : String Default : 10.192.11.0/24 Important! The CIDR blocks of two subnets must not overlap with the CIDR block that's associated with the VPC. Subnets must exist within a VPC, so this is how we associate these two subnets within our VPC: # c) Create a public subnet in AZ 1 (will be public once it is associated with public route table) myPublicSubnet1 : Type : AWS::EC2::Subnet Properties : VpcId : !Ref myVPC AvailabilityZone : !Select [ 0 , !GetAZs '' ] # AZ 1 CidrBlock : !Ref paramPublicSubnet1CIDR MapPublicIpOnLaunch : true Tags : - Key : Name Value : !Ref paramUniqueName # Create a public subnet in AZ 2 (will be public once it is associated with public route table) myPublicSubnet2 : Type : AWS::EC2::Subnet Properties : VpcId : !Ref myVPC AvailabilityZone : !Select [ 1 , !GetAZs '' ] # AZ 2 CidrBlock : !Ref paramPublicSubnet2CIDR MapPublicIpOnLaunch : true Tags : - Key : Name Value : !Ref paramUniqueName By setting MapPublicIpOnLaunch to true instances launched into the subnet will be allocated a public IP address by default. This means that any instances in this subnet will be reachable from the Internet via the Internet Gateway attached to the VPC. And finally, it\u2019s important to associate the route table with both public subnets: # d) Associate the public route table with the public subnet in AZ 1 myPublicSubnet1RouteTableAssociation : Type : AWS::EC2::SubnetRouteTableAssociation Properties : RouteTableId : !Ref myPublicRouteTable SubnetId : !Ref myPublicSubnet1 # Associate the public route table with the public subnet in AZ 2 myPublicSubnet2RouteTableAssociation : Type : AWS::EC2::SubnetRouteTableAssociation Properties : RouteTableId : !Ref myPublicRouteTable SubnetId : !Ref myPublicSubnet2 At that point, we are done with public subnets, now let\u2019s create private subnets","title":"Step 2. Create a public route table and public subnets across two AZs"},{"location":"aws/cloud_formation/vpc/vpc_public_private_subnets/#step-3-create-nat-gateways-private-route-tables-and-private-subnets-across-two-azs","text":"We don't want instances within our private subnets to be reachable from the public internet. But we do want these instances to be able to initiate outbound connections. Also we want them to be able to do this without having public IP addresses. That is when NAT gateway comes in handy. NAT gateway enables instances in a private subnet to connect to the internet or other AWS services, but prevent the internet from initiating a connection with those instances. It will have a public IP address and will be associated with a public subnet. Private instances in private subnets will be able to use this to initiate outbound connections. But the NAT will not allow the reverse, a party on the public internet cannot use the NAT to connect to our private instances. Thus, we need an Elastic IP (EIP) address and a NAT gateway. Not even one, but two! Because a single NAT Gateway in a single AZ has redundancy within that AZ only, so if there are any zonal issues then instances in other AZs would have no route to the internet. To remain highly available, we need a NAT Gateway in each AZ and a different route table for each AZ. # a) Specify an Elastic IP (EIP) address for a NAT Gateway in AZ 1 myEIPforNatGateway1 : Type : AWS::EC2::EIP DependsOn : myVPCGatewayAttachment Properties : Domain : vpc # Specify an Elastic IP (EIP) address for a NAT Gateway in AZ 2 myEIPforNatGateway2 : Type : AWS::EC2::EIP DependsOn : myVPCGatewayAttachment Properties : Domain : vpc # b) Create a NAT Gateway in the public subnet for AZ 1 myNatGateway1 : Type : AWS::EC2::NatGateway Properties : AllocationId : !GetAtt myEIPforNatGateway1.AllocationId SubnetId : !Ref myPublicSubnet1 # Create a NAT Gateway in the public subnet for AZ 2 myNatGateway2 : Type : AWS::EC2::NatGateway Properties : AllocationId : !GetAtt myEIPforNatGateway2.AllocationId SubnetId : !Ref myPublicSubnet2 # c) Create a private route table for AZ 1 myPrivateRouteTable1 : Type : AWS::EC2::RouteTable Properties : VpcId : !Ref myVPC Tags : - Key : Name Value : !Ref paramUniqueName # Create a private route table for AZ 2 myPrivateRouteTable2 : Type : AWS::EC2::RouteTable Properties : VpcId : !Ref myVPC Tags : - Key : Name Value : !Ref paramUniqueName # d) Associate the private route table with the Nat Gateway in AZ 1 myPrivateRouteForAz1 : Type : AWS::EC2::Route DependsOn : myVPCGatewayAttachment Properties : RouteTableId : !Ref myPrivateRouteTable1 DestinationCidrBlock : 0.0.0.0/0 NatGatewayId : !Ref myNatGateway1 # Associate the private route table with the Nat Gateway in AZ 2 myPrivateRouteForAz2 : Type : AWS::EC2::Route DependsOn : myVPCGatewayAttachment Properties : RouteTableId : !Ref myPrivateRouteTable2 DestinationCidrBlock : 0.0.0.0/0 NatGatewayId : !Ref myNatGateway2 In the sample above, we created and specified an Elastic IP address to associate with the NAT gateway for each AZ. After creating a NAT gateway, we created and updated the route table associated with each private subnet to point internet-bound traffic to the NAT gateway. This enables instances in our private subnets to communicate with the internet. And our final step is to create two private subnets with a size /24 IPv4 CIDR block in each of two AZs. And then associate the private route table with the private subnet for each AZ. Parameters : # etc paramPrivateSubnet1CIDR : Description : Enter the IP range (CIDR notation) for the private subnet in AZ A Type : String Default : 10.192.20.0/24 paramPrivateSubnet2CIDR : Description : Enter the IP range (CIDR notation) for the private subnet in AZ B Type : String Default : 10.192.21.0/24 # e) Create a private subnet in AZ 1 myPrivateSubnet1 : Type : AWS::EC2::Subnet Properties : VpcId : !Ref myVPC AvailabilityZone : !Select [ 0 , !GetAZs '' ] # AZ 1 CidrBlock : !Ref paramPrivateSubnet1CIDR MapPublicIpOnLaunch : true Tags : - Key : Name Value : !Ref paramUniqueName # Create a private subnet in AZ 2 myPrivateSubnet2 : Type : AWS::EC2::Subnet Properties : VpcId : !Ref myVPC AvailabilityZone : !Select [ 1 , !GetAZs '' ] # AZ 2 CidrBlock : !Ref paramPrivateSubnet2CIDR MapPublicIpOnLaunch : true Tags : - Key : Name Value : !Ref paramUniqueName # f) Associate the private route table with the private subnet in AZ 1 myPrivateSubnet1RouteTableAssociation : Type : AWS::EC2::SubnetRouteTableAssociation Properties : RouteTableId : !Ref myPrivateRouteTable1 SubnetId : !Ref myPrivateSubnet1 # Associate the private route table with the private subnet in AZ 2 myPrivateSubnet2RouteTableAssociation : Type : AWS::EC2::SubnetRouteTableAssociation Properties : RouteTableId : !Ref myPrivateRouteTable2 SubnetId : !Ref myPrivateSubnet2 Optionally, we can add an Output section to get the list of newly created private and public subnets Outputs : outputVPC : Description : A reference to the created VPC Value : !Ref myVPC outputPublicSubnets : Description : A list of the public subnets Value : !Join [ \",\" , [ !Ref myPublicSubnet1 , !Ref myPublicSubnet2 ]] outputPrivateSubnets : Description : A list of the private subnets Value : !Join [ \",\" , [ !Ref myPrivateSubnet1 , !Ref myPrivateSubnet2 ]]","title":"Step 3. Create NAT Gateways, private route tables and private subnets across two AZs"},{"location":"aws/cloud_formation/vpc/vpc_public_private_subnets/#cf-template","text":"## =================== VERSION =================== # AWSTemplateFormatVersion : 2010-09-09 ## =================== DESCRIPTION =================== # Description : >- AWS CloudFormation sample template. Create a custom VPC with a pair of public and private subnets spread across two AZs ## =================== PARAMETERS =================== # Parameters : paramVpcCIDR : Description : Enter the IP range (CIDR notation) for VPC Type : String Default : 10.192.0.0/16 paramPublicSubnet1CIDR : Description : Enter the IP range (CIDR notation) for the public subnet in AZ A Type : String Default : 10.192.10.0/24 paramPublicSubnet2CIDR : Description : Enter the IP range (CIDR notation) for the public subnet in AZ B Type : String Default : 10.192.11.0/24 paramPrivateSubnet1CIDR : Description : Enter the IP range (CIDR notation) for the private subnet in AZ A Type : String Default : 10.192.20.0/24 paramPrivateSubnet2CIDR : Description : Enter the IP range (CIDR notation) for the private subnet in AZ B Type : String Default : 10.192.21.0/24 paramUniqueName : Description : Give a unique name for \"CloudFormationLab\" tag value Type : String Default : DaenerysTargaryen ## =================== RESOURCES =================== # Resources : # ---------------- Step 1 ---------------- # a) Create a VPC myVPC : Type : AWS::EC2::VPC Properties : CidrBlock : !Ref paramVpcCIDR EnableDnsSupport : true # let instances in the VPC get DNS hostnames EnableDnsHostnames : true # allow DNS resolution Tags : - Key : MasteringCF Value : !Ref paramUniqueName # b) Create a Internet Gateway myInternetGateway : Type : AWS::EC2::InternetGateway Properties : Tags : - Key : MasteringCF Value : !Ref paramUniqueName # c) Attach the Internet Gateway to the VPC myVPCGatewayAttachment : Type : AWS::EC2::VPCGatewayAttachment Properties : VpcId : !Ref myVPC InternetGatewayId : !Ref myInternetGateway # ---------------- Step 2 ---------------- # a) Create a public route table for the VPC (will be public once it is associated with the Internet Gateway) myPublicRouteTable : Type : AWS::EC2::RouteTable Properties : VpcId : !Ref myVPC Tags : - Key : MasteringCF Value : !Ref paramUniqueName # b) Associate the public route table with the Internet Gateway myPublicRoute : Type : AWS::EC2::Route DependsOn : myVPCGatewayAttachment Properties : RouteTableId : !Ref myPublicRouteTable DestinationCidrBlock : 0.0.0.0/0 GatewayId : !Ref myInternetGateway # c) Create a public subnet in AZ 1 (will be public once it is associated with public route table) myPublicSubnet1 : Type : AWS::EC2::Subnet Properties : VpcId : !Ref myVPC AvailabilityZone : !Select [ 0 , !GetAZs '' ] # AZ 1 CidrBlock : !Ref paramPublicSubnet1CIDR MapPublicIpOnLaunch : true # allow instances launched in this subnet receive a public IPv4 address Tags : - Key : MasteringCF Value : !Ref paramUniqueName # Create a public subnet in AZ 2 (will be public once it is associated with public route table) myPublicSubnet2 : Type : AWS::EC2::Subnet Properties : VpcId : !Ref myVPC AvailabilityZone : !Select [ 1 , !GetAZs '' ] # AZ 2 CidrBlock : !Ref paramPublicSubnet2CIDR MapPublicIpOnLaunch : true # allow instances launched in this subnet receive a public IPv4 address Tags : - Key : MasteringCF Value : !Ref paramUniqueName # d) Associate the public route table with the public subnet in AZ 1 myPublicSubnet1RouteTableAssociation : Type : AWS::EC2::SubnetRouteTableAssociation Properties : RouteTableId : !Ref myPublicRouteTable SubnetId : !Ref myPublicSubnet1 # Associate the public route table with the public subnet in AZ 2 myPublicSubnet2RouteTableAssociation : Type : AWS::EC2::SubnetRouteTableAssociation Properties : RouteTableId : !Ref myPublicRouteTable SubnetId : !Ref myPublicSubnet2 # ---------------- Step 3 ---------------- # a) Specify an Elastic IP (EIP) address for a NAT Gateway in AZ 1 myEIPforNatGateway1 : Type : AWS::EC2::EIP DependsOn : myVPCGatewayAttachment Properties : Domain : vpc # if the region supports EC2-Classic, the default is \"standard\", otherwise - \"vpc\" Tags : - Key : MasteringCF Value : !Ref paramUniqueName # Specify an Elastic IP (EIP) address for a NAT Gateway in AZ 2 myEIPforNatGateway2 : Type : AWS::EC2::EIP DependsOn : myVPCGatewayAttachment Properties : Domain : vpc # if the region supports EC2-Classic, the default is \"standard\", otherwise - \"vpc\" Tags : - Key : MasteringCF Value : !Ref paramUniqueName # b) Create a NAT Gateway in the public subnet for AZ 1 myNatGateway1 : Type : AWS::EC2::NatGateway Properties : AllocationId : !GetAtt myEIPforNatGateway1.AllocationId SubnetId : !Ref myPublicSubnet1 Tags : - Key : MasteringCF Value : !Ref paramUniqueName # Create a NAT Gateway in the public subnet for AZ 2 myNatGateway2 : Type : AWS::EC2::NatGateway Properties : AllocationId : !GetAtt myEIPforNatGateway2.AllocationId SubnetId : !Ref myPublicSubnet2 Tags : - Key : MasteringCF Value : !Ref paramUniqueName # c) Create a private route table for AZ 1 myPrivateRouteTable1 : Type : AWS::EC2::RouteTable Properties : VpcId : !Ref myVPC Tags : - Key : MasteringCF Value : !Ref paramUniqueName # Create a private route table for AZ 2 myPrivateRouteTable2 : Type : AWS::EC2::RouteTable Properties : VpcId : !Ref myVPC Tags : - Key : MasteringCF Value : !Ref paramUniqueName # d) Associate the private route table with the Nat Gateway in AZ 1 myPrivateRouteForAz1 : Type : AWS::EC2::Route DependsOn : myVPCGatewayAttachment Properties : RouteTableId : !Ref myPrivateRouteTable1 DestinationCidrBlock : 0.0.0.0/0 NatGatewayId : !Ref myNatGateway1 # Associate the private route table with the Nat Gateway in AZ 2 myPrivateRouteForAz2 : Type : AWS::EC2::Route DependsOn : myVPCGatewayAttachment Properties : RouteTableId : !Ref myPrivateRouteTable2 DestinationCidrBlock : 0.0.0.0/0 NatGatewayId : !Ref myNatGateway2 # e) Create a private subnet in AZ 1 myPrivateSubnet1 : Type : AWS::EC2::Subnet Properties : VpcId : !Ref myVPC AvailabilityZone : !Select [ 0 , !GetAZs '' ] # AZ 1 CidrBlock : !Ref paramPrivateSubnet1CIDR MapPublicIpOnLaunch : false # private subnet doesn't need public IP Tags : - Key : MasteringCF Value : !Ref paramUniqueName # Create a private subnet in AZ 2 myPrivateSubnet2 : Type : AWS::EC2::Subnet Properties : VpcId : !Ref myVPC AvailabilityZone : !Select [ 1 , !GetAZs '' ] # AZ 2 CidrBlock : !Ref paramPrivateSubnet2CIDR MapPublicIpOnLaunch : false # private subnet doesn't need public IP Tags : - Key : MasteringCF Value : !Ref paramUniqueName # f) Associate the private route table with the private subnet in AZ 1 myPrivateSubnet1RouteTableAssociation : Type : AWS::EC2::SubnetRouteTableAssociation Properties : RouteTableId : !Ref myPrivateRouteTable1 SubnetId : !Ref myPrivateSubnet1 # Associate the private route table with the private subnet in AZ 2 myPrivateSubnet2RouteTableAssociation : Type : AWS::EC2::SubnetRouteTableAssociation Properties : RouteTableId : !Ref myPrivateRouteTable2 SubnetId : !Ref myPrivateSubnet2 ## =================== OUTPUTS =================== # Outputs : outputVPC : Description : A reference to the created VPC Value : !Ref myVPC outputPublicSubnets : Description : A list of the public subnets Value : !Join [ \",\" , [ !Ref myPublicSubnet1 , !Ref myPublicSubnet2 ]] outputPrivateSubnets : Description : A list of the private subnets Value : !Join [ \",\" , [ !Ref myPrivateSubnet1 , !Ref myPrivateSubnet2 ]]","title":"CF Template"},{"location":"aws/cloud_formation/vpc/vpc_public_subnets/","text":"VPC with only public subnets \u00b6 Provision a VPC with two public subnets using CloudFormation. This CF template can be modified to fit general use cases or used as is. AWS CloudFormation is the most reliable method to create and manage a Virtual Private Cloud (VPC), complete with subnets, NATting, route tables, etc. The emphasis is on use of CF and Infrastructure as Code to build and manage resources in AWS, less about the issues of VPC design. A private subnet does not route traffic to an internet gateway. Resources in a private subnet do not have public IP addresses, and can only communicate with resources in other subnets in the same VPC. One or more subnets can be placed in a VPC. It is possible to mix and match public and private subnets in a VPC, allowing some resources in the VPC to access the internet, and some to only access other resources in the VPC. A VPC with public subnets allows instances to access the internet. If those instances have public IP addresses, they can also be access from the internet. Create VPC with public subnets \u00b6 CloudFormation Template: VPC with public subnets Parameters : Tag : Type : String Resources : VPC : Type : \"AWS::EC2::VPC\" Properties : CidrBlock : \"10.0.0.0/16\" InstanceTenancy : \"default\" Tags : - Key : \"Name\" Value : !Ref \"Tag\" SubnetA : Type : \"AWS::EC2::Subnet\" Properties : AvailabilityZone : !Select - 0 - !GetAZs Ref : 'AWS::Region' VpcId : !Ref \"VPC\" CidrBlock : \"10.0.0.0/24\" SubnetB : Type : \"AWS::EC2::Subnet\" Properties : AvailabilityZone : !Select - 1 - !GetAZs Ref : 'AWS::Region' VpcId : !Ref \"VPC\" CidrBlock : \"10.0.1.0/24\" RouteTable : Type : \"AWS::EC2::RouteTable\" Properties : VpcId : !Ref \"VPC\" InternetGateway : Type : \"AWS::EC2::InternetGateway\" VPCGatewayAttachment : Type : \"AWS::EC2::VPCGatewayAttachment\" Properties : VpcId : !Ref \"VPC\" InternetGatewayId : !Ref \"InternetGateway\" InternetRoute : Type : \"AWS::EC2::Route\" Properties : DestinationCidrBlock : \"0.0.0.0/0\" GatewayId : !Ref InternetGateway RouteTableId : !Ref RouteTable SubnetARouteTableAssociation : Type : \"AWS::EC2::SubnetRouteTableAssociation\" Properties : RouteTableId : !Ref RouteTable SubnetId : !Ref SubnetA SubnetBRouteTableAssociation : Type : \"AWS::EC2::SubnetRouteTableAssociation\" Properties : RouteTableId : !Ref RouteTable SubnetId : !Ref SubnetB Outputs : VpcId : Description : The VPC ID Value : !Ref VPC The template above builds on the template presented in the previous post, Create a private AWS VPC with CloudFormation, adding an internet gateway and the route tables required to direct traffic to the internet. Refer to the previous post to read the details of the VPC, Subnet, and Route Table resources. To connect the VPC to the internet, you must attach an internet gateway, represented by the AWSEC2InternetGateway resource. This resource does not support any configuration properties beyond adding custom tags: InternetGateway : Type : \"AWS::EC2::InternetGateway\" The internet gateway is attached to a VPC via the AWSEC2VPCGatewayAttachment resource: VPCGatewayAttachment : Type : \"AWS::EC2::VPCGatewayAttachment\" Properties : VpcId : !Ref \"VPC\" InternetGatewayId : !Ref \"InternetGateway\" To direct external traffic through the internet gateway, you must create a route, represented by the AWSEC2Route resource. The route below defines a DestinationCidrBlock of 0.0.0.0/0 , which matches all traffic. This route will be applied after the default route which connects instances in subnets in the same VPC, so only traffic not destined for another instance in the VPC will be affected by this route. In practice this means any external traffic is directed through the internet gateway: InternetRoute : Type : \"AWS::EC2::Route\" Properties : DestinationCidrBlock : \"0.0.0.0/0\" GatewayId : !Ref InternetGateway RouteTableId : !Ref RouteTable The route is then associated with both subnets via the AWSEC2SubnetRouteTableAssociation resource, which makes them public subnets: SubnetARouteTableAssociation : Type : \"AWS::EC2::SubnetRouteTableAssociation\" Properties : RouteTableId : !Ref RouteTable SubnetId : !Ref SubnetA SubnetBRouteTableAssociation : Type : \"AWS::EC2::SubnetRouteTableAssociation\" Properties : RouteTableId : !Ref RouteTable SubnetId : !Ref SubnetB To deploy this template, use the Deploy an AWS CloudFormation template step. Note how EC2 instances placed in this VPC have the option to receive public IP addresses: The IP address assigned to the EC2 instance allows you to SSH into it from your local PC:","title":"VPC public subnets"},{"location":"aws/cloud_formation/vpc/vpc_public_subnets/#vpc-with-only-public-subnets","text":"Provision a VPC with two public subnets using CloudFormation. This CF template can be modified to fit general use cases or used as is. AWS CloudFormation is the most reliable method to create and manage a Virtual Private Cloud (VPC), complete with subnets, NATting, route tables, etc. The emphasis is on use of CF and Infrastructure as Code to build and manage resources in AWS, less about the issues of VPC design. A private subnet does not route traffic to an internet gateway. Resources in a private subnet do not have public IP addresses, and can only communicate with resources in other subnets in the same VPC. One or more subnets can be placed in a VPC. It is possible to mix and match public and private subnets in a VPC, allowing some resources in the VPC to access the internet, and some to only access other resources in the VPC. A VPC with public subnets allows instances to access the internet. If those instances have public IP addresses, they can also be access from the internet.","title":"VPC with only public subnets"},{"location":"aws/cloud_formation/vpc/vpc_public_subnets/#create-vpc-with-public-subnets","text":"CloudFormation Template: VPC with public subnets Parameters : Tag : Type : String Resources : VPC : Type : \"AWS::EC2::VPC\" Properties : CidrBlock : \"10.0.0.0/16\" InstanceTenancy : \"default\" Tags : - Key : \"Name\" Value : !Ref \"Tag\" SubnetA : Type : \"AWS::EC2::Subnet\" Properties : AvailabilityZone : !Select - 0 - !GetAZs Ref : 'AWS::Region' VpcId : !Ref \"VPC\" CidrBlock : \"10.0.0.0/24\" SubnetB : Type : \"AWS::EC2::Subnet\" Properties : AvailabilityZone : !Select - 1 - !GetAZs Ref : 'AWS::Region' VpcId : !Ref \"VPC\" CidrBlock : \"10.0.1.0/24\" RouteTable : Type : \"AWS::EC2::RouteTable\" Properties : VpcId : !Ref \"VPC\" InternetGateway : Type : \"AWS::EC2::InternetGateway\" VPCGatewayAttachment : Type : \"AWS::EC2::VPCGatewayAttachment\" Properties : VpcId : !Ref \"VPC\" InternetGatewayId : !Ref \"InternetGateway\" InternetRoute : Type : \"AWS::EC2::Route\" Properties : DestinationCidrBlock : \"0.0.0.0/0\" GatewayId : !Ref InternetGateway RouteTableId : !Ref RouteTable SubnetARouteTableAssociation : Type : \"AWS::EC2::SubnetRouteTableAssociation\" Properties : RouteTableId : !Ref RouteTable SubnetId : !Ref SubnetA SubnetBRouteTableAssociation : Type : \"AWS::EC2::SubnetRouteTableAssociation\" Properties : RouteTableId : !Ref RouteTable SubnetId : !Ref SubnetB Outputs : VpcId : Description : The VPC ID Value : !Ref VPC The template above builds on the template presented in the previous post, Create a private AWS VPC with CloudFormation, adding an internet gateway and the route tables required to direct traffic to the internet. Refer to the previous post to read the details of the VPC, Subnet, and Route Table resources. To connect the VPC to the internet, you must attach an internet gateway, represented by the AWSEC2InternetGateway resource. This resource does not support any configuration properties beyond adding custom tags: InternetGateway : Type : \"AWS::EC2::InternetGateway\" The internet gateway is attached to a VPC via the AWSEC2VPCGatewayAttachment resource: VPCGatewayAttachment : Type : \"AWS::EC2::VPCGatewayAttachment\" Properties : VpcId : !Ref \"VPC\" InternetGatewayId : !Ref \"InternetGateway\" To direct external traffic through the internet gateway, you must create a route, represented by the AWSEC2Route resource. The route below defines a DestinationCidrBlock of 0.0.0.0/0 , which matches all traffic. This route will be applied after the default route which connects instances in subnets in the same VPC, so only traffic not destined for another instance in the VPC will be affected by this route. In practice this means any external traffic is directed through the internet gateway: InternetRoute : Type : \"AWS::EC2::Route\" Properties : DestinationCidrBlock : \"0.0.0.0/0\" GatewayId : !Ref InternetGateway RouteTableId : !Ref RouteTable The route is then associated with both subnets via the AWSEC2SubnetRouteTableAssociation resource, which makes them public subnets: SubnetARouteTableAssociation : Type : \"AWS::EC2::SubnetRouteTableAssociation\" Properties : RouteTableId : !Ref RouteTable SubnetId : !Ref SubnetA SubnetBRouteTableAssociation : Type : \"AWS::EC2::SubnetRouteTableAssociation\" Properties : RouteTableId : !Ref RouteTable SubnetId : !Ref SubnetB To deploy this template, use the Deploy an AWS CloudFormation template step. Note how EC2 instances placed in this VPC have the option to receive public IP addresses: The IP address assigned to the EC2 instance allows you to SSH into it from your local PC:","title":"Create VPC with public subnets"},{"location":"aws/cloud_formation/vpc/vpc_test_network/","text":"Setup VPC for testing network connectivity \u00b6 ## =================== VERSION =================== # AWSTemplateFormatVersion : 2010-09-09 ## =================== DESCRIPTION =================== # Description : >- AWS CloudFormation sample template. Create a custom VPC with a pair of public and private subnets spread across two AZs deploys a basic VPC / Network. ## =================== PARAMETERS =================== # # This CloudFormation template deploys a basic VPC / Network. Parameters : paramVpcCIDR : Description : Enter the IP range (CIDR notation) for VPC Type : String Default : 10.0.0.0/16 Resources : # a) Create a VPC myVPC : Type : AWS::EC2::VPC Properties : CidrBlock : !Ref paramVpcCIDR EnableDnsSupport : true EnableDnsHostnames : true Tags : - Key : CloudFormation #Value: !Ref paramUniqueName Value : !Join [ '' , [ !Ref \"AWS::StackName\" , \"-VPC\" ]]","title":"VPC Test Network"},{"location":"aws/cloud_formation/vpc/vpc_test_network/#setup-vpc-for-testing-network-connectivity","text":"## =================== VERSION =================== # AWSTemplateFormatVersion : 2010-09-09 ## =================== DESCRIPTION =================== # Description : >- AWS CloudFormation sample template. Create a custom VPC with a pair of public and private subnets spread across two AZs deploys a basic VPC / Network. ## =================== PARAMETERS =================== # # This CloudFormation template deploys a basic VPC / Network. Parameters : paramVpcCIDR : Description : Enter the IP range (CIDR notation) for VPC Type : String Default : 10.0.0.0/16 Resources : # a) Create a VPC myVPC : Type : AWS::EC2::VPC Properties : CidrBlock : !Ref paramVpcCIDR EnableDnsSupport : true EnableDnsHostnames : true Tags : - Key : CloudFormation #Value: !Ref paramUniqueName Value : !Join [ '' , [ !Ref \"AWS::StackName\" , \"-VPC\" ]]","title":"Setup VPC for testing network connectivity"},{"location":"aws/cloud_formation/vpc_priv_elb/","text":"Attach instances in private subnet to ELB \u00b6 https://awstut.com/en/2021/12/11/attaching-instances-in-private-subnet-to-elb/ Two apache webservers, each running on EC2 instances.They are not exposed to the web and are running behind the public ELB. Display hostname of webservers, showing the instance id. It is possible to also show the private range IP of instances. The ELB health checks take 5 minutes to finish testing. Architecture \u00b6 Requirements \u00b6 AWS CLI S3 Bucket(Here, the bucket name is MY-BUCKET and region is us-east-1 ) Usage \u00b6 Create New Bucket \u00b6 Check if bucket name exists and then create bucket. curl -sI https://<>.s3.amazonaws.com | grep bucket-region curl -sI https://cfn-templates-409072330537.s3.amazonaws.com | grep us-east-1 # empty response indicates no bucket exists with that name aws s3 mb s3://cfn-templates-409072330537 Template File Modification \u00b6 Modify the following locations in fa-001.yaml. Parameters : TemplateBucketName : Type : String Default : MY-BUCKET Upload Template Files to S3 Bucket \u00b6 aws s3 cp . s3://MY-BUCKET/fa-001/ --recursive aws s3 cp . s3://cfn-templates-409072330537/fa-001/ --recursive CloudFormation Stack Creation \u00b6 https://cfn-templates-409072330537.s3.us-east-1.amazonaws.com/fa-001/fa-001.yaml aws cloudformation create-stack \\ --stack-name fa-001 \\ --template-url https://cfn-templates-409072330537.s3.us-east-1.amazonaws.com/fa-001/fa-001.yaml \\ --capabilities CAPABILITY_IAM ## Expected output shows cloudformation arn { \"StackId\" : \"arn:aws:cloudformation:us-east-1:409072330537:stack/fa-001/4a0ab580-95d2-11ed-9ad4-0e66ffc61147\" } Load Balancer \u00b6 Our LB is configured with two public subnets. Load Balancer There are two registered targets, mapped to the EC2 instances in private subnets. Registered targets for the Load Balancer Operation Check \u00b6 curl the LB DNS name and the EC2 instance id will be displayed. The EC2 id will randomly alternate, as the LB routes traffic to each instance. $ curl http://fa-001-ALB-2051751187.us-east-1.elb.amazonaws.com instance-id: i-091ea91d28c61997c $ curl http://fa-001-ALB-2051751187.us-east-1.elb.amazonaws.com instance-id: i-075569ded2089466f $ curl http://fa-001-ALB-2051751187.us-east-1.elb.amazonaws.com instance-id: i-091ea91d28c61997c $ curl http://fa-001-ALB-2051751187.us-east-1.elb.amazonaws.com instance-id: i-075569ded2089466f From the output results, we can see that the two instances placed in the private subnet are accessed alternately.","title":"Attach instances in private subnet to ELB"},{"location":"aws/cloud_formation/vpc_priv_elb/#attach-instances-in-private-subnet-to-elb","text":"https://awstut.com/en/2021/12/11/attaching-instances-in-private-subnet-to-elb/ Two apache webservers, each running on EC2 instances.They are not exposed to the web and are running behind the public ELB. Display hostname of webservers, showing the instance id. It is possible to also show the private range IP of instances. The ELB health checks take 5 minutes to finish testing.","title":"Attach instances in private subnet to ELB"},{"location":"aws/cloud_formation/vpc_priv_elb/#architecture","text":"","title":"Architecture"},{"location":"aws/cloud_formation/vpc_priv_elb/#requirements","text":"AWS CLI S3 Bucket(Here, the bucket name is MY-BUCKET and region is us-east-1 )","title":"Requirements"},{"location":"aws/cloud_formation/vpc_priv_elb/#usage","text":"","title":"Usage"},{"location":"aws/cloud_formation/vpc_priv_elb/#create-new-bucket","text":"Check if bucket name exists and then create bucket. curl -sI https://<>.s3.amazonaws.com | grep bucket-region curl -sI https://cfn-templates-409072330537.s3.amazonaws.com | grep us-east-1 # empty response indicates no bucket exists with that name aws s3 mb s3://cfn-templates-409072330537","title":"Create New Bucket"},{"location":"aws/cloud_formation/vpc_priv_elb/#template-file-modification","text":"Modify the following locations in fa-001.yaml. Parameters : TemplateBucketName : Type : String Default : MY-BUCKET","title":"Template File Modification"},{"location":"aws/cloud_formation/vpc_priv_elb/#upload-template-files-to-s3-bucket","text":"aws s3 cp . s3://MY-BUCKET/fa-001/ --recursive aws s3 cp . s3://cfn-templates-409072330537/fa-001/ --recursive","title":"Upload Template Files to S3 Bucket"},{"location":"aws/cloud_formation/vpc_priv_elb/#cloudformation-stack-creation","text":"https://cfn-templates-409072330537.s3.us-east-1.amazonaws.com/fa-001/fa-001.yaml aws cloudformation create-stack \\ --stack-name fa-001 \\ --template-url https://cfn-templates-409072330537.s3.us-east-1.amazonaws.com/fa-001/fa-001.yaml \\ --capabilities CAPABILITY_IAM ## Expected output shows cloudformation arn { \"StackId\" : \"arn:aws:cloudformation:us-east-1:409072330537:stack/fa-001/4a0ab580-95d2-11ed-9ad4-0e66ffc61147\" }","title":"CloudFormation Stack Creation"},{"location":"aws/cloud_formation/vpc_priv_elb/#load-balancer","text":"Our LB is configured with two public subnets. Load Balancer There are two registered targets, mapped to the EC2 instances in private subnets. Registered targets for the Load Balancer","title":"Load Balancer"},{"location":"aws/cloud_formation/vpc_priv_elb/#operation-check","text":"curl the LB DNS name and the EC2 instance id will be displayed. The EC2 id will randomly alternate, as the LB routes traffic to each instance. $ curl http://fa-001-ALB-2051751187.us-east-1.elb.amazonaws.com instance-id: i-091ea91d28c61997c $ curl http://fa-001-ALB-2051751187.us-east-1.elb.amazonaws.com instance-id: i-075569ded2089466f $ curl http://fa-001-ALB-2051751187.us-east-1.elb.amazonaws.com instance-id: i-091ea91d28c61997c $ curl http://fa-001-ALB-2051751187.us-east-1.elb.amazonaws.com instance-id: i-075569ded2089466f From the output results, we can see that the two instances placed in the private subnet are accessed alternately.","title":"Operation Check"},{"location":"aws/cloud_formation/vpc_priv_elb/ssl_fqdn/","text":"Add SSL to Domain Name \u00b6 Request Certificate \u00b6 AWS ACM provides free certificates with DNS validation. The certificate takes a few minutes to be issued, however the CNAME is available while pending. Request a public certificate Request Certificate Add domain name to certificate. Registered wildcard domain and domain. Load Balancer \u00b6 Create Target Groups Create ALB Listener Listener \u00b6 There are two LB listeners. Target group is the same for both. Registered targets for the Load Balancer Namecheap \u00b6 Mapping names from AWS to domain registar (Namecheap). CNAME 1 ALB DNS Name Host @ Value: fa-001-ALB-2051751187.us-east-1.elb.amazonaws.com. CNAME 2 Certificate Host CNAME name _63bc95146eae05252630a94b488c023e.npcompleted.cloud. Value: CNAME value _2321460215360cda63bd61cfbecde626.kqlycvwlbp.acm-validations.aws. Confirm results \u00b6 Open any browser and test HTTPS and HTTP urls. HTTP using DNS name: fa-001-ALB-2051751187.us-east-1.elb.amazonaws.com Instance 1 Instance 2 HTTPS using fully qualified domain name Instance 1 Instance 2 End \u00b6","title":"Add SSL to Domain Name"},{"location":"aws/cloud_formation/vpc_priv_elb/ssl_fqdn/#add-ssl-to-domain-name","text":"","title":"Add SSL to Domain Name"},{"location":"aws/cloud_formation/vpc_priv_elb/ssl_fqdn/#request-certificate","text":"AWS ACM provides free certificates with DNS validation. The certificate takes a few minutes to be issued, however the CNAME is available while pending. Request a public certificate Request Certificate Add domain name to certificate. Registered wildcard domain and domain.","title":"Request Certificate"},{"location":"aws/cloud_formation/vpc_priv_elb/ssl_fqdn/#load-balancer","text":"Create Target Groups Create ALB Listener","title":"Load Balancer"},{"location":"aws/cloud_formation/vpc_priv_elb/ssl_fqdn/#listener","text":"There are two LB listeners. Target group is the same for both. Registered targets for the Load Balancer","title":"Listener"},{"location":"aws/cloud_formation/vpc_priv_elb/ssl_fqdn/#namecheap","text":"Mapping names from AWS to domain registar (Namecheap). CNAME 1 ALB DNS Name Host @ Value: fa-001-ALB-2051751187.us-east-1.elb.amazonaws.com. CNAME 2 Certificate Host CNAME name _63bc95146eae05252630a94b488c023e.npcompleted.cloud. Value: CNAME value _2321460215360cda63bd61cfbecde626.kqlycvwlbp.acm-validations.aws.","title":"Namecheap"},{"location":"aws/cloud_formation/vpc_priv_elb/ssl_fqdn/#confirm-results","text":"Open any browser and test HTTPS and HTTP urls. HTTP using DNS name: fa-001-ALB-2051751187.us-east-1.elb.amazonaws.com Instance 1 Instance 2 HTTPS using fully qualified domain name Instance 1 Instance 2","title":"Confirm results"},{"location":"aws/cloud_formation/vpc_priv_elb/ssl_fqdn/#end","text":"","title":"End"},{"location":"aws/cloud_formation/vpc_priv_elb/architecture_diagrams/architecture/","text":"Main Stack \u00b6 fa-001.yml is the primary stack . fa-001.yml VPC \u00b6 fa-001-vpc EC2 \u00b6 Two EC2 instances S3 Endpoint \u00b6 ALB \u00b6 fa-001-alb.yml is the elastic load balancer fa-001-alb.yml","title":"Main Stack"},{"location":"aws/cloud_formation/vpc_priv_elb/architecture_diagrams/architecture/#main-stack","text":"fa-001.yml is the primary stack . fa-001.yml","title":"Main Stack"},{"location":"aws/cloud_formation/vpc_priv_elb/architecture_diagrams/architecture/#vpc","text":"fa-001-vpc","title":"VPC"},{"location":"aws/cloud_formation/vpc_priv_elb/architecture_diagrams/architecture/#ec2","text":"Two EC2 instances","title":"EC2"},{"location":"aws/cloud_formation/vpc_priv_elb/architecture_diagrams/architecture/#s3-endpoint","text":"","title":"S3 Endpoint"},{"location":"aws/cloud_formation/vpc_priv_elb/architecture_diagrams/architecture/#alb","text":"fa-001-alb.yml is the elastic load balancer fa-001-alb.yml","title":"ALB"},{"location":"aws/compute/cloudfront/","text":"CloudFront \u00b6 CloudFront is a CDN that delivers data and applications globally with low latency. https://aws.amazon.com/cloudfront/ A CloudFront Origin is where content is stored, and where CloudFront gets content to serve to viewers. You can use: An S3 bucket configured with static website hosting An ELB load balancer An AWS Elemental MediaPackage endpoint or AWS Elemental MediaStore container Any HTTP server that runs on an Amazon EC2 instance (or other host). Reference: Amazon CloudFront > Origin . CloudFront content is cached in Edge Locations. A CloudFront distribution is a link between an origin server (such as an Amazon S3 bucket) and a registered domain name (such as Amazon Route 53 or a different registrar). Through this link, CloudFront identifies the object you have stored in your origin server. References: AWS glossary > distribution Routing traffic to an Amazon CloudFront web distribution by using your domain name","title":"CloudFront"},{"location":"aws/compute/cloudfront/#cloudfront","text":"CloudFront is a CDN that delivers data and applications globally with low latency. https://aws.amazon.com/cloudfront/ A CloudFront Origin is where content is stored, and where CloudFront gets content to serve to viewers. You can use: An S3 bucket configured with static website hosting An ELB load balancer An AWS Elemental MediaPackage endpoint or AWS Elemental MediaStore container Any HTTP server that runs on an Amazon EC2 instance (or other host). Reference: Amazon CloudFront > Origin . CloudFront content is cached in Edge Locations. A CloudFront distribution is a link between an origin server (such as an Amazon S3 bucket) and a registered domain name (such as Amazon Route 53 or a different registrar). Through this link, CloudFront identifies the object you have stored in your origin server. References: AWS glossary > distribution Routing traffic to an Amazon CloudFront web distribution by using your domain name","title":"CloudFront"},{"location":"aws/compute/compute_services/","text":"Exploring Compute Services \u00b6 Fargate Serverless compute engine for containers Manage containers, like Docker Scales automatically Serverless Lightsail Launch all resources for small projects Includes VM, SSD storage, DNS management and static IP Good for simple workloads, like Wordpress Outposts allows you to run cloud services in your internal data center Batch allows you process large workloads in smaller chunks (or batches) Runs hunderds and thousands of smaller batch processing jobs Dynamically provisions instances based on volumes","title":"Exploring Compute Services"},{"location":"aws/compute/compute_services/#exploring-compute-services","text":"Fargate Serverless compute engine for containers Manage containers, like Docker Scales automatically Serverless Lightsail Launch all resources for small projects Includes VM, SSD storage, DNS management and static IP Good for simple workloads, like Wordpress Outposts allows you to run cloud services in your internal data center Batch allows you process large workloads in smaller chunks (or batches) Runs hunderds and thousands of smaller batch processing jobs Dynamically provisions instances based on volumes","title":"Exploring Compute Services"},{"location":"aws/compute/ec2_overview/","text":"EC2 \u00b6 EC2 is an example of IaaS; IaaS includes the fundamental building blocks that can be rented from AWS. AWS manages the infrastructure and provides you a virtual machine that you can use however you'd like to meet your business requirements. EC2 Licensing Models \u00b6 On Demand Unpredictable workloads that cannot be interrupted Reserved Spot - Cheapest workloads can be interrupted Dedicated Host Savings Plan allows you to commit to compute usage (measured per hour) for 1 or 3 years. Amazon EC2 Reserved Instances (RI) provide a significant discount compared to On-Demand pricing in exchange for a commitment to 1-year or 3-year terms. Reserved Instances also allow capacity reservation, offering additional confidence in your ability to launch the number of instances you have reserved when you need them. Dedicated Instances \u00b6 Allow you to bring your own software licenses Dedicated Hosts \u00b6 Dedicated Hosts support special licensing requirements. An Amazon EC2 dedicated hosts is a physical server with an EC2 instance capacity that's fully dedicated for your use. Dedicated hosts allow you to use your existing per socket per call or per virtual machine software licenses and this can include things like Windows Server, Microsoft SQL Server, and SUSE Linux Enterprise Server. Spot \u00b6 Spot Instances allow you to request unused EC2 instances, which reduces your EC2 costs. Spot Instances are 90% cheaper than On-Demand. Scaling \u00b6 Horizontal Scaling \u00b6 Horizontal scaling is the act of changing the number of nodes in a computing system without changing the size of any individual node. So, with horizontal scaling, we would add instances. Vertical Scaling \u00b6 Vertical scaling is increasing the size and computing power of a single instance or node without increasing the number of nodes or instances. Resizing an EC2 instance type from a1.medium to a1.xlarge for more CPU and memory power Auto Scaling \u00b6 Add or remove EC2 instances using scaling policies. Auto scaling helps resources change to demand by adding or removing EC2 instances from your EC2 fleet based on conditions you specify. Auto Scaling allows you to automatically add or remove EC2 instances based on conditions you specify - these can include such things as at a specific time, or depending on how busy your application is. Auto Scaling cannot change the size of existing instances, nor can it add or change storage on an instance. https://aws.amazon.com/autoscaling/ Optimize Resources \u00b6 A retail company has EC2 On-Demand Instances running to serve customer transactions. There is a set pattern of traffic where demand is high at 2 points in the day, but the instances sit idle for much of the day. What is a good way to optimize these resources? Use an Auto Scaling group to scale out and in based on demand. The Auto Scaling group can be used to scale out and scale in the instances as the demand dictates. This will save money and avoid having instances sitting idle for long periods of time. AWS Auto Scaling monitors your applications and automatically adjusts your capacity to maintain steady, predictable performance at the lowest possible cost. Using AWS Auto Scaling, it\u2019s easy to set up application scaling for multiple resources across multiple services in minutes. https://aws.amazon.com/autoscaling/ End \u00b6","title":"EC2"},{"location":"aws/compute/ec2_overview/#ec2","text":"EC2 is an example of IaaS; IaaS includes the fundamental building blocks that can be rented from AWS. AWS manages the infrastructure and provides you a virtual machine that you can use however you'd like to meet your business requirements.","title":"EC2"},{"location":"aws/compute/ec2_overview/#ec2-licensing-models","text":"On Demand Unpredictable workloads that cannot be interrupted Reserved Spot - Cheapest workloads can be interrupted Dedicated Host Savings Plan allows you to commit to compute usage (measured per hour) for 1 or 3 years. Amazon EC2 Reserved Instances (RI) provide a significant discount compared to On-Demand pricing in exchange for a commitment to 1-year or 3-year terms. Reserved Instances also allow capacity reservation, offering additional confidence in your ability to launch the number of instances you have reserved when you need them.","title":"EC2  Licensing Models"},{"location":"aws/compute/ec2_overview/#dedicated-instances","text":"Allow you to bring your own software licenses","title":"Dedicated Instances"},{"location":"aws/compute/ec2_overview/#dedicated-hosts","text":"Dedicated Hosts support special licensing requirements. An Amazon EC2 dedicated hosts is a physical server with an EC2 instance capacity that's fully dedicated for your use. Dedicated hosts allow you to use your existing per socket per call or per virtual machine software licenses and this can include things like Windows Server, Microsoft SQL Server, and SUSE Linux Enterprise Server.","title":"Dedicated Hosts"},{"location":"aws/compute/ec2_overview/#spot","text":"Spot Instances allow you to request unused EC2 instances, which reduces your EC2 costs. Spot Instances are 90% cheaper than On-Demand.","title":"Spot"},{"location":"aws/compute/ec2_overview/#scaling","text":"","title":"Scaling"},{"location":"aws/compute/ec2_overview/#horizontal-scaling","text":"Horizontal scaling is the act of changing the number of nodes in a computing system without changing the size of any individual node. So, with horizontal scaling, we would add instances.","title":"Horizontal Scaling"},{"location":"aws/compute/ec2_overview/#vertical-scaling","text":"Vertical scaling is increasing the size and computing power of a single instance or node without increasing the number of nodes or instances. Resizing an EC2 instance type from a1.medium to a1.xlarge for more CPU and memory power","title":"Vertical Scaling"},{"location":"aws/compute/ec2_overview/#auto-scaling","text":"Add or remove EC2 instances using scaling policies. Auto scaling helps resources change to demand by adding or removing EC2 instances from your EC2 fleet based on conditions you specify. Auto Scaling allows you to automatically add or remove EC2 instances based on conditions you specify - these can include such things as at a specific time, or depending on how busy your application is. Auto Scaling cannot change the size of existing instances, nor can it add or change storage on an instance. https://aws.amazon.com/autoscaling/","title":"Auto Scaling"},{"location":"aws/compute/ec2_overview/#optimize-resources","text":"A retail company has EC2 On-Demand Instances running to serve customer transactions. There is a set pattern of traffic where demand is high at 2 points in the day, but the instances sit idle for much of the day. What is a good way to optimize these resources? Use an Auto Scaling group to scale out and in based on demand. The Auto Scaling group can be used to scale out and scale in the instances as the demand dictates. This will save money and avoid having instances sitting idle for long periods of time. AWS Auto Scaling monitors your applications and automatically adjusts your capacity to maintain steady, predictable performance at the lowest possible cost. Using AWS Auto Scaling, it\u2019s easy to set up application scaling for multiple resources across multiple services in minutes. https://aws.amazon.com/autoscaling/","title":"Optimize Resources"},{"location":"aws/compute/ec2_overview/#end","text":"","title":"End"},{"location":"aws/compute/ec2_public_access/","text":"Open EC2 \u00b6 Restrict EC2 access to bayer corporate network (VPN). The EC2 private IP is available directly (on VPN). Private IP of EC2: 10.88.162.132 Add SG to EC2 - port 80 - protocol TCP - Source 0.0.0.0/0 Point LB rule to target group.","title":"Open EC2"},{"location":"aws/compute/ec2_public_access/#open-ec2","text":"Restrict EC2 access to bayer corporate network (VPN). The EC2 private IP is available directly (on VPN). Private IP of EC2: 10.88.162.132 Add SG to EC2 - port 80 - protocol TCP - Source 0.0.0.0/0 Point LB rule to target group.","title":"Open EC2"},{"location":"aws/compute/ec2_ssh/","text":"Provision EC2 \u00b6 Open AWS console and select EC2. Launch instance and name instance \"webserver-v1\" AMI: Amazon Linux 2 Create Key Pair: webserver-v1 chmod 400 webserver-v1.pem Create SG: webserver SSH TCP port 22 anywhere 0.0.0.0/0 HTTP TCP port 80 custom 0.0.0.0/0 Launch Instance SSH to EC2 ssh ec2-user@ec2-3-86-153-196.compute-1.amazonaws.com -i webserver-v1.pem sudo su yum update -y SSH to EC2 on public ip \u00b6 Provision a new EC2 Create a key pair for EC2 SSH from laptop to instance ssh ec2-user@PUBLIC_IP -i MY_PEM_KEY_PAIR.pem Test functionality aws s3 ls","title":"Provision EC2"},{"location":"aws/compute/ec2_ssh/#provision-ec2","text":"Open AWS console and select EC2. Launch instance and name instance \"webserver-v1\" AMI: Amazon Linux 2 Create Key Pair: webserver-v1 chmod 400 webserver-v1.pem Create SG: webserver SSH TCP port 22 anywhere 0.0.0.0/0 HTTP TCP port 80 custom 0.0.0.0/0 Launch Instance SSH to EC2 ssh ec2-user@ec2-3-86-153-196.compute-1.amazonaws.com -i webserver-v1.pem sudo su yum update -y","title":"Provision EC2"},{"location":"aws/compute/ec2_ssh/#ssh-to-ec2-on-public-ip","text":"Provision a new EC2 Create a key pair for EC2 SSH from laptop to instance ssh ec2-user@PUBLIC_IP -i MY_PEM_KEY_PAIR.pem Test functionality aws s3 ls","title":"SSH to EC2 on public ip"},{"location":"aws/compute/elastic_load_balancer_overview/","text":"ELB overview \u00b6 Automatically distributes your incoming application traffic across multiple EC2 instanc AWS ELB Product Comparisons Product comparisons Cloud Load Balancer Categories \u00b6 Generally LBs are divided into two categories, L4 and L7. L7 operates at application layer like HTTP or HTTPS. L4 operates at TCP level. Monitor Health of Instances \u00b6 Load balancers monitor the health of EC2 instances and route the traffic to only instances that are in a healthy state.","title":"ELB overview"},{"location":"aws/compute/elastic_load_balancer_overview/#elb-overview","text":"Automatically distributes your incoming application traffic across multiple EC2 instanc AWS ELB Product Comparisons Product comparisons","title":"ELB overview"},{"location":"aws/compute/elastic_load_balancer_overview/#cloud-load-balancer-categories","text":"Generally LBs are divided into two categories, L4 and L7. L7 operates at application layer like HTTP or HTTPS. L4 operates at TCP level.","title":"Cloud Load Balancer Categories"},{"location":"aws/compute/elastic_load_balancer_overview/#monitor-health-of-instances","text":"Load balancers monitor the health of EC2 instances and route the traffic to only instances that are in a healthy state.","title":"Monitor Health of Instances"},{"location":"aws/compute/sqs/","text":"Simple Queue Service \u00b6 SQS is a message queuing service that allows you to build loosely coupled systems with asynchronous messaging and integration.","title":"Simple Queue Service"},{"location":"aws/compute/sqs/#simple-queue-service","text":"SQS is a message queuing service that allows you to build loosely coupled systems with asynchronous messaging and integration.","title":"Simple Queue Service"},{"location":"aws/compute/workspaces/","text":"AWS Workspaces \u00b6 Amazon WorkSpaces is a managed, secure Desktop-as-a-Service (DaaS) solution. You can use Amazon WorkSpaces to provision either Windows or Linux desktops in just a few minutes and quickly scale to provide thousands of desktops to workers across the globe. Amazon WorkSpaces provides a Desktop as a Service (DaaS) solution. https://aws.amazon.com/workspaces/?workspaces-blogs.sort-by=item.additionalFields.createdDate&workspaces-blogs.sort-order=desc","title":"AWS Workspaces"},{"location":"aws/compute/workspaces/#aws-workspaces","text":"Amazon WorkSpaces is a managed, secure Desktop-as-a-Service (DaaS) solution. You can use Amazon WorkSpaces to provision either Windows or Linux desktops in just a few minutes and quickly scale to provide thousands of desktops to workers across the globe. Amazon WorkSpaces provides a Desktop as a Service (DaaS) solution. https://aws.amazon.com/workspaces/?workspaces-blogs.sort-by=item.additionalFields.createdDate&workspaces-blogs.sort-order=desc","title":"AWS Workspaces"},{"location":"aws/data_storage/ec2_storage/","text":"EC2 Storage \u00b6 EC2 requires a Root Drive \u00b6 Storage Options Elastic Block Store : data persists after EC2 stops Elastic File System : Serverless Only compatible with Linux file systems works as a shared drive , for multiple instances Instance Store : ephemeral, data is lost when stop/terminate EC2 Storage Gateway : Hybrid storage for on -prem and cloud data End \u00b6","title":"EC2 Storage"},{"location":"aws/data_storage/ec2_storage/#ec2-storage","text":"","title":"EC2 Storage"},{"location":"aws/data_storage/ec2_storage/#ec2-requires-a-root-drive","text":"Storage Options Elastic Block Store : data persists after EC2 stops Elastic File System : Serverless Only compatible with Linux file systems works as a shared drive , for multiple instances Instance Store : ephemeral, data is lost when stop/terminate EC2 Storage Gateway : Hybrid storage for on -prem and cloud data","title":"EC2 requires a Root Drive"},{"location":"aws/data_storage/ec2_storage/#end","text":"","title":"End"},{"location":"aws/data_storage/migration_services/","text":"Data Migration Services \u00b6 Database Migration Service \u00b6 DMS helps you migrate databases to or within AWS. https://aws.amazon.com/dms/ AWS Snowball \u00b6 Snowball helps you migrate massive amounts of data into cloud, so it is considered a migration tool. https://aws.amazon.com/cloud-migration/ AWS Application Discovery Service \u00b6 AWS Application Discovery Service helps you gather information about your on-premises environment and is considered a migration tool. https://aws.amazon.com/cloud-migration/ Hetereogeneous Database Migrations \u00b6 In heterogeneous database migrations, the source and target databases tend to be different in schema structure, data type, and database code. Oracle and Amazon Aurora PostgreSQL are not the same. Oracle to Amazon Aurora PostgreSQL Microsoft SQL Server to Amazon Aurora PostgreSQL","title":"Data Migration Services"},{"location":"aws/data_storage/migration_services/#data-migration-services","text":"","title":"Data Migration Services"},{"location":"aws/data_storage/migration_services/#database-migration-service","text":"DMS helps you migrate databases to or within AWS. https://aws.amazon.com/dms/","title":"Database Migration Service"},{"location":"aws/data_storage/migration_services/#aws-snowball","text":"Snowball helps you migrate massive amounts of data into cloud, so it is considered a migration tool. https://aws.amazon.com/cloud-migration/","title":"AWS Snowball"},{"location":"aws/data_storage/migration_services/#aws-application-discovery-service","text":"AWS Application Discovery Service helps you gather information about your on-premises environment and is considered a migration tool. https://aws.amazon.com/cloud-migration/","title":"AWS Application Discovery Service"},{"location":"aws/data_storage/migration_services/#hetereogeneous-database-migrations","text":"In heterogeneous database migrations, the source and target databases tend to be different in schema structure, data type, and database code. Oracle and Amazon Aurora PostgreSQL are not the same. Oracle to Amazon Aurora PostgreSQL Microsoft SQL Server to Amazon Aurora PostgreSQL","title":"Hetereogeneous Database Migrations"},{"location":"aws/data_storage/redshift/","text":"Redshift \u00b6 Redshift is AWS' data warehousing service. [https://docs.aws.amazon.com/redshift/latest/mgmt/welcome.html Redshift is a scalable data warehouse solution that supports querying, reporting, analytics, and business intelligence. It can be used when you need to consolidate multiple data sources for reporting and don't require real-time transaction processing (insert, update, and delete).","title":"Redshift"},{"location":"aws/data_storage/redshift/#redshift","text":"Redshift is AWS' data warehousing service. [https://docs.aws.amazon.com/redshift/latest/mgmt/welcome.html Redshift is a scalable data warehouse solution that supports querying, reporting, analytics, and business intelligence. It can be used when you need to consolidate multiple data sources for reporting and don't require real-time transaction processing (insert, update, and delete).","title":"Redshift"},{"location":"aws/data_storage/s3/","text":"S3 \u00b6 Simple storage S3 Storage Classes \u00b6 S3 Storage Classes S3 Transfer Acceleration Bucket Policy \u00b6 Restrict access to my S3 bucket using policies. You can add a bucket access policy directly to an Amazon S3 bucket to grant IAM users access permissions for the bucket and the objects in it. S3 Lifecycle policy \u00b6 You can add rules in an S3 Lifecycle configuration to tell Amazon S3 to transition objects to another Amazon S3 storage class. For example: When you know objects are infrequently accessed, you might transition them to the S3 Standard-IA storage class. You might want to archive objects that you don't need to access in real time to the S3 Glacier storage class. https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html","title":"S3"},{"location":"aws/data_storage/s3/#s3","text":"Simple storage","title":"S3"},{"location":"aws/data_storage/s3/#s3-storage-classes","text":"S3 Storage Classes S3 Transfer Acceleration","title":"S3 Storage Classes"},{"location":"aws/data_storage/s3/#bucket-policy","text":"Restrict access to my S3 bucket using policies. You can add a bucket access policy directly to an Amazon S3 bucket to grant IAM users access permissions for the bucket and the objects in it.","title":"Bucket Policy"},{"location":"aws/data_storage/s3/#s3-lifecycle-policy","text":"You can add rules in an S3 Lifecycle configuration to tell Amazon S3 to transition objects to another Amazon S3 storage class. For example: When you know objects are infrequently accessed, you might transition them to the S3 Standard-IA storage class. You might want to archive objects that you don't need to access in real time to the S3 Glacier storage class. https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html","title":"S3 Lifecycle policy"},{"location":"aws/data_storage/warehouse/","text":"Data Warehouse \u00b6 A data warehouse aggregates massive amounts of historical data from disparate sources. Not recommended for transactional data, used primarily for analytics and reporting. AWS Redshift \u00b6 Use cases: when you need to consolidate multiple data sources for reporting when you want a DB that doesn't require real time transaction processing (insert, update and delete)","title":"Data Warehouse"},{"location":"aws/data_storage/warehouse/#data-warehouse","text":"A data warehouse aggregates massive amounts of historical data from disparate sources. Not recommended for transactional data, used primarily for analytics and reporting.","title":"Data Warehouse"},{"location":"aws/data_storage/warehouse/#aws-redshift","text":"Use cases: when you need to consolidate multiple data sources for reporting when you want a DB that doesn't require real time transaction processing (insert, update and delete)","title":"AWS Redshift"},{"location":"aws/developer_tools/access_aws/","text":"Access AWS \u00b6 Methods to add services in AWS. AWS Console CLI When working with AWS from the CLI, you need to provide an access key and secret access key.","title":"Access AWS"},{"location":"aws/developer_tools/access_aws/#access-aws","text":"Methods to add services in AWS. AWS Console CLI When working with AWS from the CLI, you need to provide an access key and secret access key.","title":"Access AWS"},{"location":"aws/developer_tools/cloud9/","text":"Cloud9 \u00b6 Cloud9 preconfigures the development environment with the needed SDKs and libraries. Use Cases \u00b6 Write code for AWS lambda directly in web browser","title":"Cloud9"},{"location":"aws/developer_tools/cloud9/#cloud9","text":"Cloud9 preconfigures the development environment with the needed SDKs and libraries.","title":"Cloud9"},{"location":"aws/developer_tools/cloud9/#use-cases","text":"Write code for AWS lambda directly in web browser","title":"Use Cases"},{"location":"aws/developer_tools/cloudfront/","text":"CloudFront \u00b6 CloudFront is a CDN service for static web content. All content is delivered over edge locations, providing low latency. A CloudFront Origin is where content is stored, and where CloudFront gets content to serve to viewers. You can use: An S3 bucket configured with static website hosting An ELB load balancer An AWS Elemental MediaPackage endpoint or AWS Elemental MediaStore container Any HTTP server that runs on an Amazon EC2 instance (or other host). Reference: Amazon CloudFront > Origin . CloudFront content is cached in Edge Locations. A CloudFront distribution is a link between an origin server (such as an Amazon S3 bucket) and a registered domain name (such as Amazon Route 53 or a different registrar). Through this link, CloudFront identifies the object you have stored in your origin server. References: AWS glossary > distribution Routing traffic to an Amazon CloudFront web distribution by using your domain name","title":"CloudFront"},{"location":"aws/developer_tools/cloudfront/#cloudfront","text":"CloudFront is a CDN service for static web content. All content is delivered over edge locations, providing low latency. A CloudFront Origin is where content is stored, and where CloudFront gets content to serve to viewers. You can use: An S3 bucket configured with static website hosting An ELB load balancer An AWS Elemental MediaPackage endpoint or AWS Elemental MediaStore container Any HTTP server that runs on an Amazon EC2 instance (or other host). Reference: Amazon CloudFront > Origin . CloudFront content is cached in Edge Locations. A CloudFront distribution is a link between an origin server (such as an Amazon S3 bucket) and a registered domain name (such as Amazon Route 53 or a different registrar). Through this link, CloudFront identifies the object you have stored in your origin server. References: AWS glossary > distribution Routing traffic to an Amazon CloudFront web distribution by using your domain name","title":"CloudFront"},{"location":"aws/developer_tools/codebuild/","text":"Codebuild \u00b6 Run tests before deploying a new version of an application to production. Compiles source code and runs tests Enables CI/CD Produces build artifacts for deployments.","title":"Codebuild"},{"location":"aws/developer_tools/codebuild/#codebuild","text":"Run tests before deploying a new version of an application to production. Compiles source code and runs tests Enables CI/CD Produces build artifacts for deployments.","title":"Codebuild"},{"location":"aws/developer_tools/codepipeline/","text":"CodePipeline \u00b6 CodePipeline automates the software release process. https://aws.amazon.com/codepipeline/ Automate deployments from end to end.","title":"CodePipeline"},{"location":"aws/developer_tools/codepipeline/#codepipeline","text":"CodePipeline automates the software release process. https://aws.amazon.com/codepipeline/ Automate deployments from end to end.","title":"CodePipeline"},{"location":"aws/developer_tools/codestar/","text":"AWS CodeStar \u00b6 CodeStar helps developers collaboratively work on development projects and track issues via a dashboard.","title":"AWS CodeStar"},{"location":"aws/developer_tools/codestar/#aws-codestar","text":"CodeStar helps developers collaboratively work on development projects and track issues via a dashboard.","title":"AWS CodeStar"},{"location":"aws/developer_tools/personal_health_dashboard/","text":"AWS Personal Health Dashboard \u00b6 AWS Personal Health Dashboard focuses on the performance and availability of your AWS services so you can respond accordingly. https://aws.amazon.com/premiumsupport/technology/personal-health-dashboard/ After experiencing unusual behavior in your AWS account, you need to determine if there are any issues with AWS that may be affecting your account. What section of the AWS Management Console helps you inspect account alerts and find remediation guidance for your account?","title":"AWS Personal Health Dashboard"},{"location":"aws/developer_tools/personal_health_dashboard/#aws-personal-health-dashboard","text":"AWS Personal Health Dashboard focuses on the performance and availability of your AWS services so you can respond accordingly. https://aws.amazon.com/premiumsupport/technology/personal-health-dashboard/ After experiencing unusual behavior in your AWS account, you need to determine if there are any issues with AWS that may be affecting your account. What section of the AWS Management Console helps you inspect account alerts and find remediation guidance for your account?","title":"AWS Personal Health Dashboard"},{"location":"aws/developer_tools/xray/","text":"X-Ray \u00b6 AWS X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your application\u2019s underlying components. You can use X-Ray to analyze from simple three-tier applications to complex microservices applications consisting of thousands of services. https://aws.amazon.com/xray/ Analyze and debug production application Map application components View request flow end to end Trace user requests from end to end through the application","title":"X-Ray"},{"location":"aws/developer_tools/xray/#x-ray","text":"AWS X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your application\u2019s underlying components. You can use X-Ray to analyze from simple three-tier applications to complex microservices applications consisting of thousands of services. https://aws.amazon.com/xray/ Analyze and debug production application Map application components View request flow end to end Trace user requests from end to end through the application","title":"X-Ray"},{"location":"aws/ec2_lb_priv/debug_elb_ec2_private_subnets/","text":"I am using Amazon EC2, and I want to put an internet-facing ELB (load balancer) to 2 instances on a private subnet. I am using VPC with public and private subnets. Helpful stack overflow links https://stackoverflow.com/questions/22541895/amazon-elb-for-ec2-instances-in-private-subnet-in-vpc https://stackoverflow.com/questions/9257514/amazon-elb-in-vpc HTTP codes I got from the ELB: 504 - Probably security groups - allow access to the port 80 of the instances 503 - Probably the wrong target group setup 502 - Probably Apache/Server not running b/c it\u2019s not installed because there's no nat gw or method to install software. You need to attach only public subnets to your ELB, making sure that the availability zones those subnets are aligned with the availability zones of the private subnets that your instances are in. Make sure that the security group of your instances allows access from the security group of your load balancer The load balancer security group should have an egress rule allowing the health check to reach the instance Make sure that your health check is working locally on the instance. For example, if your health check in the ELB is HTTP:8080/health_check, on the instance you can curl x.x.x.x:8080/health_check (where x.x.x.x is the private IP of the instance) and get a 200 response code. The public subnet routing table should route 0.0.0.0/0 to the internet gateway attached to your VPC. The private subnet routing table should route 0.0.0.0/0 to a NAT instance or gateway in a public subnet","title":"Debug elb ec2 private subnets"},{"location":"aws/ec2_lb_priv/ec2_alb_setup/","text":"EC2 and ALB \u00b6 Register the EC2 to the target group. This allows us to put our EC2 behind a load balancer. The DNS name maps the ALB back to the EC2. Using the DNS name is more flexible, compared to the IP address. IP addresses will change, when we add/delete EC2 instances. Provision ALB and EC2 Webserver \u00b6 Open EC2 console and select load balancer. Select ALB ALB name my-alb Check every AZ use the webserver security group created earlier Create target group target group name webservers Launch ALB. Wait ~5 minutes for ALB to be available. Configure new server \u00b6 Make a note of AZ the EC2 instance is running in. My EC2 AZ is us-east-1a Provision new EC2 Set subnet to AZ , which differs than the other EC2 AZ. I chose us-east-1d EC2 name webserver-v2 Use webserver SG, we created this earlier. Scroll down to \"Advanced details \" and add bootstrap script. #!/bin/bash yum update -y yum install httpd -y service httpd start chkconfig on cd /var/www/html echo \"<html><body><h1> My webserver v2 </h1></body></html>\" > index.html In ALB copy DNS IP my-alb-307624658.us-east-1.elb.amazonaws.com . This DNS name will now resolve to our EC2. Next add ec2 we just created webserver-v2 behind ALB. open target groups and click on target group webservers select \"register target\" select \"include as pending below\" and \"register target\" Give the new target a few minutes to pass health checks","title":"EC2 and ALB"},{"location":"aws/ec2_lb_priv/ec2_alb_setup/#ec2-and-alb","text":"Register the EC2 to the target group. This allows us to put our EC2 behind a load balancer. The DNS name maps the ALB back to the EC2. Using the DNS name is more flexible, compared to the IP address. IP addresses will change, when we add/delete EC2 instances.","title":"EC2 and ALB"},{"location":"aws/ec2_lb_priv/ec2_alb_setup/#provision-alb-and-ec2-webserver","text":"Open EC2 console and select load balancer. Select ALB ALB name my-alb Check every AZ use the webserver security group created earlier Create target group target group name webservers Launch ALB. Wait ~5 minutes for ALB to be available.","title":"Provision ALB and EC2 Webserver"},{"location":"aws/ec2_lb_priv/ec2_alb_setup/#configure-new-server","text":"Make a note of AZ the EC2 instance is running in. My EC2 AZ is us-east-1a Provision new EC2 Set subnet to AZ , which differs than the other EC2 AZ. I chose us-east-1d EC2 name webserver-v2 Use webserver SG, we created this earlier. Scroll down to \"Advanced details \" and add bootstrap script. #!/bin/bash yum update -y yum install httpd -y service httpd start chkconfig on cd /var/www/html echo \"<html><body><h1> My webserver v2 </h1></body></html>\" > index.html In ALB copy DNS IP my-alb-307624658.us-east-1.elb.amazonaws.com . This DNS name will now resolve to our EC2. Next add ec2 we just created webserver-v2 behind ALB. open target groups and click on target group webservers select \"register target\" select \"include as pending below\" and \"register target\" Give the new target a few minutes to pass health checks","title":"Configure new server"},{"location":"aws/ec2_lb_priv/ec2_lb_ocelot_setup/","text":"Create EC2 and Load Balancer \u00b6 This is a simple EC2 with an apache web server installed. The webserver is accessable behind a load balancer. Requirements \u00b6 This demo requires the VPC with pub to priv cidr installed from service catalog. fargate-infra SC product. This deploys the ALB. transit gateway from SC for ocelot (Mulesoft is not required for this doc but is a separate gateway) Ocelot \u00b6 Setup ocelot to proxy traffic. Add the route name to Ocleot and route ownership group. The prod versions of Ocelot and Akana can only route to prod accounts; and, likewise, non-prod versions can only route to non-prod accounts. Each service will need its own route that forwards traffic to a domain name in the form of https://<AppName>.<UniqueName>.<AccountId>.cloud.bayer.com where the components are these: AppName is the value provided to fg-deploy in the app_parameters.json file UniqueName is the value provided during infrastructure setup (see \"Create DNS Record, ALB, etc.\") AccountId is the ID of the AWS account where you deployed the service Ocelot Prod route name: nginx-mike.velocity.ag route ownership: We use API DSS but your developer group will be different. Add a host name, such as https://nginx-mike.fargate.722540083300.cloud.bayer.com host http://<application-name><.fargate.722540083300><.cloud.bayer.com> application-name can be whaterver you want 722540083300 is the AWS account number always use .cloud.bayer.com Ocelot takes HTTP requests and proxies to one or more approved backend hosts. Here you define how to reach those hosts from Ocelot, either by hostname or IP. The host name requies a protocol and the port is optional. You should omit the path or make it '/'. Ocelot Route key \u00b6 A route key has been generated for your route: nginx-mike.velocity.ag Please save this key, as it will not be available again. If you lose this key you will need to generate a new one. Ocelot will send this key with all routed requests in the ocelot-route-key header. At your target, you can use this key to validate that the call did originate from a valid route definition in Ocelot. Save key from ocelot 569d7b80-74d9-11ed-bf2e-9fefebdc0ebb Install EC2 \u00b6 Install EC2. I use Ubuntu 20.04 and attach the VPC SG. The VPC SG is typically listed as default but verify, when there are multiple VPCs in the account. The inbound and outbound rules for this SG allow all traffic. This will allow the ALB SG created by the fargate-infra product to connect to our EC2. EC2 > network settings > Use default VPC. This VPC is installed from service catalog and has the pub/priv subnets. Apache \u00b6 Install apache webserver in the EC2 bootstrap script #!/bin/bash sudo apt update -y sudo apt install -y apache2 nano sudo systemctl start apache2 sudo systemctl enable apache2 ## enable apache to load on boot yum update -y yum install httpd -y systemctl start httpd systemctl enable httpd Connect to EC2 secure session manager. Make sure apache is running. bash sudo -u ubuntu -i ## sudo systemctl status apache2 ## check web server ## hostname -I ## copy and check addressess in browser ## curl -4 icanhazip.com ## Icanhazip tool, which should give you your public IP address as read from another location on the internet: ## sudo ufw status ## Verify http traffic is allowed Security Groups \u00b6 Configure virtual firewall on EC2, to allow LB to proxy through Ocelot Default VPC SG Create SG to allow proxy Ocelot/Mulesoft/Akana Create the SG. Type HTTP will default to port 80. Add the SG created by the fargate-infra product to connect the EC2. This will allow the LB to allow traffic from Ocelot. **To find the SG, look for the SG with Alb in the SG name. After creating the SG we must associate it with the EC2. SG Rules for bayer proxies. - Outbound rules All traffic All Protocol , All Ports and Destination 0.0.0.0/0 - Inbound rules require port 443 - Mulesoft CIDR 10.70.200.0/21 - Ocelot CIDR 10.62.21.0/24 - Akana CIDR 192.168.0.0/24 - This SG ID = sg-0be806ad6114041cd Open EC2 and add SG sg-0be806ad6114041cd to network interface. Now our EC2 has two SGs. The inbound rules of the EC2 should look like this. Create Target Group \u00b6 In the console navigate to EC2 > Target group. Add the \"instances\" for target type. Target group name apache-target-group Add VPC in account Register target to complete this step. Load Balancer \u00b6 In the console navigate to EC2 > LB. Select the load balancer creatd from the fargate-infra product. To add a rule using the console Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. On the navigation pane, under LOAD BALANCING, choose Load Balancers. Select the load balancer and choose Listeners. For the listener to update, choose View/edit rules. Choose the Add rules icon (the plus sign) in the menu bar, which adds Insert Rule icons at the locations where you can insert a rule in the priority order. Choose one of the Insert Rule icons added in the previous step. Add one or more conditions as follows: To add a host header condition, choose Add condition, Host header and enter the hostname (for example, *.example.com ). To save the condition, choose the checkmark icon. (Optional, HTTPS listener) To authenticate users, choose Add action, Authenticate and provide the requested information. To save the action, choose the checkmark icon. For more information, see Authenticate users using an Application Load Balancer . Add one of the following actions: To add a forward action, choose Add action, Forward to and choose one or more target groups. If you use more than one target group, select a weight for each target group and optionally enable target group stickiness. If you enable target group stickiness and there is more than one target group, you must also enable sticky sessions on the target groups. To save the action, choose the checkmark icon. For more information, see Forward actions . To add a redirect action, choose Add action, Redirect to and provide the URL for the redirect. To save the action, choose the checkmark icon. For more information, see Redirect Actions . To add a fixed-response action, choose Add action, Return fixed response and provide a response code and optional response body. To save the action, choose the checkmark icon. For more information, see Fixed-response actions . The maximum size of each string is 128 characters. The comparison is not case-sensitive. The following wildcard characters are supported: * and ?. Select \"Manage Rules\" to edit the Listener Open browser and check if ocelot is proxying the DNS. Ocelot hosts: - https://apache2.fargate.722540083300.cloud.bayer.com - https://apache-mike.fargate.722540083300.cloud.bayer.com To Do Delete SG sg-067845c651b95e98e (ec2 ocelot security group) the error shows network interface error To Do Draw traffic flow for our AWS sirius sandbox prod account Apache2 EC2 SG for LB from fargate-infra sg-0e9c45979c18218e1 Inbound Rules HTTP port 80 Source: sg-0be806ad6114041cd sg-0be806ad6114041cd uses port 443 on inbound rules to source: Mulesoft 10.70.200.0/21 Ocelot 10.62.21.0/24 Akana 192.168.0.0/24","title":"Create EC2 and Load Balancer"},{"location":"aws/ec2_lb_priv/ec2_lb_ocelot_setup/#create-ec2-and-load-balancer","text":"This is a simple EC2 with an apache web server installed. The webserver is accessable behind a load balancer.","title":"Create EC2 and Load Balancer"},{"location":"aws/ec2_lb_priv/ec2_lb_ocelot_setup/#requirements","text":"This demo requires the VPC with pub to priv cidr installed from service catalog. fargate-infra SC product. This deploys the ALB. transit gateway from SC for ocelot (Mulesoft is not required for this doc but is a separate gateway)","title":"Requirements"},{"location":"aws/ec2_lb_priv/ec2_lb_ocelot_setup/#ocelot","text":"Setup ocelot to proxy traffic. Add the route name to Ocleot and route ownership group. The prod versions of Ocelot and Akana can only route to prod accounts; and, likewise, non-prod versions can only route to non-prod accounts. Each service will need its own route that forwards traffic to a domain name in the form of https://<AppName>.<UniqueName>.<AccountId>.cloud.bayer.com where the components are these: AppName is the value provided to fg-deploy in the app_parameters.json file UniqueName is the value provided during infrastructure setup (see \"Create DNS Record, ALB, etc.\") AccountId is the ID of the AWS account where you deployed the service Ocelot Prod route name: nginx-mike.velocity.ag route ownership: We use API DSS but your developer group will be different. Add a host name, such as https://nginx-mike.fargate.722540083300.cloud.bayer.com host http://<application-name><.fargate.722540083300><.cloud.bayer.com> application-name can be whaterver you want 722540083300 is the AWS account number always use .cloud.bayer.com Ocelot takes HTTP requests and proxies to one or more approved backend hosts. Here you define how to reach those hosts from Ocelot, either by hostname or IP. The host name requies a protocol and the port is optional. You should omit the path or make it '/'.","title":"Ocelot"},{"location":"aws/ec2_lb_priv/ec2_lb_ocelot_setup/#ocelot-route-key","text":"A route key has been generated for your route: nginx-mike.velocity.ag Please save this key, as it will not be available again. If you lose this key you will need to generate a new one. Ocelot will send this key with all routed requests in the ocelot-route-key header. At your target, you can use this key to validate that the call did originate from a valid route definition in Ocelot. Save key from ocelot 569d7b80-74d9-11ed-bf2e-9fefebdc0ebb","title":"Ocelot Route key"},{"location":"aws/ec2_lb_priv/ec2_lb_ocelot_setup/#install-ec2","text":"Install EC2. I use Ubuntu 20.04 and attach the VPC SG. The VPC SG is typically listed as default but verify, when there are multiple VPCs in the account. The inbound and outbound rules for this SG allow all traffic. This will allow the ALB SG created by the fargate-infra product to connect to our EC2. EC2 > network settings > Use default VPC. This VPC is installed from service catalog and has the pub/priv subnets.","title":"Install EC2"},{"location":"aws/ec2_lb_priv/ec2_lb_ocelot_setup/#apache","text":"Install apache webserver in the EC2 bootstrap script #!/bin/bash sudo apt update -y sudo apt install -y apache2 nano sudo systemctl start apache2 sudo systemctl enable apache2 ## enable apache to load on boot yum update -y yum install httpd -y systemctl start httpd systemctl enable httpd Connect to EC2 secure session manager. Make sure apache is running. bash sudo -u ubuntu -i ## sudo systemctl status apache2 ## check web server ## hostname -I ## copy and check addressess in browser ## curl -4 icanhazip.com ## Icanhazip tool, which should give you your public IP address as read from another location on the internet: ## sudo ufw status ## Verify http traffic is allowed","title":"Apache"},{"location":"aws/ec2_lb_priv/ec2_lb_ocelot_setup/#security-groups","text":"Configure virtual firewall on EC2, to allow LB to proxy through Ocelot Default VPC SG Create SG to allow proxy Ocelot/Mulesoft/Akana Create the SG. Type HTTP will default to port 80. Add the SG created by the fargate-infra product to connect the EC2. This will allow the LB to allow traffic from Ocelot. **To find the SG, look for the SG with Alb in the SG name. After creating the SG we must associate it with the EC2. SG Rules for bayer proxies. - Outbound rules All traffic All Protocol , All Ports and Destination 0.0.0.0/0 - Inbound rules require port 443 - Mulesoft CIDR 10.70.200.0/21 - Ocelot CIDR 10.62.21.0/24 - Akana CIDR 192.168.0.0/24 - This SG ID = sg-0be806ad6114041cd Open EC2 and add SG sg-0be806ad6114041cd to network interface. Now our EC2 has two SGs. The inbound rules of the EC2 should look like this.","title":"Security Groups"},{"location":"aws/ec2_lb_priv/ec2_lb_ocelot_setup/#create-target-group","text":"In the console navigate to EC2 > Target group. Add the \"instances\" for target type. Target group name apache-target-group Add VPC in account Register target to complete this step.","title":"Create Target Group"},{"location":"aws/ec2_lb_priv/ec2_lb_ocelot_setup/#load-balancer","text":"In the console navigate to EC2 > LB. Select the load balancer creatd from the fargate-infra product. To add a rule using the console Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. On the navigation pane, under LOAD BALANCING, choose Load Balancers. Select the load balancer and choose Listeners. For the listener to update, choose View/edit rules. Choose the Add rules icon (the plus sign) in the menu bar, which adds Insert Rule icons at the locations where you can insert a rule in the priority order. Choose one of the Insert Rule icons added in the previous step. Add one or more conditions as follows: To add a host header condition, choose Add condition, Host header and enter the hostname (for example, *.example.com ). To save the condition, choose the checkmark icon. (Optional, HTTPS listener) To authenticate users, choose Add action, Authenticate and provide the requested information. To save the action, choose the checkmark icon. For more information, see Authenticate users using an Application Load Balancer . Add one of the following actions: To add a forward action, choose Add action, Forward to and choose one or more target groups. If you use more than one target group, select a weight for each target group and optionally enable target group stickiness. If you enable target group stickiness and there is more than one target group, you must also enable sticky sessions on the target groups. To save the action, choose the checkmark icon. For more information, see Forward actions . To add a redirect action, choose Add action, Redirect to and provide the URL for the redirect. To save the action, choose the checkmark icon. For more information, see Redirect Actions . To add a fixed-response action, choose Add action, Return fixed response and provide a response code and optional response body. To save the action, choose the checkmark icon. For more information, see Fixed-response actions . The maximum size of each string is 128 characters. The comparison is not case-sensitive. The following wildcard characters are supported: * and ?. Select \"Manage Rules\" to edit the Listener Open browser and check if ocelot is proxying the DNS. Ocelot hosts: - https://apache2.fargate.722540083300.cloud.bayer.com - https://apache-mike.fargate.722540083300.cloud.bayer.com To Do Delete SG sg-067845c651b95e98e (ec2 ocelot security group) the error shows network interface error To Do Draw traffic flow for our AWS sirius sandbox prod account Apache2 EC2 SG for LB from fargate-infra sg-0e9c45979c18218e1 Inbound Rules HTTP port 80 Source: sg-0be806ad6114041cd sg-0be806ad6114041cd uses port 443 on inbound rules to source: Mulesoft 10.70.200.0/21 Ocelot 10.62.21.0/24 Akana 192.168.0.0/24","title":"Load Balancer"},{"location":"aws/ec2_lb_priv/ec2_nginx/","text":"EC2 Hello World \u00b6 Simple EC2 for testing cloud services. Often we have edge cases requiring a simple server in a cloud account. This EC2 has boilerplate options, which can be expanded to fit unique situations. This is an effort to minimize the intellectual burden associated with cloud infra. This project took me 3 hours to first implement. Now I can do everything in about an hour. VPC design patterns to keep our VPC secure \u00b6 The best practice is to limit the number of entry points to our VPC by using the Application Load Balancer (ALB) for HTTP/HTTPS traffic and the bastion host for SSH traffic. With this, we can deploy hundreds of applications in our VPC yet still keep the entry points to our VPC to just the ALB and the bastion host. The AWS environment I used for this post is detailed below. I don't discuss how to setup it up in this post but I will do so in another post. I will link it here when it's finished. VPC with 4 subnets: 2 private subnets and 2 public subnets. Instances in the private subnet cannot be accessed directly from the internet but the instances themselves can access the internet (i.e to get software updates and patches, etc). An application load balancer placed on the 2 public subnets. The ALB should be able to serve HTTP/HTTPs from anywhere. By design, an ALB has servers on the public subnets. Traffic goes in to these servers, and based on the request's path and host header, it should decide where to direct traffic. A bastion host that can serve SSH traffic from anywhere. We will use this as a way to access all of our instance in the private subnet. Cloud Artifacts \u00b6 Services created in AWS and pre-requirements which need to be verified. All my AWS links go to region us-east-1 AMI I use ubuntu but any bayer image is technically feasible. EC2 pick a cheap option for hardware, free tier is best Security Groups Virtual firewall for network traffic Minimum pre-requirements. Before starting verify a VPC is installed. Create a VPC using the service catalog. The SC product labelled as \"pub and priv subnets with reserved CIDR\" is typically what we use. VPC ID: vpc-08dff0d6b84e7fe21 VPC Name: SC-073416988478-pp-czuonlduuhoo4 Delete these resources when finished. Leaving behind cloud resources costs money for Bayer (bad) and confuses me later when debugging cloud issues (very very bad). :) TODO automate the provisioning/destruction of the services (PRs welcome) The AWS console is used to provision the resources. CLI/SDK tools are available for power users but are not displayed here. IAM \u00b6 Create an (IAM Role)[https://us-east-1.console.aws.amazon.com/iamv2/home?region=us-east-1#/roles] for SSM access to EC2. Create a IAM role. During creation, attach the AWS policy to the role. This policy allows SSH access and log creation. IAM Role Name: nginx_elxsj Security Group \u00b6 Fetch Bastion SG \u00b6 Copy the bastion SG ID. sg-085a4daa2d98002c6 description Bastion Host security group. Other servers in this VPC should only accept SSH traffic from this group default VPC sg sg-08de2c7be6b097812 Create Security Groups \u00b6 SG name: nginx_ec2 If we deploy our EC2 instance now, we would not be able to access it at all. This is because the security group of our EC2 instance aren't set up to accept any connections. Security groups are a set of rules for incoming and outgoing traffic. They govern which resources can communicate with a specific set of resources and in what way (i.e allow only connections via port 22 \"ssh\"). By default, a security group's rules for outgoing traffic are a pass-all (all traffic leaving the instance is allowed). For incoming traffic, however, we are left with the discretion of what resources we want to allow to connect to our EC2 instance and what kind of connections with them we would allow. We can specify these resources in 3 ways: a range of IP addresses (i.e allow all computers within the IP range of 192.168.0.0/24 to connect to my instance across all ports). a specific IP address (i.e allow 192.168.12.1 to connect to my ec2 instance via port 80 [http]), a security group (i.e allow instances with the security group \"bastion-host-sg\" to connect to my instance via port 22). Using this option is easier if the resources you are giving access to are within AWS. This is because you can just keep on adding instances into the chosen security group rather than add a new rule in this security group for every new instance we want to give access to For example: rather than creating a rule to allow SSH traffic from 192.168.12.1 (\"EC2 instance A\") and another rule to allow SSH traffic from 192.168.12.2 (\"EC2 instance B\"), we can create a security group (\"bastion-host-sg\"), add EC2 instance A and B there, and add this security group to the rules of the security group for this EC2 instance (\"ec2-nginx-sg\"). With this, security groups serve 2 purposes. They contain a set of rules to govern incoming and outgoing traffic. It also serves as a grouping of AWS resources. This grouping can be referred to by other security groups in their own rules. We have to set up the security group of our EC2 instance to be able to accept SSH traffic (so we can connect to it via SSH) and accept traffic from port 80 (http) from the load balancer. For our setup, we will create a new security group and name it \"ec2-nginx-sg\". We would allow: port 22 (SSH) connections from the security group of the bastion host port 80 (HTTP) connections from the security group of the application load balancer. Bayer Proxy SG \u00b6 Allow reverse proxy to access VPC. Open EC2 > Security Groups and verify Bayer proxies, e.g.: Akana TCP 443 192.168.0.0/24 Akana Cidr Ocelot TCP 443 10.62.21.0/24 Ocelot Cidr Mulesoft TCP 443 10.70.200.0/21 Mulesoft Cidr Provision EC2 \u00b6 This EC2 should take about five minutes to provision. Check the EC2 Console to verify the \"Instance state\" is Running. EC2 name: nginx EC2 SG name: nginx_default EC2 key name: nginx_ec2 nginx_ec2_keypair_sirius_np CloudWatch Logs \u00b6 Create log groups for SSM log group name: ssm_session EC2 Session Manager \u00b6 The session manager allow us to connect to any (almost) running EC2 and reduces the need for SSH keys. Open SSM from Systems manager | | | :--: | | | | | Optionally we can open SSM from EC2 dashboard Enable CloudWatch logging Add the log group we created this a moment ago Open SSM and check for ubuntu user in a shell $ awk -F: '{ print $1}' /etc/passwd | grep ub ubuntu $ bash ssm-user@ip-10-88-214-149:~$ sudo apt-get -y update sudo apt install -y nginx sudo -u ubuntu -i Verify nginx is running \u00b6 ssm-user@ip-10-88-215-170:/var/snap/amazon-ssm-agent/6312$ systemctl status nginx \u25cf nginx.service - A high performance web server and a reverse proxy server Loaded: loaded ( /lib/systemd/system/nginx.service ; enabled ; vendor preset: enabled ) Active: active ( running ) since Sat 2022 -11-19 00 :15:46 UTC ; 2min 31s ago Docs: man:nginx ( 8 ) Main PID: 2855 ( nginx ) Tasks: 2 ( limit: 1145 ) Memory: 4 .6M CGroup: /system.slice/nginx.service \u251c\u25002855 nginx: master process /usr/sbin/nginx -g daemon on ; master_process on ; \u2514\u25002856 nginx: worker process Run this if nginx is not running sudo service nginx start Configure nginx \u00b6 Adjusting ubuntu firewall (ufw), if you think it is needed to install then you can work with this step. Firstly you should show the ufw list with the execution following command: $ sudo ufw app list Available applications: Nginx Full Nginx HTTP Nginx HTTPS Allow port 80 for http request with the following command: $ sudo ufw allow \u2018NginxHTTP\u2019 $ sudo ufw status ( to see ufw status ) $ nginx -v ( to see nginx version ) $ sudo nginx -t ( to see nginx configuration file is ok or not ) $ sudo systemctl reload nginx ( to reload nginx if any change in conf. file ) $ sudo systemctl stop nginx ( to stop nginx service ) $ sudo systemctl restart nginx ( to restart nginx )","title":"Ec2 nginx"},{"location":"aws/ec2_lb_priv/ec2_nginx/#ec2-hello-world","text":"Simple EC2 for testing cloud services. Often we have edge cases requiring a simple server in a cloud account. This EC2 has boilerplate options, which can be expanded to fit unique situations. This is an effort to minimize the intellectual burden associated with cloud infra. This project took me 3 hours to first implement. Now I can do everything in about an hour.","title":"EC2 Hello World"},{"location":"aws/ec2_lb_priv/ec2_nginx/#vpc-design-patterns-to-keep-our-vpc-secure","text":"The best practice is to limit the number of entry points to our VPC by using the Application Load Balancer (ALB) for HTTP/HTTPS traffic and the bastion host for SSH traffic. With this, we can deploy hundreds of applications in our VPC yet still keep the entry points to our VPC to just the ALB and the bastion host. The AWS environment I used for this post is detailed below. I don't discuss how to setup it up in this post but I will do so in another post. I will link it here when it's finished. VPC with 4 subnets: 2 private subnets and 2 public subnets. Instances in the private subnet cannot be accessed directly from the internet but the instances themselves can access the internet (i.e to get software updates and patches, etc). An application load balancer placed on the 2 public subnets. The ALB should be able to serve HTTP/HTTPs from anywhere. By design, an ALB has servers on the public subnets. Traffic goes in to these servers, and based on the request's path and host header, it should decide where to direct traffic. A bastion host that can serve SSH traffic from anywhere. We will use this as a way to access all of our instance in the private subnet.","title":"VPC design patterns to keep our VPC secure"},{"location":"aws/ec2_lb_priv/ec2_nginx/#cloud-artifacts","text":"Services created in AWS and pre-requirements which need to be verified. All my AWS links go to region us-east-1 AMI I use ubuntu but any bayer image is technically feasible. EC2 pick a cheap option for hardware, free tier is best Security Groups Virtual firewall for network traffic Minimum pre-requirements. Before starting verify a VPC is installed. Create a VPC using the service catalog. The SC product labelled as \"pub and priv subnets with reserved CIDR\" is typically what we use. VPC ID: vpc-08dff0d6b84e7fe21 VPC Name: SC-073416988478-pp-czuonlduuhoo4 Delete these resources when finished. Leaving behind cloud resources costs money for Bayer (bad) and confuses me later when debugging cloud issues (very very bad). :) TODO automate the provisioning/destruction of the services (PRs welcome) The AWS console is used to provision the resources. CLI/SDK tools are available for power users but are not displayed here.","title":"Cloud Artifacts"},{"location":"aws/ec2_lb_priv/ec2_nginx/#iam","text":"Create an (IAM Role)[https://us-east-1.console.aws.amazon.com/iamv2/home?region=us-east-1#/roles] for SSM access to EC2. Create a IAM role. During creation, attach the AWS policy to the role. This policy allows SSH access and log creation. IAM Role Name: nginx_elxsj","title":"IAM"},{"location":"aws/ec2_lb_priv/ec2_nginx/#security-group","text":"","title":"Security Group"},{"location":"aws/ec2_lb_priv/ec2_nginx/#fetch-bastion-sg","text":"Copy the bastion SG ID. sg-085a4daa2d98002c6 description Bastion Host security group. Other servers in this VPC should only accept SSH traffic from this group default VPC sg sg-08de2c7be6b097812","title":"Fetch Bastion SG"},{"location":"aws/ec2_lb_priv/ec2_nginx/#create-security-groups","text":"SG name: nginx_ec2 If we deploy our EC2 instance now, we would not be able to access it at all. This is because the security group of our EC2 instance aren't set up to accept any connections. Security groups are a set of rules for incoming and outgoing traffic. They govern which resources can communicate with a specific set of resources and in what way (i.e allow only connections via port 22 \"ssh\"). By default, a security group's rules for outgoing traffic are a pass-all (all traffic leaving the instance is allowed). For incoming traffic, however, we are left with the discretion of what resources we want to allow to connect to our EC2 instance and what kind of connections with them we would allow. We can specify these resources in 3 ways: a range of IP addresses (i.e allow all computers within the IP range of 192.168.0.0/24 to connect to my instance across all ports). a specific IP address (i.e allow 192.168.12.1 to connect to my ec2 instance via port 80 [http]), a security group (i.e allow instances with the security group \"bastion-host-sg\" to connect to my instance via port 22). Using this option is easier if the resources you are giving access to are within AWS. This is because you can just keep on adding instances into the chosen security group rather than add a new rule in this security group for every new instance we want to give access to For example: rather than creating a rule to allow SSH traffic from 192.168.12.1 (\"EC2 instance A\") and another rule to allow SSH traffic from 192.168.12.2 (\"EC2 instance B\"), we can create a security group (\"bastion-host-sg\"), add EC2 instance A and B there, and add this security group to the rules of the security group for this EC2 instance (\"ec2-nginx-sg\"). With this, security groups serve 2 purposes. They contain a set of rules to govern incoming and outgoing traffic. It also serves as a grouping of AWS resources. This grouping can be referred to by other security groups in their own rules. We have to set up the security group of our EC2 instance to be able to accept SSH traffic (so we can connect to it via SSH) and accept traffic from port 80 (http) from the load balancer. For our setup, we will create a new security group and name it \"ec2-nginx-sg\". We would allow: port 22 (SSH) connections from the security group of the bastion host port 80 (HTTP) connections from the security group of the application load balancer.","title":"Create Security Groups"},{"location":"aws/ec2_lb_priv/ec2_nginx/#bayer-proxy-sg","text":"Allow reverse proxy to access VPC. Open EC2 > Security Groups and verify Bayer proxies, e.g.: Akana TCP 443 192.168.0.0/24 Akana Cidr Ocelot TCP 443 10.62.21.0/24 Ocelot Cidr Mulesoft TCP 443 10.70.200.0/21 Mulesoft Cidr","title":"Bayer Proxy SG"},{"location":"aws/ec2_lb_priv/ec2_nginx/#provision-ec2","text":"This EC2 should take about five minutes to provision. Check the EC2 Console to verify the \"Instance state\" is Running. EC2 name: nginx EC2 SG name: nginx_default EC2 key name: nginx_ec2 nginx_ec2_keypair_sirius_np","title":"Provision EC2"},{"location":"aws/ec2_lb_priv/ec2_nginx/#cloudwatch-logs","text":"Create log groups for SSM log group name: ssm_session","title":"CloudWatch Logs"},{"location":"aws/ec2_lb_priv/ec2_nginx/#ec2-session-manager","text":"The session manager allow us to connect to any (almost) running EC2 and reduces the need for SSH keys. Open SSM from Systems manager | | | :--: | | | | | Optionally we can open SSM from EC2 dashboard Enable CloudWatch logging Add the log group we created this a moment ago Open SSM and check for ubuntu user in a shell $ awk -F: '{ print $1}' /etc/passwd | grep ub ubuntu $ bash ssm-user@ip-10-88-214-149:~$ sudo apt-get -y update sudo apt install -y nginx sudo -u ubuntu -i","title":"EC2 Session Manager"},{"location":"aws/ec2_lb_priv/ec2_nginx/#verify-nginx-is-running","text":"ssm-user@ip-10-88-215-170:/var/snap/amazon-ssm-agent/6312$ systemctl status nginx \u25cf nginx.service - A high performance web server and a reverse proxy server Loaded: loaded ( /lib/systemd/system/nginx.service ; enabled ; vendor preset: enabled ) Active: active ( running ) since Sat 2022 -11-19 00 :15:46 UTC ; 2min 31s ago Docs: man:nginx ( 8 ) Main PID: 2855 ( nginx ) Tasks: 2 ( limit: 1145 ) Memory: 4 .6M CGroup: /system.slice/nginx.service \u251c\u25002855 nginx: master process /usr/sbin/nginx -g daemon on ; master_process on ; \u2514\u25002856 nginx: worker process Run this if nginx is not running sudo service nginx start","title":"Verify nginx is running"},{"location":"aws/ec2_lb_priv/ec2_nginx/#configure-nginx","text":"Adjusting ubuntu firewall (ufw), if you think it is needed to install then you can work with this step. Firstly you should show the ufw list with the execution following command: $ sudo ufw app list Available applications: Nginx Full Nginx HTTP Nginx HTTPS Allow port 80 for http request with the following command: $ sudo ufw allow \u2018NginxHTTP\u2019 $ sudo ufw status ( to see ufw status ) $ nginx -v ( to see nginx version ) $ sudo nginx -t ( to see nginx configuration file is ok or not ) $ sudo systemctl reload nginx ( to reload nginx if any change in conf. file ) $ sudo systemctl stop nginx ( to stop nginx service ) $ sudo systemctl restart nginx ( to restart nginx )","title":"Configure nginx"},{"location":"aws/ec2_lb_priv/ec2_priv_sub/","text":"Apache on Private Subnet \u00b6 Install ec2 using cloudformation. \u201cA load balancer receives requests and then transfers them to targets defined in a target group. We can create an Application Load balancer either using the AWS management console or AWS CLI. There are several routing options with AWS Application Load Balancer, e.g., Host-Based routing. In Host-based routing, incoming traffic is routed on the basis of the domain name or host name given in the Host Header. In this tutorial, we are going to create an Application Load balancer with Host-Based routing.\u201d Provision EC2 Webserver \u00b6 Save the instance name of the EC2. This is used for viewing the webserver and is required for managing the LB; however the CF template saves this in the parameters. Stack name: apache-priv-elxsj IntanceName: apache-priv-elxsj Security Groups \u00b6 During CF provisioning, thee default VPC SG is attached to the EC2. In addition a SG for Ocelot traffic is required. Skip this step if Ocelot is not used. sg-0be806ad6114041cd - Create a new SG - Add Inbound rules - use the existing Ocelot SG in account. Create the Ocelot Security Group. The existing Ocelot SG in the VPC, is used to represent the Ocelot traffic IPs. Associate SG with EC2 Associate the new SG with the EC2. Notice our EC2 now has two SGs. Load Balancer \u00b6 Normally a new LB would be provisioned. For simplicity , we use an existing LB. The fargate-infra service catalog product provisions an LB. fargate-infra creates infrastructure required by ECS Fargate services. The latest version includes an ALB, a Route53 alias record, an ACM certificate, and an ECS cluster. See https://devtools.bayer.com/docs/hosting/aws/fargate/ Create a New Target Group \u00b6 Navigate to EC2 > Target groups > Create target group target group name: apache-priv-subnet Choose instance based target type. Target Group type For the \u201cProtocol\u201d and \u201cPort\u201d options, select \u201cHTTP\u201d and \u201c80\u201d, respectively. For the \u201cVPC\u201d option, choose the VPC containing your instances. We can modify health checks but skip that for now. Register Targets \u00b6 Register targets to ensure that your load balancer routes traffic to this target group. Register targets. Select EC2 instance and select \"include as pending below\". Copy target name to use in listener Target Group name: apache-priv-subnets-elxsj It is sometimes necessary to stop an EC2 instance. After stopping the EC2 instance the target group will have to be re-registered. Add listener rule \u00b6 After the load balancer is created and its status becomes active, we are required to add traffic forward rules. Navigate to the Listeners tab and under the \u201cRules\u201d column, click on the \u201cView/Edit rules\u201d link. A new page appears here first; click on the \u201c+\u201d icon, then click on the \u201cInsert Rule\u201d link. The LB should be listed as fargate-LB . For the IF column, enter the host or domain name inside the field corresponding to the label \u201cis\u201d. For example: apache-priv-elxsj.fargate.722540083300.cloud.bayer.com For the THEN column, add the target group. LB listener rule Using Ocelot \u00b6 Copy the host from the listener rule and transfer to reverse proxy host apache-priv-elxsj.fargate.722540083300.cloud.bayer.com Check Apache Service in EC2 \u00b6 systemctl status httpd End \u00b6","title":"Apache on Private Subnet"},{"location":"aws/ec2_lb_priv/ec2_priv_sub/#apache-on-private-subnet","text":"Install ec2 using cloudformation. \u201cA load balancer receives requests and then transfers them to targets defined in a target group. We can create an Application Load balancer either using the AWS management console or AWS CLI. There are several routing options with AWS Application Load Balancer, e.g., Host-Based routing. In Host-based routing, incoming traffic is routed on the basis of the domain name or host name given in the Host Header. In this tutorial, we are going to create an Application Load balancer with Host-Based routing.\u201d","title":"Apache on Private Subnet"},{"location":"aws/ec2_lb_priv/ec2_priv_sub/#provision-ec2-webserver","text":"Save the instance name of the EC2. This is used for viewing the webserver and is required for managing the LB; however the CF template saves this in the parameters. Stack name: apache-priv-elxsj IntanceName: apache-priv-elxsj","title":"Provision EC2 Webserver"},{"location":"aws/ec2_lb_priv/ec2_priv_sub/#security-groups","text":"During CF provisioning, thee default VPC SG is attached to the EC2. In addition a SG for Ocelot traffic is required. Skip this step if Ocelot is not used. sg-0be806ad6114041cd - Create a new SG - Add Inbound rules - use the existing Ocelot SG in account. Create the Ocelot Security Group. The existing Ocelot SG in the VPC, is used to represent the Ocelot traffic IPs. Associate SG with EC2 Associate the new SG with the EC2. Notice our EC2 now has two SGs.","title":"Security Groups"},{"location":"aws/ec2_lb_priv/ec2_priv_sub/#load-balancer","text":"Normally a new LB would be provisioned. For simplicity , we use an existing LB. The fargate-infra service catalog product provisions an LB. fargate-infra creates infrastructure required by ECS Fargate services. The latest version includes an ALB, a Route53 alias record, an ACM certificate, and an ECS cluster. See https://devtools.bayer.com/docs/hosting/aws/fargate/","title":"Load Balancer"},{"location":"aws/ec2_lb_priv/ec2_priv_sub/#create-a-new-target-group","text":"Navigate to EC2 > Target groups > Create target group target group name: apache-priv-subnet Choose instance based target type. Target Group type For the \u201cProtocol\u201d and \u201cPort\u201d options, select \u201cHTTP\u201d and \u201c80\u201d, respectively. For the \u201cVPC\u201d option, choose the VPC containing your instances. We can modify health checks but skip that for now.","title":"Create a New Target Group"},{"location":"aws/ec2_lb_priv/ec2_priv_sub/#register-targets","text":"Register targets to ensure that your load balancer routes traffic to this target group. Register targets. Select EC2 instance and select \"include as pending below\". Copy target name to use in listener Target Group name: apache-priv-subnets-elxsj It is sometimes necessary to stop an EC2 instance. After stopping the EC2 instance the target group will have to be re-registered.","title":"Register Targets"},{"location":"aws/ec2_lb_priv/ec2_priv_sub/#add-listener-rule","text":"After the load balancer is created and its status becomes active, we are required to add traffic forward rules. Navigate to the Listeners tab and under the \u201cRules\u201d column, click on the \u201cView/Edit rules\u201d link. A new page appears here first; click on the \u201c+\u201d icon, then click on the \u201cInsert Rule\u201d link. The LB should be listed as fargate-LB . For the IF column, enter the host or domain name inside the field corresponding to the label \u201cis\u201d. For example: apache-priv-elxsj.fargate.722540083300.cloud.bayer.com For the THEN column, add the target group. LB listener rule","title":"Add listener rule"},{"location":"aws/ec2_lb_priv/ec2_priv_sub/#using-ocelot","text":"Copy the host from the listener rule and transfer to reverse proxy host apache-priv-elxsj.fargate.722540083300.cloud.bayer.com","title":"Using Ocelot"},{"location":"aws/ec2_lb_priv/ec2_priv_sub/#check-apache-service-in-ec2","text":"systemctl status httpd","title":"Check Apache Service in EC2"},{"location":"aws/ec2_lb_priv/ec2_priv_sub/#end","text":"","title":"End"},{"location":"aws/ec2_lb_priv/ec2_webserver/","text":"EC2 Webserver \u00b6 Add webserver to my EC2 Install apache as webserver. SSH to EC2 ssh ec2-user@ec2-3-86-153-196.compute-1.amazonaws.com -i webserver-v1.pem sudo su yum update -y yum install httpd -y Start apache service service httpd start Restart web server systemctl restart httpd systemctl status apache2 cd /var/www/html Check apache root directory Any files we add to /var/www/html will appear in public ip of EC2. nano index.html Add webserver page <html><body><h1> My EC2 WebServer 1 </h1></body></html> In the browser open public IP or public ip DNS - public IP 3.86.153.196 - public IP DNS ec2-3-86-153-196.compute-1.amazonaws.com","title":"EC2 Webserver"},{"location":"aws/ec2_lb_priv/ec2_webserver/#ec2-webserver","text":"Add webserver to my EC2 Install apache as webserver. SSH to EC2 ssh ec2-user@ec2-3-86-153-196.compute-1.amazonaws.com -i webserver-v1.pem sudo su yum update -y yum install httpd -y Start apache service service httpd start Restart web server systemctl restart httpd systemctl status apache2 cd /var/www/html Check apache root directory Any files we add to /var/www/html will appear in public ip of EC2. nano index.html Add webserver page <html><body><h1> My EC2 WebServer 1 </h1></body></html> In the browser open public IP or public ip DNS - public IP 3.86.153.196 - public IP DNS ec2-3-86-153-196.compute-1.amazonaws.com","title":"EC2 Webserver"},{"location":"aws/ec2_lb_priv/load_balancer_listeners/","text":"Hello again team. I finally have an application running in Fargate, but I'm struggling to actually connect to it. I have an api created in the non-prod Akana api portal (CAM360 Account and Contact API (TEST)) with a target endpoint of https://cam360-account-contact-api.customer360-dev.827059494196.cloud.bayer.com which I believe matches the directions here. I do see the DnsAliasCloudBayerCom in my cloudformation stack with a value of *.customer360-dev.827059494196.cloud.bayer.com in the EC2 console go to Load Balancers > pick your LB by name > Listeners > View/Edit rules and check that your host header cam360-account-contact-api.customer360-dev.827059494196.cloud.bayer.com you are using is listed and pointing at expected target group. Are you getting 404 or are the calls timing out after 1 or 2 min? Should the service catalog steps have created teh Host Header value? it is typically defined in the CFN template as a AWS::ElasticLoadBalancingV2::ListenerRule Additional Debugging if its a timeout after 1-2 min with 503, that typically indicates security group misconfiguration. if its instant, verify that the task is actually running in the ECS console (if the app is dying immediately on startup, you'd see a lot of stopped tasks for that service) first thing I would suggest is to make sure the target group health checks are passing. That at least would indicate the app is up and responding to pings. if the 504 persists after that, best to look at the links between your http request, Akana, the ELB, and the listener rule. The task is set to awsvpc mode which doesn't allow port publishing supposedly. awsvpc is the correct mode. doublecheck the port and URL that the Target group is trying to call. typically something like / /ping or /ping we had to add the app root url into the target ping url for example, when we call https://velocity-np.ag/valmon-sets-ar-association-api/ping, our target group hits /valmon-sets-ar-association-api/ping","title":"Load balancer listeners"},{"location":"aws/kubernetes/launch_eks_cluster/","text":"Launching an EKS Cluster \u00b6 Introduction \u00b6 Elastic Kubernetes Service (EKS) is a fully managed Kubernetes service from AWS. In this lab, you will work with the AWS command line interface and console, using command line utilities like eksctl and kubectl to launch an EKS cluster, provision a Kubernetes deployment and pod running instances of nginx, and create a LoadBalancer service to expose your application over the internet. Course files can be found here: https://github.com/ACloudGuru-Resources/Course_EKS-Basics Note that us-east-1 can experience capacity issues in certain Availability Zones. Since the AZ numbering (lettering) system differs between AWS accounts we cannot exclude that AZ from the lab steps. If you do experience an UnsupportedAvailabilityZoneException error regarding capacity in a particular zone, you can add the --zones switch to eksctl create cluster and specify three AZs which do not include the under-capacity zone. For example, eksctl create cluster --name dev --region us-east-1 --zones=us-east-1a,us-east-1b,us-east-1d --nodegroup-name standard-workers --node-type t3.medium --nodes 3 --nodes-min 1 --nodes-max 4 --managed Architecture diagram showing the project overview. Solution \u00b6 Log in to the live AWS environment using the credentials provided. Make sure you're in the N. Virginia (us-east-1) region throughout the lab. Create an IAM User with Admin Permissions \u00b6 Navigate to IAM > Users. Click Add user. Set the following values: User name: k8-admin Access type: Programmatic access Click Next: Permissions. Select Attach existing policies directly. Select AdministratorAccess. Click Next: Tags > Next: Review. Click Create user. Copy the access key ID and secret access key, and paste them into a text file, as we'll need them in the next step. key: AKIAQ42PSD27TLRJO7UD secret: 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 Launch an EC2 Instance and Configure the Command Line Tools \u00b6 Navigate to EC2 > Instances. Click Launch Instance. On the AMI page, select the Amazon Linux 2 AMI. Leave t2.micro selected, and click Next: Configure Instance Details. On the Configure Instance Details page: Network: Edit > Enable Auto-assign IP Subnet: Leave default Auto-assign Public IP: Enable Click Next: Add Storage: Default Add Tags > Next: Default Configure Security Group: Default Click Review and Launch, and then Launch. In the key pair dialog, select Create a new key pair Give it a Key pair name of \"mynvkp\". Click Download Key Pair, and then Launch Instances Click View Instances, and give it a few minutes to enter the running state Once the instance is fully created, check the checkbox next to it and click Connect at the top of the window In the Connect to your instance dialog, select EC2 Instance Connect (browser-based SSH connection Click Connect In the command line window, check the AWS CLI version: aws --version It should be an older version. Download v2: curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" Unzip the file: unzip awscliv2.zip See where the current AWS CLI is installed: which aws It should be /usr/bin/aws . Update it: sudo ./aws/install --bin-dir /usr/bin --install-dir /usr/bin/aws-cli --update Check the version of AWS CLI: aws --version It should now be updated. Configure the CLI: aws configure For AWS Access Key ID , paste in the access key ID you copied earlier. For AWS Secret Access Key , paste in the secret access key you copied earlier. For Default region name , enter us-east-1 . For Default output format , enter json . Download kubectl : curl -o kubectl https://amazon-eks.s3.us-west-2.amazonaws.com/1.16.8/2020-04-16/bin/linux/amd64/kubectl Apply execute permissions to the binary: chmod +x ./kubectl Copy the binary to a directory in your path: mkdir -p $HOME/bin && cp ./kubectl $HOME/bin/kubectl && export PATH=$PATH:$HOME/bin Ensure kubectl is installed: kubectl version --short --client Download eksctl: curl --silent --location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz -C /tmp Move the extracted binary to /usr/bin: sudo mv /tmp/eksctl /usr/bin Get the version of eksctl: eksctl version See the options with eksctl: eksctl help Provision an EKS Cluster \u00b6 Create a dev cluster , with managed worker nodes. Provision an EKS cluster with three worker nodes in us-east-1: eksctl create cluster --name dev --region us-east-1 --nodegroup-name standard-workers --node-type t3.medium --nodes 3 --nodes-min 1 --nodes-max 4 --managed CloudFormation provisions three stacks Create control plane eksctl-dev-cluster Create nodes eksctl-dev-nodegroup-standard-workers Test install worked Show K8s nodes kubectl get nodes [ec2-user@ip-10-0-0-62 ~]$ kubectl get nodes NAME STATUS ROLES AGE VERSION ip-192-168-13-197.ec2.internal Ready <none> 2m55s v1.23.13-eks-fb459a0 ip-192-168-26-5.ec2.internal Ready <none> 2m54s v1.23.13-eks-fb459a0 ip-192-168-53-44.ec2.internal Ready <none> 2m49s v1.23.13-eks-fb459a0 Check cluster and region eksctl get cluster [ec2-user@ip-10-0-0-62 ~]$ eksctl get cluster NAME REGION EKSCTL CREATED dev us-east-1 True If your EKS resources can't be deployed due to AWS capacity issues, delete your eksctl-dev-cluster CloudFormation stack and retry the command using the --zones parameter and suggested availability zones from the CREATE_FAILED message: AWS::EKS::Cluster/ControlPlane: CREATE_FAILED \u2013 \"Resource handler returned message: \\\"Cannot create cluster 'dev' because us-east-1e, the targeted availability zone, does not currently have sufficient capacity to support the cluster. Retry and choose from these availability zones: us-east-1a, us-east-1b, us-east-1c, us-east-1d, us-east-1f (Service: Eks, Status Code: 400, Request ID: 21e7e4aa-17a5-4c79-a911-bf86c4e93373)\\\" (RequestToken: 18b731b0-92a1-a779-9a69-f61e90b97ee1, HandlerErrorCode: InvalidRequest)\" In this example, the --zones parameter was added using the us-east-1a,us-east-1b,us-east-1c,us-east-1d,us-east-1f AZs from the message above: eksctl create cluster --name dev --region us-east-1 --zones us-east-1a,us-east-1b,us-east-1c,us-east-1d,us-east-1f --nodegroup-name standard-workers --node-type t3.medium --nodes 3 --nodes-min 1 --nodes-max 4 --managed It will take 10\u201315 minutes since it's provisioning the control plane and worker nodes, attaching the worker nodes to the control plane, and creating the VPC, security group, and Auto Scaling group. In the AWS Management Console, navigate to CloudFormation and take a look at what\u2019s going on there. Select the eksctl-dev-cluster stack (this is our control plane). Click Events, so you can see all the resources that are being created. We should then see another new stack being created \u2014 this one is our node group Once both stacks are complete, navigate to Elastic Kubernetes Service > Clusters. Click the listed cluster. If you see a Your current user or role does not have access to Kubernetes objects on this EKS cluster message just ignore it, as it won't impact the next steps of the activity. Click the Compute tab (under Configuration), and then click the listed node group. There, we'll see the Kubernetes version, instance type, status, etc. Click dev in the breadcrumb navigation link at the top of the screen. Click the Networking tab (under Configuration), where we'll see the VPC, subnets, etc. Click the Logging tab (under Configuration), where we'll see the control plane logging info. The control plane is abstracted \u2014 we can only interact with it using the command line utilities or the console. It\u2019s not an EC2 instance we can log into and start running Linux commands on. Navigate to EC2 > Instances, where you should see the instances have been launched. Close out of the existing CLI window, if you still have it open. Select the original t2.micro instance, and click Connect at the top of the window. In the Connect to your instance dialog, select EC2 Instance Connect (browser-based SSH connection). Click Connect. In the CLI, check the cluster: eksctl get cluster Enable kubectl to connect to our cluster: aws eks update-kubeconfig --name dev --region us-east-1 Create a Deployment on Your EKS Cluster \u00b6 Create Load Balancer service , the deployment, three pods and containers. Install Git: sudo yum install -y git Download the course files: git clone https://github.com/ACloudGuru-Resources/Course_EKS-Basics Change directory: cd Course_EKS-Basics Take a look at the deployment file: cat nginx-deployment.yaml Take a look at the service file: cat nginx-svc.yaml Create the load balancer before creating the pods and deployments. When K8s starts a container it creates some env variables pointing to all the services, which are running when the container is started. Any service a pod wants to access should be available, before creating the pod itself. Create the service: kubectl apply -f ./nginx-svc.yaml Check its status: kubectl get service [ec2-user@ip-10-0-0-62 Course_EKS-Basics]$ kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.100.0.1 <none> 443/TCP 29m nginx-svc LoadBalancer 10.100.10.175 a8798bfab5df44ff0a2144db7ff65198-1007282284.us-east-1.elb.amazonaws.com 80:31586/TCP 16s Copy the external DNS hostname of the load balancer a8798bfab5df44ff0a2144db7ff65198-1007282284.us-east-1.elb.amazonaws.com , and paste it into a text file, as we'll need it in a minute. Create the deployment: kubectl apply -f ./nginx-deployment.yaml Check its status: kubectl get deployment [ec2-user@ip-10-0-0-62 Course_EKS-Basics]$ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 3/3 3 3 15s View the pods: kubectl get pod [ec2-user@ip-10-0-0-62 Course_EKS-Basics]$ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-deployment-77d77c797d-97s8s 1/1 Running 0 39s nginx-deployment-77d77c797d-jhdwr 1/1 Running 0 40s nginx-deployment-77d77c797d-jzzhx 1/1 Running 0 39s View the ReplicaSets: kubectl get rs [ec2-user@ip-10-0-0-62 Course_EKS-Basics]$ kubectl get rs NAME DESIRED CURRENT READY AGE nginx-deployment-77d77c797d 3 3 3 60s View the nodes (three EC2 instances): kubectl get node [ec2-user@ip-10-0-0-62 Course_EKS-Basics]$ kubectl get node NAME STATUS ROLES AGE VERSION ip-192-168-13-197.ec2.internal Ready <none> 22m v1.23.13-eks-fb459a0 ip-192-168-26-5.ec2.internal Ready <none> 22m v1.23.13-eks-fb459a0 ip-192-168-53-44.ec2.internal Ready <none> 22m v1.23.13-eks-fb459a0 The LB exposes the application to the internet. Access the application using the load balancer, replacing with the IP you copied earlier (it might take a couple of minutes to update): curl \"<LOAD_BALANCER_DNS_HOSTNAME>\" [ec2-user@ip-10-0-0-62 Course_EKS-Basics]$ curl a8798bfab5df44ff0a2144db7ff65198-1007282284.us-east-1.elb.amazonaws.com <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } </style> </head> <body> <h1>Welcome to nginx!</h1> <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p> <p>For online documentation and support please refer to <a href=\"http://nginx.org/\">nginx.org</a>.<br/> Commercial support is available at <a href=\"http://nginx.com/\">nginx.com</a>.</p> <p><em>Thank you for using nginx.</em></p> </body> </html> The output should be the HTML for a default Nginx web page. In a new browser tab, navigate to the same IP, where we should then see the same Nginx web page. Test the High Availability Features of Your EKS Cluster \u00b6 In the AWS console, on the EC2 instances page, select the worker node instances. Click Actions > Instance State > Stop. In the dialog, click Yes, Stop. After a few minutes, we should see EKS launching new instances to keep our service running. In the CLI, check the status of our nodes: kubectl get node [ec2-user@ip-10-0-0-62 Course_EKS-Basics]$ kubectl get node NAME STATUS ROLES AGE VERSION ip-192-168-13-197.ec2.internal Ready <none> 27m v1.23.13-eks-fb459a0 ip-192-168-26-5.ec2.internal Ready <none> 27m v1.23.13-eks-fb459a0 ip-192-168-53-44.ec2.internal Ready <none> 27m v1.23.13-eks-fb459a0 All the nodes should be down (i.e., display a NotReady status). Check the pods: kubectl get pod [ec2-user@ip-10-0-0-62 Course_EKS-Basics]$ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-deployment-77d77c797d-97s8s 1/1 Terminating 0 8m48s nginx-deployment-77d77c797d-gdk8j 0/1 Pending 0 57s nginx-deployment-77d77c797d-jhdwr 1/1 Running 0 8m49s nginx-deployment-77d77c797d-jzzhx 1/1 Running 0 8m48s We'll see a few different statuses \u2014 Terminating, Running, and Pending \u2014 because, as the instances shut down, EKS is trying to restart the pods. Check the nodes again: kubectl get node We should see a new node, which we can identify by its age. Wait a few minutes, and then check the nodes again: kubectl get node We should have one in a Ready state. Check the pods again: kubectl get pod We should see a couple pods are now running as well. Check the service status: kubectl get service [ec2-user@ip-10-0-0-62 Course_EKS-Basics]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.100.0.1 <none> 443/TCP 40m nginx-svc LoadBalancer 10.100.10.175 a8798bfab5df44ff0a2144db7ff65198-1007282284.us-east-1.elb.amazonaws.com 80:31586/TCP 11m Copy the external DNS Hostname listed in the output. Access the application using the load balancer, replacing with the DNS Hostname you just copied: curl \" \" We should see the Nginx web page HTML again. (If you don't, wait a few more minutes.) In a new browser tab, navigate to the same IP, where we should again see the Nginx web page. In the CLI, delete all : eksctl delete cluster dev uses the cluster name , for example the syntax eksctl delete cluster <cluster_name> Delete EC2 instance and IAM User from console. Conclusion","title":"EKS Cluster"},{"location":"aws/kubernetes/launch_eks_cluster/#launching-an-eks-cluster","text":"","title":"Launching an EKS Cluster"},{"location":"aws/kubernetes/launch_eks_cluster/#introduction","text":"Elastic Kubernetes Service (EKS) is a fully managed Kubernetes service from AWS. In this lab, you will work with the AWS command line interface and console, using command line utilities like eksctl and kubectl to launch an EKS cluster, provision a Kubernetes deployment and pod running instances of nginx, and create a LoadBalancer service to expose your application over the internet. Course files can be found here: https://github.com/ACloudGuru-Resources/Course_EKS-Basics Note that us-east-1 can experience capacity issues in certain Availability Zones. Since the AZ numbering (lettering) system differs between AWS accounts we cannot exclude that AZ from the lab steps. If you do experience an UnsupportedAvailabilityZoneException error regarding capacity in a particular zone, you can add the --zones switch to eksctl create cluster and specify three AZs which do not include the under-capacity zone. For example, eksctl create cluster --name dev --region us-east-1 --zones=us-east-1a,us-east-1b,us-east-1d --nodegroup-name standard-workers --node-type t3.medium --nodes 3 --nodes-min 1 --nodes-max 4 --managed Architecture diagram showing the project overview.","title":"Introduction"},{"location":"aws/kubernetes/launch_eks_cluster/#solution","text":"Log in to the live AWS environment using the credentials provided. Make sure you're in the N. Virginia (us-east-1) region throughout the lab.","title":"Solution"},{"location":"aws/kubernetes/launch_eks_cluster/#create-an-iam-user-with-admin-permissions","text":"Navigate to IAM > Users. Click Add user. Set the following values: User name: k8-admin Access type: Programmatic access Click Next: Permissions. Select Attach existing policies directly. Select AdministratorAccess. Click Next: Tags > Next: Review. Click Create user. Copy the access key ID and secret access key, and paste them into a text file, as we'll need them in the next step. key: AKIAQ42PSD27TLRJO7UD secret: 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","title":"Create an IAM User with Admin Permissions"},{"location":"aws/kubernetes/launch_eks_cluster/#launch-an-ec2-instance-and-configure-the-command-line-tools","text":"Navigate to EC2 > Instances. Click Launch Instance. On the AMI page, select the Amazon Linux 2 AMI. Leave t2.micro selected, and click Next: Configure Instance Details. On the Configure Instance Details page: Network: Edit > Enable Auto-assign IP Subnet: Leave default Auto-assign Public IP: Enable Click Next: Add Storage: Default Add Tags > Next: Default Configure Security Group: Default Click Review and Launch, and then Launch. In the key pair dialog, select Create a new key pair Give it a Key pair name of \"mynvkp\". Click Download Key Pair, and then Launch Instances Click View Instances, and give it a few minutes to enter the running state Once the instance is fully created, check the checkbox next to it and click Connect at the top of the window In the Connect to your instance dialog, select EC2 Instance Connect (browser-based SSH connection Click Connect In the command line window, check the AWS CLI version: aws --version It should be an older version. Download v2: curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" Unzip the file: unzip awscliv2.zip See where the current AWS CLI is installed: which aws It should be /usr/bin/aws . Update it: sudo ./aws/install --bin-dir /usr/bin --install-dir /usr/bin/aws-cli --update Check the version of AWS CLI: aws --version It should now be updated. Configure the CLI: aws configure For AWS Access Key ID , paste in the access key ID you copied earlier. For AWS Secret Access Key , paste in the secret access key you copied earlier. For Default region name , enter us-east-1 . For Default output format , enter json . Download kubectl : curl -o kubectl https://amazon-eks.s3.us-west-2.amazonaws.com/1.16.8/2020-04-16/bin/linux/amd64/kubectl Apply execute permissions to the binary: chmod +x ./kubectl Copy the binary to a directory in your path: mkdir -p $HOME/bin && cp ./kubectl $HOME/bin/kubectl && export PATH=$PATH:$HOME/bin Ensure kubectl is installed: kubectl version --short --client Download eksctl: curl --silent --location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz -C /tmp Move the extracted binary to /usr/bin: sudo mv /tmp/eksctl /usr/bin Get the version of eksctl: eksctl version See the options with eksctl: eksctl help","title":"Launch an EC2 Instance and Configure the Command Line Tools"},{"location":"aws/kubernetes/launch_eks_cluster/#provision-an-eks-cluster","text":"Create a dev cluster , with managed worker nodes. Provision an EKS cluster with three worker nodes in us-east-1: eksctl create cluster --name dev --region us-east-1 --nodegroup-name standard-workers --node-type t3.medium --nodes 3 --nodes-min 1 --nodes-max 4 --managed CloudFormation provisions three stacks Create control plane eksctl-dev-cluster Create nodes eksctl-dev-nodegroup-standard-workers Test install worked Show K8s nodes kubectl get nodes [ec2-user@ip-10-0-0-62 ~]$ kubectl get nodes NAME STATUS ROLES AGE VERSION ip-192-168-13-197.ec2.internal Ready <none> 2m55s v1.23.13-eks-fb459a0 ip-192-168-26-5.ec2.internal Ready <none> 2m54s v1.23.13-eks-fb459a0 ip-192-168-53-44.ec2.internal Ready <none> 2m49s v1.23.13-eks-fb459a0 Check cluster and region eksctl get cluster [ec2-user@ip-10-0-0-62 ~]$ eksctl get cluster NAME REGION EKSCTL CREATED dev us-east-1 True If your EKS resources can't be deployed due to AWS capacity issues, delete your eksctl-dev-cluster CloudFormation stack and retry the command using the --zones parameter and suggested availability zones from the CREATE_FAILED message: AWS::EKS::Cluster/ControlPlane: CREATE_FAILED \u2013 \"Resource handler returned message: \\\"Cannot create cluster 'dev' because us-east-1e, the targeted availability zone, does not currently have sufficient capacity to support the cluster. Retry and choose from these availability zones: us-east-1a, us-east-1b, us-east-1c, us-east-1d, us-east-1f (Service: Eks, Status Code: 400, Request ID: 21e7e4aa-17a5-4c79-a911-bf86c4e93373)\\\" (RequestToken: 18b731b0-92a1-a779-9a69-f61e90b97ee1, HandlerErrorCode: InvalidRequest)\" In this example, the --zones parameter was added using the us-east-1a,us-east-1b,us-east-1c,us-east-1d,us-east-1f AZs from the message above: eksctl create cluster --name dev --region us-east-1 --zones us-east-1a,us-east-1b,us-east-1c,us-east-1d,us-east-1f --nodegroup-name standard-workers --node-type t3.medium --nodes 3 --nodes-min 1 --nodes-max 4 --managed It will take 10\u201315 minutes since it's provisioning the control plane and worker nodes, attaching the worker nodes to the control plane, and creating the VPC, security group, and Auto Scaling group. In the AWS Management Console, navigate to CloudFormation and take a look at what\u2019s going on there. Select the eksctl-dev-cluster stack (this is our control plane). Click Events, so you can see all the resources that are being created. We should then see another new stack being created \u2014 this one is our node group Once both stacks are complete, navigate to Elastic Kubernetes Service > Clusters. Click the listed cluster. If you see a Your current user or role does not have access to Kubernetes objects on this EKS cluster message just ignore it, as it won't impact the next steps of the activity. Click the Compute tab (under Configuration), and then click the listed node group. There, we'll see the Kubernetes version, instance type, status, etc. Click dev in the breadcrumb navigation link at the top of the screen. Click the Networking tab (under Configuration), where we'll see the VPC, subnets, etc. Click the Logging tab (under Configuration), where we'll see the control plane logging info. The control plane is abstracted \u2014 we can only interact with it using the command line utilities or the console. It\u2019s not an EC2 instance we can log into and start running Linux commands on. Navigate to EC2 > Instances, where you should see the instances have been launched. Close out of the existing CLI window, if you still have it open. Select the original t2.micro instance, and click Connect at the top of the window. In the Connect to your instance dialog, select EC2 Instance Connect (browser-based SSH connection). Click Connect. In the CLI, check the cluster: eksctl get cluster Enable kubectl to connect to our cluster: aws eks update-kubeconfig --name dev --region us-east-1","title":"Provision an EKS Cluster"},{"location":"aws/kubernetes/launch_eks_cluster/#create-a-deployment-on-your-eks-cluster","text":"Create Load Balancer service , the deployment, three pods and containers. Install Git: sudo yum install -y git Download the course files: git clone https://github.com/ACloudGuru-Resources/Course_EKS-Basics Change directory: cd Course_EKS-Basics Take a look at the deployment file: cat nginx-deployment.yaml Take a look at the service file: cat nginx-svc.yaml Create the load balancer before creating the pods and deployments. When K8s starts a container it creates some env variables pointing to all the services, which are running when the container is started. Any service a pod wants to access should be available, before creating the pod itself. Create the service: kubectl apply -f ./nginx-svc.yaml Check its status: kubectl get service [ec2-user@ip-10-0-0-62 Course_EKS-Basics]$ kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.100.0.1 <none> 443/TCP 29m nginx-svc LoadBalancer 10.100.10.175 a8798bfab5df44ff0a2144db7ff65198-1007282284.us-east-1.elb.amazonaws.com 80:31586/TCP 16s Copy the external DNS hostname of the load balancer a8798bfab5df44ff0a2144db7ff65198-1007282284.us-east-1.elb.amazonaws.com , and paste it into a text file, as we'll need it in a minute. Create the deployment: kubectl apply -f ./nginx-deployment.yaml Check its status: kubectl get deployment [ec2-user@ip-10-0-0-62 Course_EKS-Basics]$ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 3/3 3 3 15s View the pods: kubectl get pod [ec2-user@ip-10-0-0-62 Course_EKS-Basics]$ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-deployment-77d77c797d-97s8s 1/1 Running 0 39s nginx-deployment-77d77c797d-jhdwr 1/1 Running 0 40s nginx-deployment-77d77c797d-jzzhx 1/1 Running 0 39s View the ReplicaSets: kubectl get rs [ec2-user@ip-10-0-0-62 Course_EKS-Basics]$ kubectl get rs NAME DESIRED CURRENT READY AGE nginx-deployment-77d77c797d 3 3 3 60s View the nodes (three EC2 instances): kubectl get node [ec2-user@ip-10-0-0-62 Course_EKS-Basics]$ kubectl get node NAME STATUS ROLES AGE VERSION ip-192-168-13-197.ec2.internal Ready <none> 22m v1.23.13-eks-fb459a0 ip-192-168-26-5.ec2.internal Ready <none> 22m v1.23.13-eks-fb459a0 ip-192-168-53-44.ec2.internal Ready <none> 22m v1.23.13-eks-fb459a0 The LB exposes the application to the internet. Access the application using the load balancer, replacing with the IP you copied earlier (it might take a couple of minutes to update): curl \"<LOAD_BALANCER_DNS_HOSTNAME>\" [ec2-user@ip-10-0-0-62 Course_EKS-Basics]$ curl a8798bfab5df44ff0a2144db7ff65198-1007282284.us-east-1.elb.amazonaws.com <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } </style> </head> <body> <h1>Welcome to nginx!</h1> <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p> <p>For online documentation and support please refer to <a href=\"http://nginx.org/\">nginx.org</a>.<br/> Commercial support is available at <a href=\"http://nginx.com/\">nginx.com</a>.</p> <p><em>Thank you for using nginx.</em></p> </body> </html> The output should be the HTML for a default Nginx web page. In a new browser tab, navigate to the same IP, where we should then see the same Nginx web page.","title":"Create a Deployment on Your EKS Cluster"},{"location":"aws/kubernetes/launch_eks_cluster/#test-the-high-availability-features-of-your-eks-cluster","text":"In the AWS console, on the EC2 instances page, select the worker node instances. Click Actions > Instance State > Stop. In the dialog, click Yes, Stop. After a few minutes, we should see EKS launching new instances to keep our service running. In the CLI, check the status of our nodes: kubectl get node [ec2-user@ip-10-0-0-62 Course_EKS-Basics]$ kubectl get node NAME STATUS ROLES AGE VERSION ip-192-168-13-197.ec2.internal Ready <none> 27m v1.23.13-eks-fb459a0 ip-192-168-26-5.ec2.internal Ready <none> 27m v1.23.13-eks-fb459a0 ip-192-168-53-44.ec2.internal Ready <none> 27m v1.23.13-eks-fb459a0 All the nodes should be down (i.e., display a NotReady status). Check the pods: kubectl get pod [ec2-user@ip-10-0-0-62 Course_EKS-Basics]$ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-deployment-77d77c797d-97s8s 1/1 Terminating 0 8m48s nginx-deployment-77d77c797d-gdk8j 0/1 Pending 0 57s nginx-deployment-77d77c797d-jhdwr 1/1 Running 0 8m49s nginx-deployment-77d77c797d-jzzhx 1/1 Running 0 8m48s We'll see a few different statuses \u2014 Terminating, Running, and Pending \u2014 because, as the instances shut down, EKS is trying to restart the pods. Check the nodes again: kubectl get node We should see a new node, which we can identify by its age. Wait a few minutes, and then check the nodes again: kubectl get node We should have one in a Ready state. Check the pods again: kubectl get pod We should see a couple pods are now running as well. Check the service status: kubectl get service [ec2-user@ip-10-0-0-62 Course_EKS-Basics]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.100.0.1 <none> 443/TCP 40m nginx-svc LoadBalancer 10.100.10.175 a8798bfab5df44ff0a2144db7ff65198-1007282284.us-east-1.elb.amazonaws.com 80:31586/TCP 11m Copy the external DNS Hostname listed in the output. Access the application using the load balancer, replacing with the DNS Hostname you just copied: curl \" \" We should see the Nginx web page HTML again. (If you don't, wait a few more minutes.) In a new browser tab, navigate to the same IP, where we should again see the Nginx web page. In the CLI, delete all : eksctl delete cluster dev uses the cluster name , for example the syntax eksctl delete cluster <cluster_name> Delete EC2 instance and IAM User from console. Conclusion","title":"Test the High Availability Features of Your EKS Cluster"},{"location":"aws/kubernetes/p2_alb_controller/","text":"Install the LB Controller Add On \u00b6 The AWS Load Balancer Controller manages AWS Elastic Load Balancers for a Kubernetes cluster. The controller provisions the following resources: An AWS Application Load Balancer (ALB) when you create a Kubernetes Ingress. An AWS Network Load Balancer (NLB) when you create a Kubernetes service of type LoadBalancer. In the past, the Kubernetes network load balancer was used for instance targets, but the AWS Load balancer Controller was used for IP targets. With the AWS Load Balancer Controller version 2.3.0 or later, you can create NLBs using either target type. For more information about NLB target types, see Target type in the User Guide for Network Load Balancers. Installing AWS LB Controller add on Creating an IAM OIDC provider for your cluster \u00b6 Version 2.11.3 or later or 1.27.93 or later of the AWS CLI installed and configured on your device or AWS CloudShell. You can check your current version with aws --version | cut -d / -f2 | cut -d ' ' -f1 . The kubectl command line tool is installed on your device or AWS CloudShell. The version can be the same as or up to one minor version earlier or later than the Kubernetes version of your cluster. For example, if your cluster version is 1.24, you can use kubectl version 1.23, 1.24, or 1.25 with it. To install or upgrade kubectl , see Installing or updating kubectl . An existing kubectl config file that contains your cluster configuration. To create a kubectl config file, see Creating or updating a kubeconfig file for an Amazon EKS cluster . You can create an IAM OIDC provider for your cluster using eksctl or the AWS Management Console. eksctl \u00b6 Version 0.135.0 or later of the eksctl command line tool installed on your device or AWS CloudShell. To install or update eksctl , see Installing or updating eksctl . install eksctl \u00b6 Open cloudshell Determine whether you already have eksctl installed on your device. eksctl version If you have eksctl installed in the path of your device, the example output is as follows. If you want to update the version that you currently have installed with a later version, complete the next step, making sure to install the new version in the same location that your current version is in. 0.135.0 If you receive no output, then you either don't have eksctl installed, or it's not installed in a location that's in your device's path. Download and extract the latest release of eksctl with the following command. curl --silent --location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz -C /tmp Move the extracted binary to /usr/local/bin. sudo mv /tmp/eksctl /usr/local/bin Test that your installation was successful with the following command. eksctl version To create an IAM OIDC identity provider for your cluster with eksctl Determine whether you have an existing IAM OIDC provider for your cluster. Retrieve your cluster's OIDC provider ID and store it in a variable. oidc_id=$(aws eks describe-cluster --name cluster-in-existing-vpc --query \"cluster.identity.oidc.issuer\" --output text | cut -d '/' -f 5) Determine whether an IAM OIDC provider with your cluster's ID is already in your account. aws iam list-open-id-connect-providers | grep $oidc_id | cut -d \"/\" -f4 If output is returned, then you already have an IAM OIDC provider for your cluster and you can skip the next step. If no output is returned, then you must create an IAM OIDC provider for your cluster. Create an IAM OIDC identity provider for your cluster with the following command. Replace my-cluster with your own value. eksctl utils associate-iam-oidc-provider --cluster cluster-in-existing-vpc --approve Next step \u00b6 Configuring a Kubernetes service account to assume an IAM role To associate an IAM role with a Kubernetes service account \u00b6 If you want to associate an existing IAM policy to your IAM role, skip to the next step. Create an IAM policy. You can create your own policy, or copy an AWS managed policy that already grants some of the permissions that you need and customize it to your specific requirements. For more information, see Creating IAM policies in the IAM User Guide. a. Create a file that includes the permissions for the AWS services that you want your pods to access. For a list of all actions for all AWS services, see the Service Authorization Reference. You can run the following command to create an example policy file that allows read-only access to an Amazon S3 bucket. You can optionally store configuration information or a bootstrap script in this bucket, and the containers in your pod can read the file from the bucket and load it into your application. If you want to create this example policy, copy the following contents to your device. Replace my-pod-secrets-bucket with your bucket name and run the command.","title":"Install the LB Controller Add On"},{"location":"aws/kubernetes/p2_alb_controller/#install-the-lb-controller-add-on","text":"The AWS Load Balancer Controller manages AWS Elastic Load Balancers for a Kubernetes cluster. The controller provisions the following resources: An AWS Application Load Balancer (ALB) when you create a Kubernetes Ingress. An AWS Network Load Balancer (NLB) when you create a Kubernetes service of type LoadBalancer. In the past, the Kubernetes network load balancer was used for instance targets, but the AWS Load balancer Controller was used for IP targets. With the AWS Load Balancer Controller version 2.3.0 or later, you can create NLBs using either target type. For more information about NLB target types, see Target type in the User Guide for Network Load Balancers. Installing AWS LB Controller add on","title":"Install the LB Controller Add On"},{"location":"aws/kubernetes/p2_alb_controller/#creating-an-iam-oidc-provider-for-your-cluster","text":"Version 2.11.3 or later or 1.27.93 or later of the AWS CLI installed and configured on your device or AWS CloudShell. You can check your current version with aws --version | cut -d / -f2 | cut -d ' ' -f1 . The kubectl command line tool is installed on your device or AWS CloudShell. The version can be the same as or up to one minor version earlier or later than the Kubernetes version of your cluster. For example, if your cluster version is 1.24, you can use kubectl version 1.23, 1.24, or 1.25 with it. To install or upgrade kubectl , see Installing or updating kubectl . An existing kubectl config file that contains your cluster configuration. To create a kubectl config file, see Creating or updating a kubeconfig file for an Amazon EKS cluster . You can create an IAM OIDC provider for your cluster using eksctl or the AWS Management Console.","title":"Creating an IAM OIDC provider for your cluster"},{"location":"aws/kubernetes/p2_alb_controller/#eksctl","text":"Version 0.135.0 or later of the eksctl command line tool installed on your device or AWS CloudShell. To install or update eksctl , see Installing or updating eksctl .","title":"eksctl"},{"location":"aws/kubernetes/p2_alb_controller/#install-eksctl","text":"Open cloudshell Determine whether you already have eksctl installed on your device. eksctl version If you have eksctl installed in the path of your device, the example output is as follows. If you want to update the version that you currently have installed with a later version, complete the next step, making sure to install the new version in the same location that your current version is in. 0.135.0 If you receive no output, then you either don't have eksctl installed, or it's not installed in a location that's in your device's path. Download and extract the latest release of eksctl with the following command. curl --silent --location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz -C /tmp Move the extracted binary to /usr/local/bin. sudo mv /tmp/eksctl /usr/local/bin Test that your installation was successful with the following command. eksctl version To create an IAM OIDC identity provider for your cluster with eksctl Determine whether you have an existing IAM OIDC provider for your cluster. Retrieve your cluster's OIDC provider ID and store it in a variable. oidc_id=$(aws eks describe-cluster --name cluster-in-existing-vpc --query \"cluster.identity.oidc.issuer\" --output text | cut -d '/' -f 5) Determine whether an IAM OIDC provider with your cluster's ID is already in your account. aws iam list-open-id-connect-providers | grep $oidc_id | cut -d \"/\" -f4 If output is returned, then you already have an IAM OIDC provider for your cluster and you can skip the next step. If no output is returned, then you must create an IAM OIDC provider for your cluster. Create an IAM OIDC identity provider for your cluster with the following command. Replace my-cluster with your own value. eksctl utils associate-iam-oidc-provider --cluster cluster-in-existing-vpc --approve","title":"install eksctl"},{"location":"aws/kubernetes/p2_alb_controller/#next-step","text":"Configuring a Kubernetes service account to assume an IAM role","title":"Next step"},{"location":"aws/kubernetes/p2_alb_controller/#to-associate-an-iam-role-with-a-kubernetes-service-account","text":"If you want to associate an existing IAM policy to your IAM role, skip to the next step. Create an IAM policy. You can create your own policy, or copy an AWS managed policy that already grants some of the permissions that you need and customize it to your specific requirements. For more information, see Creating IAM policies in the IAM User Guide. a. Create a file that includes the permissions for the AWS services that you want your pods to access. For a list of all actions for all AWS services, see the Service Authorization Reference. You can run the following command to create an example policy file that allows read-only access to an Amazon S3 bucket. You can optionally store configuration information or a bootstrap script in this bucket, and the containers in your pod can read the file from the bucket and load it into your application. If you want to create this example policy, copy the following contents to your device. Replace my-pod-secrets-bucket with your bucket name and run the command.","title":"To associate an IAM role with a Kubernetes service account"},{"location":"aws/kubernetes/eks/p1_eksctl/","text":"Creating and managing kubernetes clusters \u00b6 Tutorial https://eksctl.io/usage/creating-and-managing-clusters/ Creating a cluster \u00b6 Create a simple cluster with the following command: eksctl create cluster That will create an EKS cluster in your default region (as specified by your AWS CLI configuration) with one managed nodegroup containing two m5.large nodes. eksctl create cluster -f cluster.yaml If you needed to use an existing VPC, you can use a config file like this: apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : cluster-in-existing-vpc region : us-east-1 vpc : subnets : private : us-east-1 : { id : subnet-0c97f80b8c1f38da9 } us-east-1 : { id : subnet-0732990f5244e5931 } nodeGroups : - name : ng-1-workers labels : { role : workers } instanceType : m5.xlarge desiredCapacity : 10 privateNetworking : true - name : ng-2-builders labels : { role : builders } instanceType : m5.2xlarge desiredCapacity : 2 privateNetworking : true iam : withAddonPolicies : imageBuilder : true Note The cluster name or nodegroup name can contain only alphanumeric characters (case-sensitive) and hyphens. It must start with an alphabetic character and can't be longer than 128 characters otherwise you will get a validation error. More information can be found here To delete this cluster, run: eksctl delete cluster -f cluster.yml Note Without the --wait flag, this will only issue a delete operation to the cluster's CloudFormation stack and won't wait for its deletion. In some cases, AWS resources using the cluster or its VPC may cause cluster deletion to fail. To ensure any deletion errors are propagated in eksctl delete cluster, the --wait flag must be used. If your delete fails or you forget the wait flag, you may have to go to the CloudFormation GUI and delete the eks stacks from there. Note When deleting a cluster with nodegroups, in some scenarios, Pod Disruption Budget (PDB) policies can prevent nodes from being removed successfully from nodepools. E.g. a cluster with aws-ebs-csi-driver installed, by default, spins off two pods while having a PDB policy that allows at most one pod to be unavailable at a time. This will make the other pod unevictable during deletion. To successfully delete the cluster, one should use disable-nodegroup-eviction flag. This will bypass checking PDB policies. eksctl delete cluster -f cluster.yaml --disable-nodegroup-eviction","title":"Creating and managing  kubernetes clusters"},{"location":"aws/kubernetes/eks/p1_eksctl/#creating-and-managing-kubernetes-clusters","text":"Tutorial https://eksctl.io/usage/creating-and-managing-clusters/","title":"Creating and managing  kubernetes clusters"},{"location":"aws/kubernetes/eks/p1_eksctl/#creating-a-cluster","text":"Create a simple cluster with the following command: eksctl create cluster That will create an EKS cluster in your default region (as specified by your AWS CLI configuration) with one managed nodegroup containing two m5.large nodes. eksctl create cluster -f cluster.yaml If you needed to use an existing VPC, you can use a config file like this: apiVersion : eksctl.io/v1alpha5 kind : ClusterConfig metadata : name : cluster-in-existing-vpc region : us-east-1 vpc : subnets : private : us-east-1 : { id : subnet-0c97f80b8c1f38da9 } us-east-1 : { id : subnet-0732990f5244e5931 } nodeGroups : - name : ng-1-workers labels : { role : workers } instanceType : m5.xlarge desiredCapacity : 10 privateNetworking : true - name : ng-2-builders labels : { role : builders } instanceType : m5.2xlarge desiredCapacity : 2 privateNetworking : true iam : withAddonPolicies : imageBuilder : true Note The cluster name or nodegroup name can contain only alphanumeric characters (case-sensitive) and hyphens. It must start with an alphabetic character and can't be longer than 128 characters otherwise you will get a validation error. More information can be found here To delete this cluster, run: eksctl delete cluster -f cluster.yml Note Without the --wait flag, this will only issue a delete operation to the cluster's CloudFormation stack and won't wait for its deletion. In some cases, AWS resources using the cluster or its VPC may cause cluster deletion to fail. To ensure any deletion errors are propagated in eksctl delete cluster, the --wait flag must be used. If your delete fails or you forget the wait flag, you may have to go to the CloudFormation GUI and delete the eks stacks from there. Note When deleting a cluster with nodegroups, in some scenarios, Pod Disruption Budget (PDB) policies can prevent nodes from being removed successfully from nodepools. E.g. a cluster with aws-ebs-csi-driver installed, by default, spins off two pods while having a PDB policy that allows at most one pod to be unavailable at a time. This will make the other pod unevictable during deletion. To successfully delete the cluster, one should use disable-nodegroup-eviction flag. This will bypass checking PDB policies. eksctl delete cluster -f cluster.yaml --disable-nodegroup-eviction","title":"Creating a cluster"},{"location":"aws/lambda/create_lambda_function/","text":"Create a Lambda Function \u00b6 Scales Automatically Serverless Lambda has a 15 minute timeout Use Cases \u00b6 Real time file processing Upload CSV to S3 > trigger Lambda to read file > Store data in DynamoDB Lambda Response Code \u00b6 Copy this into lambda code, replacing the sample code from AWS: import json def lambda_handler ( event , context ): message = 'Hello {} {} ! Keep being awesome!' . format ( event [ 'first_name' ], event [ 'last_name' ]) #print to CloudWatch logs print ( message ) return { 'message' : message } Configure a test event as my_test , replacing with your name. { \"first_name\" : \"Mike\" , \"last_name\" : \"Madsen\" } After invoking the test code. Check the results in CloudWatch. Navigate to CloudWatch > Log groups > /aws/lambda/myfunction and the print statement will be there. End \u00b6","title":"Create a Lambda Function"},{"location":"aws/lambda/create_lambda_function/#create-a-lambda-function","text":"Scales Automatically Serverless Lambda has a 15 minute timeout","title":"Create a Lambda Function"},{"location":"aws/lambda/create_lambda_function/#use-cases","text":"Real time file processing Upload CSV to S3 > trigger Lambda to read file > Store data in DynamoDB","title":"Use Cases"},{"location":"aws/lambda/create_lambda_function/#lambda-response-code","text":"Copy this into lambda code, replacing the sample code from AWS: import json def lambda_handler ( event , context ): message = 'Hello {} {} ! Keep being awesome!' . format ( event [ 'first_name' ], event [ 'last_name' ]) #print to CloudWatch logs print ( message ) return { 'message' : message } Configure a test event as my_test , replacing with your name. { \"first_name\" : \"Mike\" , \"last_name\" : \"Madsen\" } After invoking the test code. Check the results in CloudWatch. Navigate to CloudWatch > Log groups > /aws/lambda/myfunction and the print statement will be there.","title":"Lambda Response Code"},{"location":"aws/lambda/create_lambda_function/#end","text":"","title":"End"},{"location":"aws/lb_route53/direct_connect/","text":"Direct Connect \u00b6 Direct Connect is a dedicated physical network connection from on-premises data center to AWS. Dedicated physical network connection Connects on-premises data center to AWS Data travels on private network Supports hybrid (public and private clouds) cloud architectures","title":"Direct Connect"},{"location":"aws/lb_route53/direct_connect/#direct-connect","text":"Direct Connect is a dedicated physical network connection from on-premises data center to AWS. Dedicated physical network connection Connects on-premises data center to AWS Data travels on private network Supports hybrid (public and private clouds) cloud architectures","title":"Direct Connect"},{"location":"aws/lb_route53/internet_gateway/","text":"Internet Gateway \u00b6 An internet gateway enables resources inside your VPC to reach the internet, as long as route tables and IP addresses are correctly configured in your environment. https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Internet_Gateway.html Use Case Example \u00b6 A company has provisioned an EC2 instance as a web server. The web application on the server is running within a subnet within a VPC. For some reason, the application is unable to access the internet. Which component is missing? An internet gateway allows public traffic to the internet from the VPC.","title":"Internet Gateway"},{"location":"aws/lb_route53/internet_gateway/#internet-gateway","text":"An internet gateway enables resources inside your VPC to reach the internet, as long as route tables and IP addresses are correctly configured in your environment. https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Internet_Gateway.html","title":"Internet Gateway"},{"location":"aws/lb_route53/internet_gateway/#use-case-example","text":"A company has provisioned an EC2 instance as a web server. The web application on the server is running within a subnet within a VPC. For some reason, the application is unable to access the internet. Which component is missing? An internet gateway allows public traffic to the internet from the VPC.","title":"Use Case Example"},{"location":"aws/lb_route53/lb_target_group/","text":"Register Target \u00b6 Elastic Load Balancing automatically distributes your incoming traffic across multiple targets, such as EC2 instances, containers, and IP addresses, in one or more Availability Zones. It monitors the health of its registered targets, and routes traffic only to the healthy targets. Elastic Load Balancing scales your load balancer as your incoming traffic changes over time. It can automatically scale to the vast majority of workloads. Application Load Balancer components \u00b6 A load balancer serves as the single point of contact for clients. The load balancer distributes incoming application traffic across multiple targets, such as EC2 instances, in multiple Availability Zones. This increases the availability of your application. You add one or more listeners to your load balancer. A listener checks for connection requests from clients, using the protocol and port that you configure. The rules that you define for a listener determine how the load balancer routes requests to its registered targets. Each rule consists of a priority, one or more actions, and one or more conditions. When the conditions for a rule are met, then its actions are performed. You must define a default rule for each listener, and you can optionally define additional rules. Each target group routes requests to one or more registered targets, such as EC2 instances, using the protocol and port number that you specify. You can register a target with multiple target groups. You can configure health checks on a per target group basis. Health checks are performed on all targets registered to a target group that is specified in a listener rule for your load balancer. The following diagram illustrates the basic components. Notice that each listener contains a default rule, and one listener contains another rule that routes requests to a different target group. One target is registered with two target groups.","title":"Register Target"},{"location":"aws/lb_route53/lb_target_group/#register-target","text":"Elastic Load Balancing automatically distributes your incoming traffic across multiple targets, such as EC2 instances, containers, and IP addresses, in one or more Availability Zones. It monitors the health of its registered targets, and routes traffic only to the healthy targets. Elastic Load Balancing scales your load balancer as your incoming traffic changes over time. It can automatically scale to the vast majority of workloads.","title":"Register Target"},{"location":"aws/lb_route53/lb_target_group/#application-load-balancer-components","text":"A load balancer serves as the single point of contact for clients. The load balancer distributes incoming application traffic across multiple targets, such as EC2 instances, in multiple Availability Zones. This increases the availability of your application. You add one or more listeners to your load balancer. A listener checks for connection requests from clients, using the protocol and port that you configure. The rules that you define for a listener determine how the load balancer routes requests to its registered targets. Each rule consists of a priority, one or more actions, and one or more conditions. When the conditions for a rule are met, then its actions are performed. You must define a default rule for each listener, and you can optionally define additional rules. Each target group routes requests to one or more registered targets, such as EC2 instances, using the protocol and port number that you specify. You can register a target with multiple target groups. You can configure health checks on a per target group basis. Health checks are performed on all targets registered to a target group that is specified in a listener rule for your load balancer. The following diagram illustrates the basic components. Notice that each listener contains a default rule, and one listener contains another rule that routes requests to a different target group. One target is registered with two target groups.","title":"Application Load Balancer components"},{"location":"aws/lb_route53/network_security/","text":"Network Security \u00b6 https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-network-security.html Security Group \u00b6 Security groups act like built-in firewalls for your virtual servers \u2014 the rules you create define what is allowed to talk to your instances and how. Although network access control lists can be used to block or deny traffic, these operate at the subnet level (covering all instances in the subnet with the same ruleset), not per instance as the question specifies. Route Tables \u00b6 Route tables tell traffic where it should go next to reach its destination.","title":"Network Security"},{"location":"aws/lb_route53/network_security/#network-security","text":"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-network-security.html","title":"Network Security"},{"location":"aws/lb_route53/network_security/#security-group","text":"Security groups act like built-in firewalls for your virtual servers \u2014 the rules you create define what is allowed to talk to your instances and how. Although network access control lists can be used to block or deny traffic, these operate at the subnet level (covering all instances in the subnet with the same ruleset), not per instance as the question specifies.","title":"Security Group"},{"location":"aws/lb_route53/network_security/#route-tables","text":"Route tables tell traffic where it should go next to reach its destination.","title":"Route  Tables"},{"location":"aws/lb_route53/osi_model/","text":"OSI Model \u00b6 ByteByteGo OSI Model Open Systems Interconnect Model (OSI) is a theoretical framework which provides one way of thinking about networking. OSI splits the network communication between two devices on a network into seven abstraction layers. Seven layers of OSI model. Physical Layer Transmits raw bits of data across physical connection. Data Link Layer Takes raw bits and organizes into frames. Ethernet lives in this layer. Network Layer Routes data frames across different networks. Transport Layer This handles end to end communication between two nodes. Application Layers In practice, OSI is too granular and it is better to collapse layer 5-7 into one layer.","title":"OSI Model"},{"location":"aws/lb_route53/osi_model/#osi-model","text":"ByteByteGo OSI Model Open Systems Interconnect Model (OSI) is a theoretical framework which provides one way of thinking about networking. OSI splits the network communication between two devices on a network into seven abstraction layers. Seven layers of OSI model. Physical Layer Transmits raw bits of data across physical connection. Data Link Layer Takes raw bits and organizes into frames. Ethernet lives in this layer. Network Layer Routes data frames across different networks. Transport Layer This handles end to end communication between two nodes. Application Layers In practice, OSI is too granular and it is better to collapse layer 5-7 into one layer.","title":"OSI Model"},{"location":"aws/lb_route53/register_domain_name/","text":"Register a Domain Name \u00b6 DNS stands for Domain Name System, and it works exactly like a phone book. It's the process that computers use to resolve domain names to IP addresses. So if we were to go to, we know computers or we www.wkcsv.com, basically what's happening is a computer is looking this up and it's resolving an IP address. And then it's connecting your browser to that website using that IP address. So that's all that DNS is. It's a way for computers to look up IP addresses. DNS works on port 53. Host Website on S3 \u00b6 Add record to route53 \u00b6 In Route53 create an A record. And essentially if I browse to acloudguru2019example.com, it's going to redirect me, or it's going to serve up S3-website-us-east-1.amazon.com, and then it will be forward slash and then the bucket name. Set this as an alias record. This means means that it's basically going to be a naked domain name. So there's no like www, it's like, it's literally just acloudguru2019example.com.","title":"Register a Domain Name"},{"location":"aws/lb_route53/register_domain_name/#register-a-domain-name","text":"DNS stands for Domain Name System, and it works exactly like a phone book. It's the process that computers use to resolve domain names to IP addresses. So if we were to go to, we know computers or we www.wkcsv.com, basically what's happening is a computer is looking this up and it's resolving an IP address. And then it's connecting your browser to that website using that IP address. So that's all that DNS is. It's a way for computers to look up IP addresses. DNS works on port 53.","title":"Register a Domain Name"},{"location":"aws/lb_route53/register_domain_name/#host-website-on-s3","text":"","title":"Host Website on S3"},{"location":"aws/lb_route53/register_domain_name/#add-record-to-route53","text":"In Route53 create an A record. And essentially if I browse to acloudguru2019example.com, it's going to redirect me, or it's going to serve up S3-website-us-east-1.amazon.com, and then it will be forward slash and then the bucket name. Set this as an alias record. This means means that it's basically going to be a naked domain name. So there's no like www, it's like, it's literally just acloudguru2019example.com.","title":"Add record to route53"},{"location":"aws/lb_route53/route53_overview/","text":"Route53 Overview \u00b6 Route 53 is a DNS service that routes users to applications. allows domain name registration performs health checks on AWS services supports hybrid (public and private clouds) cloud architectures Route 53 is used to route users to the nearest data center to reduce latency Route 53 is a DNS service that routes users to applications. Amazon Route 53 effectively connects user requests to infrastructure running in AWS (e.g., Amazon EC2 instances, Elastic Load Balancing load balancers, or Amazon S3 buckets) and can also be used to route users to infrastructure outside of AWS. https://aws.amazon.com/route53/ disaster recovery Route 53 can be used for disaster recovery by simply shifting traffic to the new Region. Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. It is designed to give developers and businesses an extremely reliable and cost-effective way to route end users to internet applications by translating names (like www.example.com) into the numeric IP addresses (like 192.0.2.1) that computers use to connect to each other. Amazon Route 53 is fully compliant with IPv6 as well. Route53 policies allow you to route data to a second resource if the first is unhealthy ( Failover Routing ) and to route data to resources that have better performance ( Latency Based Routing ). Working with Private Hosted Zones Working with Public Zones Route53 FAQs Route53 Quotas Montor Health of Endpoints \u00b6 Route 53 can be used to configure DNS health checks to route traffic to healthy endpoints or to monitor the health of your applications. Domain Names \u00b6 Top Level Domain: .io , .com SOA Start of Authority. This is where the DNS starts. NS Name Server. This is where the DNS information is stored. A Record is used to translate the domain to an IP address. CName Canonical name is used to resolve one domain name to another Alias records Alias records are used to map resource records sets in your hosted zones to load balances, CloudFront distributions, or S3 buckets that are configured as websites. So an Alias record really exists within the AWS ecosystem. It's not a type of DNS record that you could set up with GoDaddy or on your own web servers, for example. Alias is just a way of mapping resources within AWS, within your hosted zones to other AWS resources such as load balancers, CloudFront distributions S3 buckets, etc. So Aliases basically work like a CNAME record in that you can map one DNS name to another target DNS name. So that's all an Alias record is, just remember that CNAMEs cannot be used for naked domain names. So this is your zone apex record. So for example you cannot have a CNAME just for acloudguru.com. You need to have it for something like m.acloudguru.com. And also remember that your Alias records can be used for naked domain names or zone apex records 'cause these are mapping to individual AWS services. So there are seven different routing policies that are available with Route 53 and we're going to cover off every single one of these in this section of the course. And we're going to give you demos of them. So we'll start with simple routing, we'll then move on to weighted routing, latency-based routing, failover routing, geolocation routing, geoproximity routing, and then we'll move on finally to multivalue answer routing.","title":"Route53 Overview"},{"location":"aws/lb_route53/route53_overview/#route53-overview","text":"Route 53 is a DNS service that routes users to applications. allows domain name registration performs health checks on AWS services supports hybrid (public and private clouds) cloud architectures Route 53 is used to route users to the nearest data center to reduce latency Route 53 is a DNS service that routes users to applications. Amazon Route 53 effectively connects user requests to infrastructure running in AWS (e.g., Amazon EC2 instances, Elastic Load Balancing load balancers, or Amazon S3 buckets) and can also be used to route users to infrastructure outside of AWS. https://aws.amazon.com/route53/ disaster recovery Route 53 can be used for disaster recovery by simply shifting traffic to the new Region. Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. It is designed to give developers and businesses an extremely reliable and cost-effective way to route end users to internet applications by translating names (like www.example.com) into the numeric IP addresses (like 192.0.2.1) that computers use to connect to each other. Amazon Route 53 is fully compliant with IPv6 as well. Route53 policies allow you to route data to a second resource if the first is unhealthy ( Failover Routing ) and to route data to resources that have better performance ( Latency Based Routing ). Working with Private Hosted Zones Working with Public Zones Route53 FAQs Route53 Quotas","title":"Route53 Overview"},{"location":"aws/lb_route53/route53_overview/#montor-health-of-endpoints","text":"Route 53 can be used to configure DNS health checks to route traffic to healthy endpoints or to monitor the health of your applications.","title":"Montor Health of Endpoints"},{"location":"aws/lb_route53/route53_overview/#domain-names","text":"Top Level Domain: .io , .com SOA Start of Authority. This is where the DNS starts. NS Name Server. This is where the DNS information is stored. A Record is used to translate the domain to an IP address. CName Canonical name is used to resolve one domain name to another Alias records Alias records are used to map resource records sets in your hosted zones to load balances, CloudFront distributions, or S3 buckets that are configured as websites. So an Alias record really exists within the AWS ecosystem. It's not a type of DNS record that you could set up with GoDaddy or on your own web servers, for example. Alias is just a way of mapping resources within AWS, within your hosted zones to other AWS resources such as load balancers, CloudFront distributions S3 buckets, etc. So Aliases basically work like a CNAME record in that you can map one DNS name to another target DNS name. So that's all an Alias record is, just remember that CNAMEs cannot be used for naked domain names. So this is your zone apex record. So for example you cannot have a CNAME just for acloudguru.com. You need to have it for something like m.acloudguru.com. And also remember that your Alias records can be used for naked domain names or zone apex records 'cause these are mapping to individual AWS services. So there are seven different routing policies that are available with Route 53 and we're going to cover off every single one of these in this section of the course. And we're going to give you demos of them. So we'll start with simple routing, we'll then move on to weighted routing, latency-based routing, failover routing, geolocation routing, geoproximity routing, and then we'll move on finally to multivalue answer routing.","title":"Domain Names"},{"location":"aws/lb_route53/routing_simple/","text":"Simple Routing \u00b6 So what is simple routing policies? Well, basically, if you choose the simple routing policy you can only have one record with multiple IP addresses. If you specify multiple values in a record, then Route 53 returns all values to that user in a random order. So we've got our user, they're basically typing in our domain name, it's going to Route 53 and it could be that it goes to 30.0.0.1, or it could go to 30.0.0.2, it's just returned in a random order. I'm going to go to find Route 53, which is under Network and Content Delivery, and what I'm going to do is I'm going to go into my hosted zone. And we can see my hosted zone in here, it's hellocloudgurus123, and we can see that we've only got 2 record types in here. So what we're going to do is we're going to go ahead and hit Create Record, and this will allow us to create our very first record. Now you see that we've got 2 different views, we've got Quick Create, recommended for expert users, and then we've got our wizard, which is recommended for our new users. We're always going to use the Quick Create record. And, in here, we can select our different record types. So we've got our A records, we've got our CNAMEs, we've got MX records, which does our mail servers, we've got TXT records, we've got NS records, and we've got all kinds of different records in here. Now we're going to be using our A records, so let's click in there and we're going to use the naked domain name, so it's just going to be hellocloudgurus123.com. In here we've got our routing policies, and this is where we've got all our different routing policies that are available to us, so we're going to start with simple routing policy. In here we've got our time to live, so how long is this record going to be cached on other people's servers or in their browsers? I am always going to have it as 1 minute, that way, basically, when we go and make changes to our DNS we only have to wait about 60 seconds. Note, you can have up to 2 days, but let's not do that, let's just start with 60 seconds. And in here we've got our values. So what IP address do we want? Well let's go in and open up a new tab, and we'll go over to EC2, and we're going to open up a tab and grab our IP addresses for both Northern Virginia and then we're going to get it for Tokyo. And it says, here, enter multiple values on our separate lines. Okay, so here I am in the EC2 console, I'm in Northern Virginia, and all I want to do is just click on my running instance, click in here, and copy this public IP address to my clipboard and then going to paste that in here for Route 53, and then all I want to do is change from Northern Virginia all the way back to Tokyo. I'm going to add in Tokyo in here, and then I'm just going to grab the public IP address for my Tokyo web server. So it's this one here, and I'm going to paste that in there. So we're now going to create this record, it's just an A record, it's using simple routing, our TTL is 60, we're not using an alias, we're going to put it towards these 2 public IP addresses, and I'm going to go ahead and hit Create Record. That has now created our A record, which we can see in here, and we can see that it's an A record, rooting is simple, and we're going to send our traffic to these 2 different IP addresses, So let's go ahead and test this out by going to hellocloudgurus123.com. Okay, so I've just gone to hellocloudgurus123.com. We can see it's directing us to Northern Virginia. And, so, I've just opened up a new browser in incognito mode and this has now directed me to Tokyo, and you will be able to just test it randomly. Just go ahead and open up your browser and go to the domain names in incognito mode, but in different browsers, and you'll be able to see that sometimes it will hit Tokyo, sometimes it will hit Northern Virginia.","title":"Simple Routing"},{"location":"aws/lb_route53/routing_simple/#simple-routing","text":"So what is simple routing policies? Well, basically, if you choose the simple routing policy you can only have one record with multiple IP addresses. If you specify multiple values in a record, then Route 53 returns all values to that user in a random order. So we've got our user, they're basically typing in our domain name, it's going to Route 53 and it could be that it goes to 30.0.0.1, or it could go to 30.0.0.2, it's just returned in a random order. I'm going to go to find Route 53, which is under Network and Content Delivery, and what I'm going to do is I'm going to go into my hosted zone. And we can see my hosted zone in here, it's hellocloudgurus123, and we can see that we've only got 2 record types in here. So what we're going to do is we're going to go ahead and hit Create Record, and this will allow us to create our very first record. Now you see that we've got 2 different views, we've got Quick Create, recommended for expert users, and then we've got our wizard, which is recommended for our new users. We're always going to use the Quick Create record. And, in here, we can select our different record types. So we've got our A records, we've got our CNAMEs, we've got MX records, which does our mail servers, we've got TXT records, we've got NS records, and we've got all kinds of different records in here. Now we're going to be using our A records, so let's click in there and we're going to use the naked domain name, so it's just going to be hellocloudgurus123.com. In here we've got our routing policies, and this is where we've got all our different routing policies that are available to us, so we're going to start with simple routing policy. In here we've got our time to live, so how long is this record going to be cached on other people's servers or in their browsers? I am always going to have it as 1 minute, that way, basically, when we go and make changes to our DNS we only have to wait about 60 seconds. Note, you can have up to 2 days, but let's not do that, let's just start with 60 seconds. And in here we've got our values. So what IP address do we want? Well let's go in and open up a new tab, and we'll go over to EC2, and we're going to open up a tab and grab our IP addresses for both Northern Virginia and then we're going to get it for Tokyo. And it says, here, enter multiple values on our separate lines. Okay, so here I am in the EC2 console, I'm in Northern Virginia, and all I want to do is just click on my running instance, click in here, and copy this public IP address to my clipboard and then going to paste that in here for Route 53, and then all I want to do is change from Northern Virginia all the way back to Tokyo. I'm going to add in Tokyo in here, and then I'm just going to grab the public IP address for my Tokyo web server. So it's this one here, and I'm going to paste that in there. So we're now going to create this record, it's just an A record, it's using simple routing, our TTL is 60, we're not using an alias, we're going to put it towards these 2 public IP addresses, and I'm going to go ahead and hit Create Record. That has now created our A record, which we can see in here, and we can see that it's an A record, rooting is simple, and we're going to send our traffic to these 2 different IP addresses, So let's go ahead and test this out by going to hellocloudgurus123.com. Okay, so I've just gone to hellocloudgurus123.com. We can see it's directing us to Northern Virginia. And, so, I've just opened up a new browser in incognito mode and this has now directed me to Tokyo, and you will be able to just test it randomly. Just go ahead and open up your browser and go to the domain names in incognito mode, but in different browsers, and you'll be able to see that sometimes it will hit Tokyo, sometimes it will hit Northern Virginia.","title":"Simple Routing"},{"location":"aws/machine_learning/comprehend/","text":"Comprehend \u00b6 NLP service that finds relationships in text. Can be used to process sentiment on social media posts.","title":"Comprehend"},{"location":"aws/machine_learning/comprehend/#comprehend","text":"NLP service that finds relationships in text. Can be used to process sentiment on social media posts.","title":"Comprehend"},{"location":"aws/machine_learning/lex/","text":"Lex \u00b6 Lex helps you build conversational interfaces like chatbots. Lex would be used to add a support chatbot to the help page on a company website.","title":"Lex"},{"location":"aws/machine_learning/lex/#lex","text":"Lex helps you build conversational interfaces like chatbots. Lex would be used to add a support chatbot to the help page on a company website.","title":"Lex"},{"location":"aws/machine_learning/polly/","text":"Polly \u00b6 Amazon Polly \u2013 Text to Speech in 47 Voices and 24 Languages","title":"Polly"},{"location":"aws/machine_learning/polly/#polly","text":"Amazon Polly \u2013 Text to Speech in 47 Voices and 24 Languages","title":"Polly"},{"location":"aws/machine_learning/rekognition/","text":"Rekognition \u00b6 Allows you to automate your image and video analysis. Upload images and videos and get confidence levels on predictions. Use Cases \u00b6 Identify custom labels in images and videos Face and text detection in images and videos","title":"Rekognition"},{"location":"aws/machine_learning/rekognition/#rekognition","text":"Allows you to automate your image and video analysis. Upload images and videos and get confidence levels on predictions.","title":"Rekognition"},{"location":"aws/machine_learning/rekognition/#use-cases","text":"Identify custom labels in images and videos Face and text detection in images and videos","title":"Use Cases"},{"location":"aws/security/WAF/","text":"WAF \u00b6 Web Application Firewall helps protect your web applications against common web attacks. protects against common attack patterns protects against sql injection protects against cross site scripting WAF can stand in front of CloudFront or ALB-EC2. End \u00b6","title":"WAF"},{"location":"aws/security/WAF/#waf","text":"Web Application Firewall helps protect your web applications against common web attacks. protects against common attack patterns protects against sql injection protects against cross site scripting WAF can stand in front of CloudFront or ALB-EC2.","title":"WAF"},{"location":"aws/security/WAF/#end","text":"","title":"End"},{"location":"aws/security/artifact/","text":"Artifact \u00b6 Artifact offers on demand access to AWS security and compliance reports. Central repository for compliance reports from third party vendors. Service Organization Controls (SOC) reports Payment Card Industry (PCI) reports ISO reporting End \u00b6","title":"Artifact"},{"location":"aws/security/artifact/#artifact","text":"Artifact offers on demand access to AWS security and compliance reports. Central repository for compliance reports from third party vendors. Service Organization Controls (SOC) reports Payment Card Industry (PCI) reports ISO reporting","title":"Artifact"},{"location":"aws/security/artifact/#end","text":"","title":"End"},{"location":"aws/security/athena_vs_macie/","text":"Athena vs Macie \u00b6 Athena Overview \u00b6 Athena is an interactive query service which enables you to analyse and query data located in S3 using standard SQL. Can be used as a data warehouse or to mine a data lake. Serverless, nothing to provision, pay per query/per TB scanned No need to set up complex ETL Works directly with S3 Athena Use Cases \u00b6 Query log files stored in S3 so it could be your ELB logs, your S3 access logs, etc. Generates business reports on data stored in S3 Analyze AWS costs and usage reports as well. Run queries on click-stream data Macie \u00b6 Macie is a data privacy service that helps you uncover and protect your sensitive data, such as personally identifiable information (PII) like credit card numbers, passport numbers, social security numbers, and more. Macie utilizes NLP to protect personally identifiable information in S3 or CloudTrail logs. End \u00b6","title":"Athena vs Macie"},{"location":"aws/security/athena_vs_macie/#athena-vs-macie","text":"","title":"Athena vs Macie"},{"location":"aws/security/athena_vs_macie/#athena-overview","text":"Athena is an interactive query service which enables you to analyse and query data located in S3 using standard SQL. Can be used as a data warehouse or to mine a data lake. Serverless, nothing to provision, pay per query/per TB scanned No need to set up complex ETL Works directly with S3","title":"Athena Overview"},{"location":"aws/security/athena_vs_macie/#athena-use-cases","text":"Query log files stored in S3 so it could be your ELB logs, your S3 access logs, etc. Generates business reports on data stored in S3 Analyze AWS costs and usage reports as well. Run queries on click-stream data","title":"Athena Use Cases"},{"location":"aws/security/athena_vs_macie/#macie","text":"Macie is a data privacy service that helps you uncover and protect your sensitive data, such as personally identifiable information (PII) like credit card numbers, passport numbers, social security numbers, and more. Macie utilizes NLP to protect personally identifiable information in S3 or CloudTrail logs.","title":"Macie"},{"location":"aws/security/athena_vs_macie/#end","text":"","title":"End"},{"location":"aws/security/auth/","text":"Authentication \u00b6 Users authenticate using an identity (username) and providing a verification (password).","title":"Authentication"},{"location":"aws/security/auth/#authentication","text":"Users authenticate using an identity (username) and providing a verification (password).","title":"Authentication"},{"location":"aws/security/aws_certificate_manager/","text":"AWS Certificate Manger \u00b6","title":"AWS Certificate Manger"},{"location":"aws/security/aws_certificate_manager/#aws-certificate-manger","text":"","title":"AWS Certificate Manger"},{"location":"aws/security/aws_config/","text":"AWS Config \u00b6 AWS Config provides a detailed view of the configuration of your AWS resources in your AWS account. This includes how the resources are related to one another, and how they were configured in the past. So you can see the configurations and relationships change over time. Well AWS Config, what that does is it monitors the configuration of the settings of your AWS environment. So it could be that you, your web DMZ had, port 22, so the SSH port locked down to a particular IP address up until last Friday and then somebody went in and deleted that, or they made a change and opened it up to the world. The cool thing about Config is it will show you, that configuration,","title":"AWS Config"},{"location":"aws/security/aws_config/#aws-config","text":"AWS Config provides a detailed view of the configuration of your AWS resources in your AWS account. This includes how the resources are related to one another, and how they were configured in the past. So you can see the configurations and relationships change over time. Well AWS Config, what that does is it monitors the configuration of the settings of your AWS environment. So it could be that you, your web DMZ had, port 22, so the SSH port locked down to a particular IP address up until last Friday and then somebody went in and deleted that, or they made a change and opened it up to the world. The cool thing about Config is it will show you, that configuration,","title":"AWS Config"},{"location":"aws/security/certificates_namecheap/","text":"Link my domain to AWS \u00b6 Domain must be secured with SSL certifcate. Add SSL Certificate to your domain YouTube Demo: Add SSL Certificate To Your Domain || AWS || Namecheap Point Custom Domain from Namecheap to AWS Route 53 for a Static Website in S3 Bucket AWS Load Balancer HTTPS Setup with Route 53 and Certificate Manager & HTTP Redirect to HTTPS (AWS Route53 Subdomain Delegation)[https://www.youtube.com/watch?v=COaARRYXdts] Stack Overflow - What certificates are needed for multi-level subdomains? Youtube - AWS Load Balancer HTTPS Setup with Route 53 and Certificate Manager & HTTP Redirect to HTTPS Do you want to move the domain from namecheap to Route53, or just use Route53 for DNS? These are two different things. You do not need to move the domain to Route53 to use it for DNS. To do this, create a zone in Route53 - this will result in an NS and SOA record. Copy these, then log into to Namecheap & update your DNS settings, replacing the NS and SOA records. Keep in mind that if you have any existing records (i.e MX for email, or something like www for a web site) you should first create them in Route53 so you won't lose them during the switchover. First, you will need to point nameservers in your AWS account and create a hosted zone for your Namecheap domain. Once you\u2019ve finished, follow these steps: Workflow \u00b6 Summary of workflow. I created an X.509 certificate using the AWS certificate manager. I used a wildcard designation, *.mydomain.com, and validated it using the AWS DNS. I then attached it to my Elastic Load Balancer (ELB) along with the instances running my web service. I then set up a CNAME record in my AWS DNS where the alias name is dev.360yield.admin.mydomain.com and points to the canonical DNS name of the ELB. Request a public certificate \u00b6 Request a public SSL/TLS certificate from Amazon. By default, public certificates are trusted by browsers and operating systems. Request a public certificate in AWS. I add a wildcard to allow subdomains. Next we need to add some CNAME records from ACM to Route53 hosted zone. This will enable ACM to validate the ownership of our domains. According to RFC 2818: Names may contain the wildcard character * which is considered to match any single domain name component or component fragment. For example: - .a.com matches foo.a.com but not bar.foo.a.com. - f .com matches foo.com but not bar.com. FQDN \u00b6 A fully qualified domain name (FQDN) is the unique name of an organization or individual on the Internet. It must end in a top-level domain extension such as .com or .org. Type the fully qualified domain name of the site that you want to secure with an SSL/TLS certificate (for example, www.example.com). Use an asterisk (*) to request a wildcard certificate to protect several sites in the same domain. For example, *.example.com protects www.example.com, site.example.com, and images.example.com. FQDN: *.npcompleted.cloud DNS validation: Choose this option if you are authorized to modify the DNS configuration for the domains in your certificate request. Key algorithm: RSA 2048. RSA is the most widely used key type. Load Balancer \u00b6 Create load balancer. This is best provisioned using cloudformation. Domain Provider - Link domain to AWS \u00b6 Sign in to namecheap and select the Custom DNS option from the drop-down for Nameservers and enter your 4 Custom nameservers (from your AWS account) into the fields given. Once entered, make sure you click on the green checkmark to save the changes: Add records in Namecheap Navigate to Namecheap > Domain List > Advanced DNS Add new record Record 1 DNS name from ALB CNAME Record - Get this from the A record in route53 Host @ Value internal-fargate-LB-304302973.us-east-1.elb.amazonaws.com. Value: MWAAEnvironment-ALB-263175543.us-east-1.elb.amazonaws.com Record 2 Get this from AWS ACM. It will be the CNAME name and CNAME value from the domain. Type: CNAME Record Host: _ee5e1f3ceed7d9c371354d53e6b0248b Value: _944142ae6d0f702ad2c85be4a2105fd9.xmkpffzlvd.acm-validations.aws. Copy the ns from Route53 Add the ns to Namecheap","title":"Link my domain to AWS"},{"location":"aws/security/certificates_namecheap/#link-my-domain-to-aws","text":"Domain must be secured with SSL certifcate. Add SSL Certificate to your domain YouTube Demo: Add SSL Certificate To Your Domain || AWS || Namecheap Point Custom Domain from Namecheap to AWS Route 53 for a Static Website in S3 Bucket AWS Load Balancer HTTPS Setup with Route 53 and Certificate Manager & HTTP Redirect to HTTPS (AWS Route53 Subdomain Delegation)[https://www.youtube.com/watch?v=COaARRYXdts] Stack Overflow - What certificates are needed for multi-level subdomains? Youtube - AWS Load Balancer HTTPS Setup with Route 53 and Certificate Manager & HTTP Redirect to HTTPS Do you want to move the domain from namecheap to Route53, or just use Route53 for DNS? These are two different things. You do not need to move the domain to Route53 to use it for DNS. To do this, create a zone in Route53 - this will result in an NS and SOA record. Copy these, then log into to Namecheap & update your DNS settings, replacing the NS and SOA records. Keep in mind that if you have any existing records (i.e MX for email, or something like www for a web site) you should first create them in Route53 so you won't lose them during the switchover. First, you will need to point nameservers in your AWS account and create a hosted zone for your Namecheap domain. Once you\u2019ve finished, follow these steps:","title":"Link my domain to AWS"},{"location":"aws/security/certificates_namecheap/#workflow","text":"Summary of workflow. I created an X.509 certificate using the AWS certificate manager. I used a wildcard designation, *.mydomain.com, and validated it using the AWS DNS. I then attached it to my Elastic Load Balancer (ELB) along with the instances running my web service. I then set up a CNAME record in my AWS DNS where the alias name is dev.360yield.admin.mydomain.com and points to the canonical DNS name of the ELB.","title":"Workflow"},{"location":"aws/security/certificates_namecheap/#request-a-public-certificate","text":"Request a public SSL/TLS certificate from Amazon. By default, public certificates are trusted by browsers and operating systems. Request a public certificate in AWS. I add a wildcard to allow subdomains. Next we need to add some CNAME records from ACM to Route53 hosted zone. This will enable ACM to validate the ownership of our domains. According to RFC 2818: Names may contain the wildcard character * which is considered to match any single domain name component or component fragment. For example: - .a.com matches foo.a.com but not bar.foo.a.com. - f .com matches foo.com but not bar.com.","title":"Request a public certificate"},{"location":"aws/security/certificates_namecheap/#fqdn","text":"A fully qualified domain name (FQDN) is the unique name of an organization or individual on the Internet. It must end in a top-level domain extension such as .com or .org. Type the fully qualified domain name of the site that you want to secure with an SSL/TLS certificate (for example, www.example.com). Use an asterisk (*) to request a wildcard certificate to protect several sites in the same domain. For example, *.example.com protects www.example.com, site.example.com, and images.example.com. FQDN: *.npcompleted.cloud DNS validation: Choose this option if you are authorized to modify the DNS configuration for the domains in your certificate request. Key algorithm: RSA 2048. RSA is the most widely used key type.","title":"FQDN"},{"location":"aws/security/certificates_namecheap/#load-balancer","text":"Create load balancer. This is best provisioned using cloudformation.","title":"Load Balancer"},{"location":"aws/security/certificates_namecheap/#domain-provider-link-domain-to-aws","text":"Sign in to namecheap and select the Custom DNS option from the drop-down for Nameservers and enter your 4 Custom nameservers (from your AWS account) into the fields given. Once entered, make sure you click on the green checkmark to save the changes: Add records in Namecheap Navigate to Namecheap > Domain List > Advanced DNS Add new record Record 1 DNS name from ALB CNAME Record - Get this from the A record in route53 Host @ Value internal-fargate-LB-304302973.us-east-1.elb.amazonaws.com. Value: MWAAEnvironment-ALB-263175543.us-east-1.elb.amazonaws.com Record 2 Get this from AWS ACM. It will be the CNAME name and CNAME value from the domain. Type: CNAME Record Host: _ee5e1f3ceed7d9c371354d53e6b0248b Value: _944142ae6d0f702ad2c85be4a2105fd9.xmkpffzlvd.acm-validations.aws. Copy the ns from Route53 Add the ns to Namecheap","title":"Domain Provider - Link domain to AWS"},{"location":"aws/security/cloudtrail/","text":"AWS CloudTrail \u00b6 Increases visibility into user and resource activity by recording AWS Management Console actions and API calls. You can identify which users and accounts called AWS, the source IP, from which the calls were made and when the calls occurred. What can CloudTrail track? Track IP address Username Event time and name of event Access key used Region Error code (if an error occured) AWS CloudTrail User Guide CloudTrail POC \u00b6 Set up CloudTrail to record events into file in S3 and query events using Athena. Create trail to start creating a CloudTrail. CloudTrail saves events for 90 days by default. End \u00b6","title":"AWS CloudTrail"},{"location":"aws/security/cloudtrail/#aws-cloudtrail","text":"Increases visibility into user and resource activity by recording AWS Management Console actions and API calls. You can identify which users and accounts called AWS, the source IP, from which the calls were made and when the calls occurred. What can CloudTrail track? Track IP address Username Event time and name of event Access key used Region Error code (if an error occured) AWS CloudTrail User Guide","title":"AWS CloudTrail"},{"location":"aws/security/cloudtrail/#cloudtrail-poc","text":"Set up CloudTrail to record events into file in S3 and query events using Athena. Create trail to start creating a CloudTrail. CloudTrail saves events for 90 days by default.","title":"CloudTrail POC"},{"location":"aws/security/cloudtrail/#end","text":"","title":"End"},{"location":"aws/security/cloudwatch/","text":"CloudWatch \u00b6 CloudWatch is a collection of services that help you monitor and observe cloud resources. Collects metrics, logs and events. detect anomalies in environment Set alarms Visualize logs Enable billing alerts CloudWatch Alarms can be used to determine the percentage of utilization versus the limit. For example, monitoring service limits to ensure they don't exceed free-tier usage on the account.s CloudWatch monitors performance of AWS resources. - CPU - Network - Disk - Status Check CloudWatch Events \u00b6 Send alarm when root level activity is detected in account. - Create CW event > add trigger (SNS topic) > receive email","title":"CloudWatch"},{"location":"aws/security/cloudwatch/#cloudwatch","text":"CloudWatch is a collection of services that help you monitor and observe cloud resources. Collects metrics, logs and events. detect anomalies in environment Set alarms Visualize logs Enable billing alerts CloudWatch Alarms can be used to determine the percentage of utilization versus the limit. For example, monitoring service limits to ensure they don't exceed free-tier usage on the account.s CloudWatch monitors performance of AWS resources. - CPU - Network - Disk - Status Check","title":"CloudWatch"},{"location":"aws/security/cloudwatch/#cloudwatch-events","text":"Send alarm when root level activity is detected in account. - Create CW event > add trigger (SNS topic) > receive email","title":"CloudWatch Events"},{"location":"aws/security/cognito/","text":"Cognito \u00b6 Cognito helps control access to mobile and web applications. provides authentication and authorization helps manage users assists with user sign up and sign in","title":"Cognito"},{"location":"aws/security/cognito/#cognito","text":"Cognito helps control access to mobile and web applications. provides authentication and authorization helps manage users assists with user sign up and sign in","title":"Cognito"},{"location":"aws/security/config/","text":"Config \u00b6 Config allows you to assess , audit and evaluate the configuration of your resources. Set granular guardrails and get notifications anytime those drift out of bounds. Track configuration changes over time. Deliver configuration history file to S3. Notifications via SNS for every config change. Config POC \u00b6 Provision Config in console using default settings. The config dashboard will show all resources in AWS account. End \u00b6","title":"Config"},{"location":"aws/security/config/#config","text":"Config allows you to assess , audit and evaluate the configuration of your resources. Set granular guardrails and get notifications anytime those drift out of bounds. Track configuration changes over time. Deliver configuration history file to S3. Notifications via SNS for every config change.","title":"Config"},{"location":"aws/security/config/#config-poc","text":"Provision Config in console using default settings. The config dashboard will show all resources in AWS account.","title":"Config POC"},{"location":"aws/security/config/#end","text":"","title":"End"},{"location":"aws/security/encryption/","text":"Data Encryption and Secrets Management Services \u00b6 KMS \u00b6 Key management service used to generate, store and control keys. Encrypt EBS volumes CloudHSM \u00b6 CloudHSM is a hardware security module used to generate encryption keys. User is responsible for managing the keys generated. Secrets Manager \u00b6 Secrets Manager allows you to manage and retrieve secrets (passwords and keys). Integrates natively with documentDB, RDS and Redshift. End \u00b6","title":"Data Encryption and Secrets Management Services"},{"location":"aws/security/encryption/#data-encryption-and-secrets-management-services","text":"","title":"Data Encryption and Secrets Management Services"},{"location":"aws/security/encryption/#kms","text":"Key management service used to generate, store and control keys. Encrypt EBS volumes","title":"KMS"},{"location":"aws/security/encryption/#cloudhsm","text":"CloudHSM is a hardware security module used to generate encryption keys. User is responsible for managing the keys generated.","title":"CloudHSM"},{"location":"aws/security/encryption/#secrets-manager","text":"Secrets Manager allows you to manage and retrieve secrets (passwords and keys). Integrates natively with documentDB, RDS and Redshift.","title":"Secrets Manager"},{"location":"aws/security/encryption/#end","text":"","title":"End"},{"location":"aws/security/github_secrets/","text":"Github Secrets \u00b6 Vault is a good option. There's a Github action for it, link to hashicorp vault github action . As a short term solution, you can use an app role to auth to vault, though I'd like to see if we can enable the JWT with Github OIDC option down the road (either building into Fort Knox, or once we enable namespaces at some point teams can set it up themselves). But until then, you can use an app role and you just have to maintain that as a secret in Github, and it can then access everything else it needs out of Vault. That decouples the auth to vault from the secrets as well -- you can set up an app role per repo, for example, if you want, or just have one shared app role across repos, but the actual secrets are kept in vault. https://github.","title":"Github Secrets"},{"location":"aws/security/github_secrets/#github-secrets","text":"Vault is a good option. There's a Github action for it, link to hashicorp vault github action . As a short term solution, you can use an app role to auth to vault, though I'd like to see if we can enable the JWT with Github OIDC option down the road (either building into Fort Knox, or once we enable namespaces at some point teams can set it up themselves). But until then, you can use an app role and you just have to maintain that as a secret in Github, and it can then access everything else it needs out of Vault. That decouples the auth to vault from the secrets as well -- you can set up an app role per repo, for example, if you want, or just have one shared app role across repos, but the actual secrets are kept in vault. https://github.","title":"Github Secrets"},{"location":"aws/security/guard_duty/","text":"GuardDuty \u00b6 GuardDuty can perform automated remediation actions by leveraging Amazon CloudWatch Events and AWS Lambda. GuardDuty continuously monitors for threats and unauthorized behavior to protect your AWS accounts, workloads, and data stored in Amazon S3. GuardDuty analyzes multiple AWS data sources, such as AWS CloudTrail event logs, Amazon VPC Flow Logs, and DNS logs.","title":"GuardDuty"},{"location":"aws/security/guard_duty/#guardduty","text":"GuardDuty can perform automated remediation actions by leveraging Amazon CloudWatch Events and AWS Lambda. GuardDuty continuously monitors for threats and unauthorized behavior to protect your AWS accounts, workloads, and data stored in Amazon S3. GuardDuty analyzes multiple AWS data sources, such as AWS CloudTrail event logs, Amazon VPC Flow Logs, and DNS logs.","title":"GuardDuty"},{"location":"aws/security/inspector/","text":"Inspector \u00b6 AWS Inspector assesses the security and compliance of your EC2 instances. Use Amazon Inspector for security assessment of applications. Inspector AWS Docs Agent installed on EC2 instance Spot deviations from security best practices. Reports vulnerabilities found, e.g. open ports. Checks access from the internet, remote root login, vulnerable software versions, etc. End \u00b6","title":"Inspector"},{"location":"aws/security/inspector/#inspector","text":"AWS Inspector assesses the security and compliance of your EC2 instances. Use Amazon Inspector for security assessment of applications. Inspector AWS Docs Agent installed on EC2 instance Spot deviations from security best practices. Reports vulnerabilities found, e.g. open ports. Checks access from the internet, remote root login, vulnerable software versions, etc.","title":"Inspector"},{"location":"aws/security/inspector/#end","text":"","title":"End"},{"location":"aws/security/macie/","text":"Macie \u00b6 Macie helps you discover and protect sensitive data. Personal Information credit card numbers social security numbers","title":"Macie"},{"location":"aws/security/macie/#macie","text":"Macie helps you discover and protect sensitive data. Personal Information credit card numbers social security numbers","title":"Macie"},{"location":"aws/security/penetration_testing/","text":"Penetration testing \u00b6 Customer Service Policy for Penetration Testing AWS customers are welcome to carry out security assessments or penetration tests against their AWS infrastructure without prior approval for Amazon EC2 instances, NAT gateways, elastic load balancers, and 7 other services. Reference: Penetration Testing.","title":"Penetration testing"},{"location":"aws/security/penetration_testing/#penetration-testing","text":"Customer Service Policy for Penetration Testing AWS customers are welcome to carry out security assessments or penetration tests against their AWS infrastructure without prior approval for Amazon EC2 instances, NAT gateways, elastic load balancers, and 7 other services. Reference: Penetration Testing.","title":"Penetration testing"},{"location":"aws/security/personal_health_dashboard/","text":"AWS Personal Health Dashboard \u00b6 AWS Personal Health Dashboard provides alerts and guidance for AWS events that might affect your environment. https://aws.amazon.com/premiumsupport/technology/personal-health-dashboard/","title":"AWS Personal Health Dashboard"},{"location":"aws/security/personal_health_dashboard/#aws-personal-health-dashboard","text":"AWS Personal Health Dashboard provides alerts and guidance for AWS events that might affect your environment. https://aws.amazon.com/premiumsupport/technology/personal-health-dashboard/","title":"AWS Personal Health Dashboard"},{"location":"aws/security/security_hub/","text":"AWS Security Hub \u00b6 Comprehensive view of your security alerts across multiple AWS accounts. Security Hub provides a single place that aggregates, organizes and prioritizes your security alerts or findings from multiple AWS services - such as Amazon GuardDuty , Amazon Inspector, Amazon Macie, AWS IAM Access Analyzer and AWS Firewall Manager - across multiple AWS accounts.","title":"AWS Security Hub"},{"location":"aws/security/security_hub/#aws-security-hub","text":"Comprehensive view of your security alerts across multiple AWS accounts. Security Hub provides a single place that aggregates, organizes and prioritizes your security alerts or findings from multiple AWS services - such as Amazon GuardDuty , Amazon Inspector, Amazon Macie, AWS IAM Access Analyzer and AWS Firewall Manager - across multiple AWS accounts.","title":"AWS Security Hub"},{"location":"aws/security/security_summary/","text":"Security Summary \u00b6 Shared Responsibility Model \u00b6 Shared Responsibility Model Reference: AWS Shared Responsibility Model AWS Responibility \u00b6 AWS is responsible for security OF the cloud AWS are always going to be responsible for things like hardware and global infrastructure. They're going to be responsible for their regions, their availability zones, their edge locations, and then the services that sit on top, so they're going to be responsible for the compute, the storage, the databases, the networking, and then any other software that sits on top of that sort of stack, so they go all the way up to the application layer in some specific circumstances. So things like RDS, for example, they're responsible for the underlying operating system, as well as being responsible for patching MySQL. Customer Responsibility \u00b6 Customer is responsible for security of things IN the cloud. Customer Data Platform, Applications, IAM OS, Network and Firewall Configuration Client side data encryption OSI model AWS Inspector assesses the security and compliance of your EC2 instances. A HIPAA certification attests to the fact that the AWS Platform has met the standard required for the secure storage of medical records in the US A PCI DSS Level 1 certification attests to the security of the AWS platform regarding credit card transactions. WAF is designed to stop hackers. WAF operates up to (and including) Layer 7 of the OSI Model. AWS shield is a DDOS protiection. AWS Shield Standard defends against most common, frequently occurring network and transport layer DDoS attacks that target your website or applications. Automated application (layer 7) traffic monitoring, you can subscribe to AWS Shield Advanced. Reference: How AWS Shield works. AWS inspector inspects EC2 instances AWS Trusted advicsor inspects global AWS account for cost optimization, performance, security and fault tolerance.","title":"Security Summary"},{"location":"aws/security/security_summary/#security-summary","text":"","title":"Security Summary"},{"location":"aws/security/security_summary/#shared-responsibility-model","text":"Shared Responsibility Model Reference: AWS Shared Responsibility Model","title":"Shared Responsibility Model"},{"location":"aws/security/security_summary/#aws-responibility","text":"AWS is responsible for security OF the cloud AWS are always going to be responsible for things like hardware and global infrastructure. They're going to be responsible for their regions, their availability zones, their edge locations, and then the services that sit on top, so they're going to be responsible for the compute, the storage, the databases, the networking, and then any other software that sits on top of that sort of stack, so they go all the way up to the application layer in some specific circumstances. So things like RDS, for example, they're responsible for the underlying operating system, as well as being responsible for patching MySQL.","title":"AWS Responibility"},{"location":"aws/security/security_summary/#customer-responsibility","text":"Customer is responsible for security of things IN the cloud. Customer Data Platform, Applications, IAM OS, Network and Firewall Configuration Client side data encryption OSI model AWS Inspector assesses the security and compliance of your EC2 instances. A HIPAA certification attests to the fact that the AWS Platform has met the standard required for the secure storage of medical records in the US A PCI DSS Level 1 certification attests to the security of the AWS platform regarding credit card transactions. WAF is designed to stop hackers. WAF operates up to (and including) Layer 7 of the OSI Model. AWS shield is a DDOS protiection. AWS Shield Standard defends against most common, frequently occurring network and transport layer DDoS attacks that target your website or applications. Automated application (layer 7) traffic monitoring, you can subscribe to AWS Shield Advanced. Reference: How AWS Shield works. AWS inspector inspects EC2 instances AWS Trusted advicsor inspects global AWS account for cost optimization, performance, security and fault tolerance.","title":"Customer Responsibility"},{"location":"aws/security/shared_responsibility/","text":"Security Summary \u00b6 Shared Responsibility Model \u00b6 Shared Responsibility Model Reference: AWS Shared Responsibility Model AWS Responsibility \u00b6 AWS is responsible for security OF the cloud AWS are always going to be responsible for things like hardware and global infrastructure. They're going to be responsible for their regions, their availability zones, their edge locations, and then the services that sit on top, so they're going to be responsible for the compute, the storage, the databases, the networking, and then any other software that sits on top of that sort of stack, so they go all the way up to the application layer in some specific circumstances. So things like RDS, for example, they're responsible for the underlying operating system, as well as being responsible for patching MySQL. Customer Responsibility \u00b6 Customer is responsible for security of things IN the cloud. Customer Data (EBS snapshots, RDS backups) Platform, Applications, IAM OS, Network and Firewall Configuration Client side data encryption Guest operating system Application Security Permissions (S3) A HIPAA certification attests to the fact that the AWS Platform has met the standard required for the secure storage of medical records in the US A PCI DSS Level 1 certification attests to the security of the AWS platform regarding credit card transactions. WAF is designed to stop hackers. WAF operates up to (and including) Layer 7 of the OSI Model.","title":"Security Summary"},{"location":"aws/security/shared_responsibility/#security-summary","text":"","title":"Security Summary"},{"location":"aws/security/shared_responsibility/#shared-responsibility-model","text":"Shared Responsibility Model Reference: AWS Shared Responsibility Model","title":"Shared Responsibility Model"},{"location":"aws/security/shared_responsibility/#aws-responsibility","text":"AWS is responsible for security OF the cloud AWS are always going to be responsible for things like hardware and global infrastructure. They're going to be responsible for their regions, their availability zones, their edge locations, and then the services that sit on top, so they're going to be responsible for the compute, the storage, the databases, the networking, and then any other software that sits on top of that sort of stack, so they go all the way up to the application layer in some specific circumstances. So things like RDS, for example, they're responsible for the underlying operating system, as well as being responsible for patching MySQL.","title":"AWS Responsibility"},{"location":"aws/security/shared_responsibility/#customer-responsibility","text":"Customer is responsible for security of things IN the cloud. Customer Data (EBS snapshots, RDS backups) Platform, Applications, IAM OS, Network and Firewall Configuration Client side data encryption Guest operating system Application Security Permissions (S3) A HIPAA certification attests to the fact that the AWS Platform has met the standard required for the secure storage of medical records in the US A PCI DSS Level 1 certification attests to the security of the AWS platform regarding credit card transactions. WAF is designed to stop hackers. WAF operates up to (and including) Layer 7 of the OSI Model.","title":"Customer Responsibility"},{"location":"aws/security/shield/","text":"AWS shield \u00b6 AWS Shield is a managed DDOS protection. A DDOS attack causes a traffic jam on a website to cause it to crash. Shield Standard (free) protections against common occuring attacks. Shield Advanced provides 24/7 access to AWS experts AWS shield is a DDOS protection. AWS Shield Standard defends against most common, frequently occurring network and transport layer DDoS attacks that target your website or applications. Automated application (layer 7) traffic monitoring, you can subscribe to AWS Shield Advanced. Reference: How AWS Shield works.","title":"AWS shield"},{"location":"aws/security/shield/#aws-shield","text":"AWS Shield is a managed DDOS protection. A DDOS attack causes a traffic jam on a website to cause it to crash. Shield Standard (free) protections against common occuring attacks. Shield Advanced provides 24/7 access to AWS experts AWS shield is a DDOS protection. AWS Shield Standard defends against most common, frequently occurring network and transport layer DDoS attacks that target your website or applications. Automated application (layer 7) traffic monitoring, you can subscribe to AWS Shield Advanced. Reference: How AWS Shield works.","title":"AWS shield"},{"location":"aws/security/trusted_advisor/","text":"AWS Trusted Advisor \u00b6 Security, performance, fault tolerance and cost optimization for global AWS account. AWS Trusted advisor inspects global AWS account for cost optimization, performance, security and fault tolerance. Trusted Advisor has a service limit dashboard that helps you monitor service limits. For example, monitoring service limits to ensure they don't exceed free-tier usage on the account. Use Cases \u00b6 Low utilization on EC2 instances. Trusted Advisor checks this for all customers. FYI: This was found in the \"AWS Trusted Advisor best practice checklist\" documentation linked from within the lesson. Exposed access keys. Trusted Advisor checks this for Enterprise and Business Support customers. S3 bucket permissions for public access. Trusted Advisor checks this for all customers. A customer needs to identify vulnerabilities on their EC2 instances, such as unintended network access. Which services will provide a report of findings? Trusted Advisor is a tool that provides real-time guidance to help you provision resources following AWS best practices. It will check security groups for rules that allow unrestricted access (0.0.0.0/0) to specific ports. End \u00b6","title":"AWS Trusted Advisor"},{"location":"aws/security/trusted_advisor/#aws-trusted-advisor","text":"Security, performance, fault tolerance and cost optimization for global AWS account. AWS Trusted advisor inspects global AWS account for cost optimization, performance, security and fault tolerance. Trusted Advisor has a service limit dashboard that helps you monitor service limits. For example, monitoring service limits to ensure they don't exceed free-tier usage on the account.","title":"AWS Trusted Advisor"},{"location":"aws/security/trusted_advisor/#use-cases","text":"Low utilization on EC2 instances. Trusted Advisor checks this for all customers. FYI: This was found in the \"AWS Trusted Advisor best practice checklist\" documentation linked from within the lesson. Exposed access keys. Trusted Advisor checks this for Enterprise and Business Support customers. S3 bucket permissions for public access. Trusted Advisor checks this for all customers. A customer needs to identify vulnerabilities on their EC2 instances, such as unintended network access. Which services will provide a report of findings? Trusted Advisor is a tool that provides real-time guidance to help you provision resources following AWS best practices. It will check security groups for rules that allow unrestricted access (0.0.0.0/0) to specific ports.","title":"Use Cases"},{"location":"aws/security/trusted_advisor/#end","text":"","title":"End"},{"location":"aws/security/well_architected_framework/","text":"Leveraging the Well Architected Framework \u00b6 What is the well-architected framework? The 6 pillars of the well-architected framework are a high level description of principles and best practices that should always be followed when creating architecture in the Cloud. Six pillars of the well architected framework Operational Excellence Creating applications that effectively support production workflows. Plan for and anticipate failure by identifying your most critical failure points and making sure there's redundancy. Deploy smaller, reversible changes. This is often done through infrastructure as code which is a way to version control your AWS infrastructure. CloudFormation Script operations as code and learn from failure and refine. Security Automate security tasks like logging and risk mitigation. Encrypt data in transit, and at rest. Principle of least privelege Make sure not to give users or applications privileges that they don't need. CloudTrail Track who did what and when through services like CloudTrail . Consider all application layers in your security profile. Securing your AWS account doesn't do much good if your databases for your applications are open to the world. Reliability designing systems that work consistently and recover quickly. Analyze and Recovery Make sure you understand where your applications can fail and how you can have them recover automatically. Scale horizontally It's also best practice to scale horizontally rather than vertically for resilience. For example, if you have 5 EC2 instances serving traffic and one of them goes down, that's not necessarily a huge deal. But if you have one big instance serving traffic and that one fails, well, then none of your users can access the application. You also want to reduce idle resources, manage change through automation, and test all of your recovery procedures. Performance Efficiency This Performance Efficiency pillar focuses on the effective use of resources to meet demand. In this pillar, you would use the information gathered through the evaluation process to actively drive adoption of new services or resources. You would also define a process to improve workload performance, and you would need to stay up-to-date on new resources and services. Use serverless architectures first Use multi-region deployments Cost Optimization Utilize consumption based pricing Implement Cloud financial management Measure overalll efficiency Pay only for resources your application requires. Sustainability Environmental impacts, especially energy consumption and efficiency. Use managed services to reduce idle activity End \u00b6","title":"Leveraging the Well Architected Framework"},{"location":"aws/security/well_architected_framework/#leveraging-the-well-architected-framework","text":"What is the well-architected framework? The 6 pillars of the well-architected framework are a high level description of principles and best practices that should always be followed when creating architecture in the Cloud. Six pillars of the well architected framework Operational Excellence Creating applications that effectively support production workflows. Plan for and anticipate failure by identifying your most critical failure points and making sure there's redundancy. Deploy smaller, reversible changes. This is often done through infrastructure as code which is a way to version control your AWS infrastructure. CloudFormation Script operations as code and learn from failure and refine. Security Automate security tasks like logging and risk mitigation. Encrypt data in transit, and at rest. Principle of least privelege Make sure not to give users or applications privileges that they don't need. CloudTrail Track who did what and when through services like CloudTrail . Consider all application layers in your security profile. Securing your AWS account doesn't do much good if your databases for your applications are open to the world. Reliability designing systems that work consistently and recover quickly. Analyze and Recovery Make sure you understand where your applications can fail and how you can have them recover automatically. Scale horizontally It's also best practice to scale horizontally rather than vertically for resilience. For example, if you have 5 EC2 instances serving traffic and one of them goes down, that's not necessarily a huge deal. But if you have one big instance serving traffic and that one fails, well, then none of your users can access the application. You also want to reduce idle resources, manage change through automation, and test all of your recovery procedures. Performance Efficiency This Performance Efficiency pillar focuses on the effective use of resources to meet demand. In this pillar, you would use the information gathered through the evaluation process to actively drive adoption of new services or resources. You would also define a process to improve workload performance, and you would need to stay up-to-date on new resources and services. Use serverless architectures first Use multi-region deployments Cost Optimization Utilize consumption based pricing Implement Cloud financial management Measure overalll efficiency Pay only for resources your application requires. Sustainability Environmental impacts, especially energy consumption and efficiency. Use managed services to reduce idle activity","title":"Leveraging the Well Architected Framework"},{"location":"aws/security/well_architected_framework/#end","text":"","title":"End"},{"location":"aws_cli/cloud_formation/","text":"Show cloudformation stacks in account: aws cloudformation list-stacks Check user is logged in aws sts get-caller-identity { \"UserId\" : \"AIDAV6PU4M4U7N6BMXKBM\" , \"Account\" : \"409072330537\" , \"Arn\" : \"arn:aws:iam::409072330537:user/nada\" } Delete cloudformation stack \u00b6 aws cloudformation delete-stack \\ --stack-name my-stack aws cloudformation delete-stack --stack-name SC-073416988478-pp-zg7jw6uf5fu2o S3 \u00b6 CF deploy. Create S3 bucket using CLI. aws cloudformation deploy --template-file /path_to_template/FILE-NAME.yaml --stack-name s3-dse-smol-elxsj ## parameters --parameter-overrides Key1 = Value1 Key2 = Value2 ## tags --tags Key1 = Value1 Key2 = Value2 Example aws cloudformation deploy --template-file cloud_formation/s3/s3.yaml --stack-name s3-dse-smol-elxsj ## parameters --parameter-overrides BucketName = cf-templates-elxsj ## tags 'MonOwnerTag=elxsj' 'ProjectTag=data-storage' 'DataClassificationTag=internal' 'EnvironmentTag=prod'","title":"Cloud formation"},{"location":"aws_cli/cloud_formation/#delete-cloudformation-stack","text":"aws cloudformation delete-stack \\ --stack-name my-stack aws cloudformation delete-stack --stack-name SC-073416988478-pp-zg7jw6uf5fu2o","title":"Delete cloudformation stack"},{"location":"aws_cli/cloud_formation/#s3","text":"CF deploy. Create S3 bucket using CLI. aws cloudformation deploy --template-file /path_to_template/FILE-NAME.yaml --stack-name s3-dse-smol-elxsj ## parameters --parameter-overrides Key1 = Value1 Key2 = Value2 ## tags --tags Key1 = Value1 Key2 = Value2 Example aws cloudformation deploy --template-file cloud_formation/s3/s3.yaml --stack-name s3-dse-smol-elxsj ## parameters --parameter-overrides BucketName = cf-templates-elxsj ## tags 'MonOwnerTag=elxsj' 'ProjectTag=data-storage' 'DataClassificationTag=internal' 'EnvironmentTag=prod'","title":"S3"},{"location":"aws_cli/configure_aws/","text":"Verify aws cli is installed $ aws --version aws-cli/2.7.6 Python/3.9.11 Linux/5.15.0-56-generic exe/x86_64.ubuntu.20 prompt/off Configure aws cli. We must first give aws the access keys so we can use the aws api. Executing aws configure creates a config and credentials file in the home directory. aws configure AWS Access Key ID [ ****************4OQZ ] : AWS Secret Access Key [ ****************GJC+ ] : Default region name [ us-east-1 ] : Default output format [ json ] :","title":"Configure aws"},{"location":"aws_cli/s3_cli/","text":"AWS CLI version \u00b6 $ aws --version aws-cli/2.4.15 Python/3.8.8 Darwin/20.6.0 exe/x86_64 prompt/off List S3 buckets aws s3api list-buckets Check if bucket name available if this returns empty bucket is not there curl -sI https://<>.s3.amazonaws.com | grep bucket-region Create a S3 bucket \u00b6 SSH to AWS acct EC2 and create bucket \u00b6 assign role to EC2 for full s3 access. Make a bucket aws s3 mb s3://MY_BUCKET Copy text to bucket echo \"hello world\" > A_SAMPLE_TEXT.txt cat A_SAMPLE_TEXT.txt aws s3 cp A_SAMPLE_TEXT.txt s3://MY_BUCKET Create bucket \u00b6 Create a S3 bucket in the current region , you can create one via the aws cli : aws s3api create-bucket --bucket sagemaker-studio-cloudformation aws s3 mb s3://<your s3 bucket name> ## Enable versioning of bucket aws s3api put-bucket-versioning --bucket <your s3 bucket name> --versioning-configuration MFADelete = Disabled,Status = Enabled aws s3api put-bucket-versioning --bucket sagemaker-studio-cloudformation --versioning-configuration MFADelete = Disabled,Status = Enabled ## Check if versioning is enabled aws s3api get-bucket-versioning --bucket <your s3 bucket name> aws s3api get-bucket-versioning --bucket sagemaker-studio-cloudformation Enable versioning of bucket \u00b6 ## Enable versioning of bucket aws s3api put-bucket-versioning --bucket <your s3 bucket name> \\ --versioning-configuration MFADelete = Disabled,Status = Enabled aws s3api put-bucket-versioning --bucket sagemaker-studio-cloudformation --versioning-configuration MFADelete = Disabled,Status = Enabled ## Check if versioning is enabled aws s3api get-bucket-versioning --bucket <your s3 bucket name> aws s3api get-bucket-versioning --bucket sagemaker-studio-cloudformation Sync bucket \u00b6 aws s3 sync s3://bucket-name \"folder\" Delete S3 bucket \u00b6 ## Delete S3 bucket aws s3api delete-bucket --bucket <mybucket> ## Remove certain files aws s3 rm s3://bucket-name --recursive --exclude \"*\" --include \"file1\" --include \"file2\" --include \"file3\" --include \"etc\" ### Download and then delete aws s3 mv s3://bucket-name/ ./folder/ --recursive aws s3 rm s3://bucket-name --recursive aws s3api delete-bucket --bucket <mybucket> --region us-east-1 aws s3api delete-bucket --bucket sm-mlops-dse --region us-east-1 Enable Encryption \u00b6 aws s3api put-bucket-encryption --bucket bucket-name --server-side-encryption-configuration '{ \"Rules\": [ { \"ApplyServerSideEncryptionByDefault\": { \"SSEAlgorithm\": \"AES256\" } } ] }' aws s3api create-bucket --bucket <bucket name> --region <bucket region> Disable ACLs Block all public access Enable versioning enable server side encryption and use s3 managed keys aws s3api put-bucket-encryption --bucket <Bucket Name> --server-side-encryption-configuration '{\"Rules\": [{\"ApplyServerSideEncryptionByDefault\": {\"SSEAlgorithm\": \"AES256\"}}]}'","title":"S3 cli"},{"location":"aws_cli/s3_cli/#aws-cli-version","text":"$ aws --version aws-cli/2.4.15 Python/3.8.8 Darwin/20.6.0 exe/x86_64 prompt/off List S3 buckets aws s3api list-buckets Check if bucket name available if this returns empty bucket is not there curl -sI https://<>.s3.amazonaws.com | grep bucket-region","title":"AWS CLI version"},{"location":"aws_cli/s3_cli/#create-a-s3-bucket","text":"","title":"Create a S3 bucket"},{"location":"aws_cli/s3_cli/#ssh-to-aws-acct-ec2-and-create-bucket","text":"assign role to EC2 for full s3 access. Make a bucket aws s3 mb s3://MY_BUCKET Copy text to bucket echo \"hello world\" > A_SAMPLE_TEXT.txt cat A_SAMPLE_TEXT.txt aws s3 cp A_SAMPLE_TEXT.txt s3://MY_BUCKET","title":"SSH to AWS acct EC2 and create bucket"},{"location":"aws_cli/s3_cli/#create-bucket","text":"Create a S3 bucket in the current region , you can create one via the aws cli : aws s3api create-bucket --bucket sagemaker-studio-cloudformation aws s3 mb s3://<your s3 bucket name> ## Enable versioning of bucket aws s3api put-bucket-versioning --bucket <your s3 bucket name> --versioning-configuration MFADelete = Disabled,Status = Enabled aws s3api put-bucket-versioning --bucket sagemaker-studio-cloudformation --versioning-configuration MFADelete = Disabled,Status = Enabled ## Check if versioning is enabled aws s3api get-bucket-versioning --bucket <your s3 bucket name> aws s3api get-bucket-versioning --bucket sagemaker-studio-cloudformation","title":"Create bucket"},{"location":"aws_cli/s3_cli/#enable-versioning-of-bucket","text":"## Enable versioning of bucket aws s3api put-bucket-versioning --bucket <your s3 bucket name> \\ --versioning-configuration MFADelete = Disabled,Status = Enabled aws s3api put-bucket-versioning --bucket sagemaker-studio-cloudformation --versioning-configuration MFADelete = Disabled,Status = Enabled ## Check if versioning is enabled aws s3api get-bucket-versioning --bucket <your s3 bucket name> aws s3api get-bucket-versioning --bucket sagemaker-studio-cloudformation","title":"Enable versioning of bucket"},{"location":"aws_cli/s3_cli/#sync-bucket","text":"aws s3 sync s3://bucket-name \"folder\"","title":"Sync bucket"},{"location":"aws_cli/s3_cli/#delete-s3-bucket","text":"## Delete S3 bucket aws s3api delete-bucket --bucket <mybucket> ## Remove certain files aws s3 rm s3://bucket-name --recursive --exclude \"*\" --include \"file1\" --include \"file2\" --include \"file3\" --include \"etc\" ### Download and then delete aws s3 mv s3://bucket-name/ ./folder/ --recursive aws s3 rm s3://bucket-name --recursive aws s3api delete-bucket --bucket <mybucket> --region us-east-1 aws s3api delete-bucket --bucket sm-mlops-dse --region us-east-1","title":"Delete S3 bucket"},{"location":"aws_cli/s3_cli/#enable-encryption","text":"aws s3api put-bucket-encryption --bucket bucket-name --server-side-encryption-configuration '{ \"Rules\": [ { \"ApplyServerSideEncryptionByDefault\": { \"SSEAlgorithm\": \"AES256\" } } ] }' aws s3api create-bucket --bucket <bucket name> --region <bucket region> Disable ACLs Block all public access Enable versioning enable server side encryption and use s3 managed keys aws s3api put-bucket-encryption --bucket <Bucket Name> --server-side-encryption-configuration '{\"Rules\": [{\"ApplyServerSideEncryptionByDefault\": {\"SSEAlgorithm\": \"AES256\"}}]}'","title":"Enable Encryption"},{"location":"aws_cli/sagemaker/","text":"Delete a SageMaker domain aws --region Region sagemaker list-domains aws --region us-east-1 sagemaker list-domains Retrieve the list of applications for the Domain to be deleted. aws --region us-east-1 sagemaker list-apps \\ --domain-id-equals d-izlvf7ippuhv Delete each application in the list. aws --region us-east-1 sagemaker delete-app \\ --domain-id d-izlvf7ippuhv \\ --app-name AppName \\ --app-type AppType \\ --user-profile-name UserProfileName aws --region us-east-1 sagemaker delete-app \\ --domain-id d-izlvf7ippuhv \\ --user-profile-name madsen 8 . Delete the Domain. To also delete the Amazon EFS volume, specify HomeEfsFileSystem=Delete. aws --region us-east-1 sagemaker delete-domain \\ --domain-id d-izlvf7ippuhv \\ --retention-policy HomeEfsFileSystem = Delete","title":"Sagemaker"},{"location":"docker/poetry_docker/","text":"Install poetry in a Dockerfile. RUN pip3 --disable-pip-version-check --no-cache-dir install poetry Use entrypoint with poetry environment \u00b6 I use groups from the pyproject.toml , so modify MY_POETRY_GROUP_IN_TOML in the file below. FROM python:3.9.5 RUN apt-get update && apt-get install --no-install-recommends -y build-essential tree && \\ apt-get clean && rm -rf /var/lib/apt/lists/* ## ----- install poetry RUN curl -sSL https://install.python-poetry.org | python3 - --git https://github.com/python-poetry/poetry.git@master ENV PATH = \"/root/.local/bin: $PATH \" ## ------ Copy poetry environment to local and duplicate on remote COPY poetry.lock pyproject.toml ./ ## ----- create poetry environment RUN poetry config virtualenvs.create false RUN poetry install --without MY_POETRY_GROUP_IN_TOML RUN pip install --no-cache-dir -U pandas-gbq ## ------ Optional - Confirm Poetry is working # RUN poetry add pycowsay # RUN poetry run pycowsay 'Hello world!' ## entrypoint directory WORKDIR \"/usr/src\" COPY \"src/\" \"/usr/src\" # COPY . . RUN ls -la && pwd #&& tree -d ## ---- CLI can overwrite params # CMD [\"poetry\", \"run\", \"python3\", \"src/entrypoint_ingest.py\"] ## ---- CLI cannot overwrite params ENTRYPOINT [ \"python3\" , \"entrypoint_ingest.py\" ]","title":"Poetry environment"},{"location":"docker/poetry_docker/#use-entrypoint-with-poetry-environment","text":"I use groups from the pyproject.toml , so modify MY_POETRY_GROUP_IN_TOML in the file below. FROM python:3.9.5 RUN apt-get update && apt-get install --no-install-recommends -y build-essential tree && \\ apt-get clean && rm -rf /var/lib/apt/lists/* ## ----- install poetry RUN curl -sSL https://install.python-poetry.org | python3 - --git https://github.com/python-poetry/poetry.git@master ENV PATH = \"/root/.local/bin: $PATH \" ## ------ Copy poetry environment to local and duplicate on remote COPY poetry.lock pyproject.toml ./ ## ----- create poetry environment RUN poetry config virtualenvs.create false RUN poetry install --without MY_POETRY_GROUP_IN_TOML RUN pip install --no-cache-dir -U pandas-gbq ## ------ Optional - Confirm Poetry is working # RUN poetry add pycowsay # RUN poetry run pycowsay 'Hello world!' ## entrypoint directory WORKDIR \"/usr/src\" COPY \"src/\" \"/usr/src\" # COPY . . RUN ls -la && pwd #&& tree -d ## ---- CLI can overwrite params # CMD [\"poetry\", \"run\", \"python3\", \"src/entrypoint_ingest.py\"] ## ---- CLI cannot overwrite params ENTRYPOINT [ \"python3\" , \"entrypoint_ingest.py\" ]","title":"Use entrypoint with poetry environment"},{"location":"environment/Makefile/","text":"GNU Make \u00b6 Make is a project management tool for building executables and program files. It ensures our project is consistent across different environments, locally and remote server. This document provides a simple template for starting a project with make . Commands should be executed in project root, using make <command> . For example run make install to invoke python library install for project. In the makefile see this block: install: cat requirements.txt | xargs poetry add Makefile template \u00b6 SHELL : = /bin/bash # Target section and Global definitions # ----------------------------------------------------------------------------- .PHONY: all clean test install run deploy down all: clean test install run deploy down hello: @echo \"Hello World\" install: cat requirements.txt | xargs poetry add # poetry add tox setuptools #pip install --upgrade pip &&\\ # pip install -r requirements.txt install_dev: cat requirements_dev.txt | xargs poetry add --group dev post-install: python -m textblob.download_corpora git: # Update Git Repo bash sync_git.sh format: # format code using black black *.py mylib/*.py lint: # format code using pylint or flake8 pylint --disable = R,C *.py mylib/*.py test: # test dockerfile docker run --rm -i hadolint/hadolint < Dockerfile # test code python -m pytest -vv --cov = mylib --cov = main test_*.py build: #build container docker build -t deploy-fastapi . a-rule: some proxypass uuid = $$ ( docker-compose ps -q myService ) ; \\ docker cp \" $$ uuid\" :/a/b/c . run: #run docker docker run -p 127 .0.0.1:8080:8080 deploy-fastapi deploy: aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 073416988478 .dkr.ecr.us-east-1.amazonaws.com docker build -t mlops_scaffold . docker tag mlops_scaffold:latest 073416988478 .dkr.ecr.us-east-1.amazonaws.com/mlops_scaffold:latest docker push 073416988478 .dkr.ecr.us-east-1.amazonaws.com/mlops_scaffold:latest run: - python -c \"import subprocess;print(subprocess.run(['hostname','-I'],capture_output=True,text=True).stdout.strip())\" PYTHONPATH = huggingfastapi/ uvicorn huggingfastapi.main:app --reload --host 0 .0.0.0 build: docker-compose build deploy: docker-compose up -d down: docker-compose down clean: -find . -name '*.pyc' -exec rm -rf {} \\; -find . -name '__pycache__' -exec rm -rf {} \\; -find . -name 'Thumbs.db' -exec rm -rf {} \\; -find . -name '*~' -exec rm -rf {} \\; -rm -rf .cache -rm -rf build -rm -rf dist -rm -rf *.egg-info -rm -rf htmlcov* -rm -rf .tox/ -rm -rf docs/_build -rm -r .coverage","title":"Make Workflow"},{"location":"environment/Makefile/#gnu-make","text":"Make is a project management tool for building executables and program files. It ensures our project is consistent across different environments, locally and remote server. This document provides a simple template for starting a project with make . Commands should be executed in project root, using make <command> . For example run make install to invoke python library install for project. In the makefile see this block: install: cat requirements.txt | xargs poetry add","title":"GNU Make"},{"location":"environment/Makefile/#makefile-template","text":"SHELL : = /bin/bash # Target section and Global definitions # ----------------------------------------------------------------------------- .PHONY: all clean test install run deploy down all: clean test install run deploy down hello: @echo \"Hello World\" install: cat requirements.txt | xargs poetry add # poetry add tox setuptools #pip install --upgrade pip &&\\ # pip install -r requirements.txt install_dev: cat requirements_dev.txt | xargs poetry add --group dev post-install: python -m textblob.download_corpora git: # Update Git Repo bash sync_git.sh format: # format code using black black *.py mylib/*.py lint: # format code using pylint or flake8 pylint --disable = R,C *.py mylib/*.py test: # test dockerfile docker run --rm -i hadolint/hadolint < Dockerfile # test code python -m pytest -vv --cov = mylib --cov = main test_*.py build: #build container docker build -t deploy-fastapi . a-rule: some proxypass uuid = $$ ( docker-compose ps -q myService ) ; \\ docker cp \" $$ uuid\" :/a/b/c . run: #run docker docker run -p 127 .0.0.1:8080:8080 deploy-fastapi deploy: aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 073416988478 .dkr.ecr.us-east-1.amazonaws.com docker build -t mlops_scaffold . docker tag mlops_scaffold:latest 073416988478 .dkr.ecr.us-east-1.amazonaws.com/mlops_scaffold:latest docker push 073416988478 .dkr.ecr.us-east-1.amazonaws.com/mlops_scaffold:latest run: - python -c \"import subprocess;print(subprocess.run(['hostname','-I'],capture_output=True,text=True).stdout.strip())\" PYTHONPATH = huggingfastapi/ uvicorn huggingfastapi.main:app --reload --host 0 .0.0.0 build: docker-compose build deploy: docker-compose up -d down: docker-compose down clean: -find . -name '*.pyc' -exec rm -rf {} \\; -find . -name '__pycache__' -exec rm -rf {} \\; -find . -name 'Thumbs.db' -exec rm -rf {} \\; -find . -name '*~' -exec rm -rf {} \\; -rm -rf .cache -rm -rf build -rm -rf dist -rm -rf *.egg-info -rm -rf htmlcov* -rm -rf .tox/ -rm -rf docs/_build -rm -r .coverage","title":"Makefile template"},{"location":"environment/poetry_group_dependencies/","text":"poetry group dependencies \u00b6 Setup groups in poetry \u00b6 Add groups to toml [ tool.poetry.group.inference.dependencies ] jinja2 mkdocs [ tool.poetry.group.docs.dependencies ] pytorch pandas keras [ tool.poetry.group.docs ] optional = true Update library for groups. poetry add jinja2 mkdocs --group docs poetry add tensorflow numpy pandas h5py --group inference Install by group poetry install --without test,docs Invoke group deps in run time. \u00b6 Github actions example \u00b6 version : 2 build : os : ubuntu-20.04 tools : python : \"3.9\" jobs : post_create_environment : # Install poetry - pip install poetry # Tell poetry to not use a virtual environment - poetry config virtualenvs.create false post_install : # Install dependencies - poetry install --with docs sphinx : configuration : docs/source/conf.py","title":"Poetry Group Dependencies"},{"location":"environment/poetry_group_dependencies/#poetry-group-dependencies","text":"","title":"poetry group dependencies"},{"location":"environment/poetry_group_dependencies/#setup-groups-in-poetry","text":"Add groups to toml [ tool.poetry.group.inference.dependencies ] jinja2 mkdocs [ tool.poetry.group.docs.dependencies ] pytorch pandas keras [ tool.poetry.group.docs ] optional = true Update library for groups. poetry add jinja2 mkdocs --group docs poetry add tensorflow numpy pandas h5py --group inference Install by group poetry install --without test,docs","title":"Setup groups in poetry"},{"location":"environment/poetry_group_dependencies/#invoke-group-deps-in-run-time","text":"","title":"Invoke group deps in run time."},{"location":"environment/poetry_group_dependencies/#github-actions-example","text":"version : 2 build : os : ubuntu-20.04 tools : python : \"3.9\" jobs : post_create_environment : # Install poetry - pip install poetry # Tell poetry to not use a virtual environment - poetry config virtualenvs.create false post_install : # Install dependencies - poetry install --with docs sphinx : configuration : docs/source/conf.py","title":"Github actions example"},{"location":"environment/poetry_install/","text":"Poetry package manager \u00b6 Poetry helps you declare, manage and install dependencies of Python projects, ensuring you have the right stack everywhere. Personally , I use pyenv and poetry for production projects. These decouple system settings from individual projects. Features \u00b6 The pyproject.toml is a standardized file to manage the dependencies. poetry will also detect if you are inside a virtualenv and install the packages accordingly. So, poetry can be installed globally and used everywhere. Poetry is isolated from your system. poetry also comes with a full fledged dependency resolution library. Install \u00b6 Install on macOS. curl -sSL https://install.python-poetry.org | python3 - Retrieving Poetry metadata # Welcome to Poetry! This will download and install the latest version of Poetry, a dependency and package manager for Python. It will add the ` poetry ` command to Poetry 's bin directory, located at: /Users/elxsj/.local/bin You can uninstall at any time by executing this script with the --uninstall option, and these changes will be reverted. Installing Poetry (1.3.2): Installing Poetry To get started you need Poetry' s bin directory ( /Users/elxsj/.local/bin ) in your ` PATH ` environment variable. Add ` export PATH = \"/Users/elxsj/.local/bin: $PATH \" ` to your shell configuration file. Alternatively, you can call Poetry explicitly with ` /Users/elxsj/.local/bin/poetry ` . You can test that everything is set up by executing: ` poetry --version ` After install the default config output shows updates to $PATH and executable information. To configure your current shell run source $HOME /.poetry/env Verify Install works as expected \u00b6 $ poetry config --list cache-dir = \"/Users/elxsj/Library/Caches/pypoetry\" experimental.new-installer = true experimental.system-git-client = false installer.max-workers = null installer.no-binary = null installer.parallel = true virtualenvs.create = true virtualenvs.in-project = true virtualenvs.options.always-copy = false virtualenvs.options.no-pip = false virtualenvs.options.no-setuptools = false virtualenvs.options.system-site-packages = false virtualenvs.path = \"{cache-dir}/virtualenvs\" # /Users/elxsj/Library/Caches/pypoetry/virtualenvs virtualenvs.prefer-active-python = false virtualenvs.prompt = \"{project_name}-py{python_version}\" # Check Poetry Path $HOME /.poetry/bin End \u00b6","title":"Poetry Installation"},{"location":"environment/poetry_install/#poetry-package-manager","text":"Poetry helps you declare, manage and install dependencies of Python projects, ensuring you have the right stack everywhere. Personally , I use pyenv and poetry for production projects. These decouple system settings from individual projects.","title":"Poetry package manager"},{"location":"environment/poetry_install/#features","text":"The pyproject.toml is a standardized file to manage the dependencies. poetry will also detect if you are inside a virtualenv and install the packages accordingly. So, poetry can be installed globally and used everywhere. Poetry is isolated from your system. poetry also comes with a full fledged dependency resolution library.","title":"Features"},{"location":"environment/poetry_install/#install","text":"Install on macOS. curl -sSL https://install.python-poetry.org | python3 - Retrieving Poetry metadata # Welcome to Poetry! This will download and install the latest version of Poetry, a dependency and package manager for Python. It will add the ` poetry ` command to Poetry 's bin directory, located at: /Users/elxsj/.local/bin You can uninstall at any time by executing this script with the --uninstall option, and these changes will be reverted. Installing Poetry (1.3.2): Installing Poetry To get started you need Poetry' s bin directory ( /Users/elxsj/.local/bin ) in your ` PATH ` environment variable. Add ` export PATH = \"/Users/elxsj/.local/bin: $PATH \" ` to your shell configuration file. Alternatively, you can call Poetry explicitly with ` /Users/elxsj/.local/bin/poetry ` . You can test that everything is set up by executing: ` poetry --version ` After install the default config output shows updates to $PATH and executable information. To configure your current shell run source $HOME /.poetry/env","title":"Install"},{"location":"environment/poetry_install/#verify-install-works-as-expected","text":"$ poetry config --list cache-dir = \"/Users/elxsj/Library/Caches/pypoetry\" experimental.new-installer = true experimental.system-git-client = false installer.max-workers = null installer.no-binary = null installer.parallel = true virtualenvs.create = true virtualenvs.in-project = true virtualenvs.options.always-copy = false virtualenvs.options.no-pip = false virtualenvs.options.no-setuptools = false virtualenvs.options.system-site-packages = false virtualenvs.path = \"{cache-dir}/virtualenvs\" # /Users/elxsj/Library/Caches/pypoetry/virtualenvs virtualenvs.prefer-active-python = false virtualenvs.prompt = \"{project_name}-py{python_version}\" # Check Poetry Path $HOME /.poetry/bin","title":"Verify Install works as expected"},{"location":"environment/poetry_install/#end","text":"","title":"End"},{"location":"environment/poetry_new_project/","text":"Poetry package manager \u00b6 Poetry helps you declare, manage and install dependencies of Python projects, ensuring you have the right stack everywhere. Personally , I use pyenv and poetry for production projects. These decouple system settings from individual projects. Poetry Features \u00b6 The pyproject.toml is a standardized file to manage the dependencies. poetry will also detect if you are inside a virtualenv and install the packages accordingly. So, poetry can be installed globally and used everywhere. Poetry is isolated from your system. This is a really good thing, bc each OS is a unique environment. poetry also comes with a full fledged dependency resolution library. Create new poetry project \u00b6 Abbreviated: Create new project with poetry env \u00b6 This is the abbreviated directions for power users. Scroll past this section to see detailed explanations. poetry new test_env Use poetry init for existing project cd $_ pyenv local 3.9.1 This will update the .python_version file pyenv shell 3.9.1 poetry env use $(pyenv which python) modify python version, in pyproject.toml by hand if required. Show correct python version is being used - poetry env info open VSCode and the shell shows the correct environment - (test-env-py3.9) [elxsj@WUST040517:~/test_env]$ Activate Virtual Environment \u00b6 Open existing poetry environment, by runnning poetry shell The poetry environment will display in shell: (MY_ENV_py3.10) bash-3.2$ Verbose: Create new project with poetry env \u00b6 In a shell generate a default project scaffold poetry new poetry_env cd $_ switch into directory OR Simulate an existing project. Create directory and add pyproject.toml poetry new poetry-demo creates a directory for a new project. This is not required for an existing project. For an existing project run poetry init in the existing project directory and step through the interactive dialogue. mkdir ~/test_error && cd ~/test_error echo -e \"[tool.poetry] \\nname = 'smol-cls-mwaa-dags' \\nversion = '0.1.0' \\ndescription = '' \\nauthors = ['memadsen <michael.madsen@bayer.com>'] \\n\\n[tool.poetry.dependencies] \\npython = '^3.9' \\n\\n[tool.poetry.dev-dependencies] \\n\\n[build-system] \\nrequires = ['poetry-core>=1.0.0'] \\nbuild-backend = 'poetry.core.masonry.api'\" > pyproject.toml cd $_ switch into directory My directory scaffold has this structure $ tree . \u251c\u2500\u2500 README.md \u251c\u2500\u2500 poetry_env \u2502 \u2514\u2500\u2500 __init__.py \u251c\u2500\u2500 pyproject.toml \u2514\u2500\u2500 tests \u2514\u2500\u2500 __init__.py pyenv install 3.9.5 Optional, if previously installed pyenv local 3.9.1 Add python version This creates a .python_version file. cat .python-version 3 .9.1 poetry env use 3.9.1 set the Python used for the Poetry-managed environment, creating it if it doesn't already exist. envs.toml is created to record what the 'current interpreter' for the environment is (as the user has explicitly selected one, instead of letting Poetry select the interpreter implicitly) Notice the pyproject.toml . This is where we define everything from our project\u2019s metadata, dependencies, scripts, and more. If you\u2019re familiar with Node.js, consider the pyproject.toml as an equivalent of the Node.js package.json . poetry env use $(pyenv which python) modify python version, in pyproject.toml by hand if required. poetry install Install and activate the virtual environment Add python interpreter to path, required for Pycharm . \u00b6 Check poetry .toml file for virtualenvs \u00b6 My preference is to have all the logic to my virtualenv in one place, e.g the .toml file. [ tool.poetry ] name = \"microservice-aws-app-runner\" version = \"0.1.0\" description = \"\" authors = [ \"memadsen <michael.madsen@bayer.com>\" ] [ tool.poetry.dependencies ] python = \"^3.9\" [ tool.poetry.dev-dependencies ] [ build-system ] requires = [ \"poetry-core>=1.0.0\" ] build-backend = \"poetry.core.masonry.api\" [ virtualenvs ] create = true in -project = true Project Dependencies \u00b6 Track project dependencies using poetry show --tree . Next poetry show --tree requests-toolbelt 0 .8.0 A utility belt for advanced users... \u2514\u2500\u2500 requests < 3 .0.0,> = 2 .0.1 \u251c\u2500\u2500 certifi > = 2017 .4.17 \u251c\u2500\u2500 chardet > = 3 .0.2,< 3 .1.0 \u251c\u2500\u2500 idna > = 2 .5,< 2 .7 \u2514\u2500\u2500 urllib3 < 1 .23,> = 1 .21.1 $ poetry show --latest pendulum 2 .0.4 1 .4.5 Python datetimes made easy. django 1 .11.11 2 .0.3 A high-level Python Web framework ... requests 2 .18.4 2 .18.4 Python HTTP for Humans. Add libraries \u00b6 poetry add mlrun == 1 .0.2 ## Specify constraints when adding library poetry add pendulum@^2.0.5 poetry add \"pendulum>=2.0.5\" ## --- Add from requirements.txt poetry add ` cat requirements.txt ` Add local py modules \u00b6 [ tool.poetry.dependencies ] python = \"3.9.5\" merge_inference_results = { path = \"src/utils/MY_MODULE.py\" , develop = true } WUST040517@elxsj $ tree . \u251c\u2500\u2500 README.md \u251c\u2500\u2500 pyproject.toml \u2502 \u2514\u2500\u2500 utils \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 __pycache__ \u2502 \u2502 \u251c\u2500\u2500 __init__.cpython-310.pyc \u2502 \u2502 \u251c\u2500\u2500 auth.cpython-310.pyc \u2502 \u2502 \u251c\u2500\u2500 file_utils.cpython-310.pyc \u2502 \u2502 \u251c\u2500\u2500 get_vault_creds.cpython-310.pyc \u2502 \u2502 \u251c\u2500\u2500 utils.cpython-310.pyc \u2502 \u2502 \u2514\u2500\u2500 vault_app_role.cpython-310.pyc \u2502 \u251c\u2500\u2500 auth.py \u2502 \u251c\u2500\u2500 get_vault_creds.py \u2502 \u251c\u2500\u2500 merge_inference_results.py Options to add libraries \u00b6 --dev (-D) : Add package as development dependency. --path : The path to a dependency. --optional : Add as an optional dependency. --dry-run : Outputs the operations but will not execute anything (implicitly enables \u2013verbose). --lock : Do not perform install (only update the lockfile). Update libraries \u00b6 If you just want to update a few packages and not all, you can list them as such: poetry update requests toml Note that this will not update versions for dependencies outside their version constraints specified in the pyproject.toml file. In other terms, poetry update foo will be a no-op if the version constraint specified for foo is ~2.3 or 2.3 and 2.4 is available. In order for foo to be updated, you must update the constraint, for example ^2.3 . You can do this using the add command. Options \u00b6 --dry-run : Outputs the operations but will not execute anything (implicitly enables \u2013verbose). --no-dev : Do not install dev dependencies. --lock : Do not perform install (only update the lockfile). poetry lock --no-update This makes it possible to remove a dependency from pyproject.toml and update the lock file without upgrading dependencies. Dependency Group \u00b6 poetry add --group dev httpx Current Configuration \u00b6 List the current configuration. $ poetry config --list cache-dir = \"/Users/elxsj/Library/Caches/pypoetry\" experimental.new-installer = true experimental.system-git-client = false installer.max-workers = null installer.no-binary = null installer.parallel = true virtualenvs.create = true virtualenvs.in-project = null virtualenvs.options.always-copy = false virtualenvs.options.no-pip = false virtualenvs.options.no-setuptools = false virtualenvs.options.system-site-packages = false virtualenvs.path = \"{cache-dir}/virtualenvs\" # /Users/elxsj/Library/Caches/pypoetry/virtualenvs virtualenvs.prefer-active-python = false virtualenvs.prompt = \"{project_name}-py{python_version}\" Check the Path and Executable for this poetry environment: $ poetry env info System Platform: darwin OS: posix Python: 3 .10.0 Path: /Users/elxsj/.pyenv/versions/3.10.0 Executable: /Users/elxsj/.pyenv/versions/3.10.0/bin/python3.10 Modify Poetry virtual environment path \u00b6 Changes to the python version (using pyenv) require update to the path used for poetry virtual environment. Python Version using pyenv \u00b6 $ pyenv version 3 .9.5 ( set by ~/MY_PROJECT/.python-version ) Set python version for poetry \u00b6 $ pyenv local 3 .9.5 $ poetry env use 3 .9.5 ## Successfull output looks like this: Recreating virtualenv MY_PROJECT in ~/MY_PROJECT/.venv Using virtualenv: ~/MY_PROJECT/.venv Poetry Export \u00b6 If you have a Poetry project, you can create a requirements.txt file from your poetry.lock file: poetry export --output requirements.txt Export without hashes to speed up install. poetry export --without-hashes --format = requirements.txt > requirements.txt Poetry Commands \u00b6 Create a new project, create a virtual envrionment and add and remove dependencies using these commands: Command Description poetry new [package-name] Start a new Python Project. poetry init Create a pyproject.toml file interactively. poetry install Install the packages inside the pyproject.toml file. poetry add [package-name] Add a package to a Virtual Environment. poetry add -D [package-name] Add a dev package to a Virtual Environment. poetry remove [package-name] Remove a package from a Virtual Environment. poetry remove -D [package-name] Remove a dev package from a Virtual Environment. poetry self:update Update poetry to the latest stable version. Environment \u00b6 poetry env list List all environments poetry env remove env-name Remove environment Troubleshooting \u00b6 Some things to try if poetry has unusual errors. poetry cache clear --all . Clear cache rm - f ./ poetry . lock poetry install -- remove - untracked References \u00b6 Poetry github https://github.com/python-poetry/poetry","title":"Poetry New Project"},{"location":"environment/poetry_new_project/#poetry-package-manager","text":"Poetry helps you declare, manage and install dependencies of Python projects, ensuring you have the right stack everywhere. Personally , I use pyenv and poetry for production projects. These decouple system settings from individual projects.","title":"Poetry package manager"},{"location":"environment/poetry_new_project/#poetry-features","text":"The pyproject.toml is a standardized file to manage the dependencies. poetry will also detect if you are inside a virtualenv and install the packages accordingly. So, poetry can be installed globally and used everywhere. Poetry is isolated from your system. This is a really good thing, bc each OS is a unique environment. poetry also comes with a full fledged dependency resolution library.","title":"Poetry Features"},{"location":"environment/poetry_new_project/#create-new-poetry-project","text":"","title":"Create new poetry project"},{"location":"environment/poetry_new_project/#abbreviated-create-new-project-with-poetry-env","text":"This is the abbreviated directions for power users. Scroll past this section to see detailed explanations. poetry new test_env Use poetry init for existing project cd $_ pyenv local 3.9.1 This will update the .python_version file pyenv shell 3.9.1 poetry env use $(pyenv which python) modify python version, in pyproject.toml by hand if required. Show correct python version is being used - poetry env info open VSCode and the shell shows the correct environment - (test-env-py3.9) [elxsj@WUST040517:~/test_env]$","title":"Abbreviated:  Create new project with poetry env"},{"location":"environment/poetry_new_project/#activate-virtual-environment","text":"Open existing poetry environment, by runnning poetry shell The poetry environment will display in shell: (MY_ENV_py3.10) bash-3.2$","title":"Activate Virtual Environment"},{"location":"environment/poetry_new_project/#verbose-create-new-project-with-poetry-env","text":"In a shell generate a default project scaffold poetry new poetry_env cd $_ switch into directory OR Simulate an existing project. Create directory and add pyproject.toml poetry new poetry-demo creates a directory for a new project. This is not required for an existing project. For an existing project run poetry init in the existing project directory and step through the interactive dialogue. mkdir ~/test_error && cd ~/test_error echo -e \"[tool.poetry] \\nname = 'smol-cls-mwaa-dags' \\nversion = '0.1.0' \\ndescription = '' \\nauthors = ['memadsen <michael.madsen@bayer.com>'] \\n\\n[tool.poetry.dependencies] \\npython = '^3.9' \\n\\n[tool.poetry.dev-dependencies] \\n\\n[build-system] \\nrequires = ['poetry-core>=1.0.0'] \\nbuild-backend = 'poetry.core.masonry.api'\" > pyproject.toml cd $_ switch into directory My directory scaffold has this structure $ tree . \u251c\u2500\u2500 README.md \u251c\u2500\u2500 poetry_env \u2502 \u2514\u2500\u2500 __init__.py \u251c\u2500\u2500 pyproject.toml \u2514\u2500\u2500 tests \u2514\u2500\u2500 __init__.py pyenv install 3.9.5 Optional, if previously installed pyenv local 3.9.1 Add python version This creates a .python_version file. cat .python-version 3 .9.1 poetry env use 3.9.1 set the Python used for the Poetry-managed environment, creating it if it doesn't already exist. envs.toml is created to record what the 'current interpreter' for the environment is (as the user has explicitly selected one, instead of letting Poetry select the interpreter implicitly) Notice the pyproject.toml . This is where we define everything from our project\u2019s metadata, dependencies, scripts, and more. If you\u2019re familiar with Node.js, consider the pyproject.toml as an equivalent of the Node.js package.json . poetry env use $(pyenv which python) modify python version, in pyproject.toml by hand if required. poetry install Install and activate the virtual environment","title":"Verbose:  Create new project with poetry env"},{"location":"environment/poetry_new_project/#add-python-interpreter-to-path-required-for-pycharm","text":"","title":"Add python interpreter to path, required for Pycharm."},{"location":"environment/poetry_new_project/#check-poetry-toml-file-for-virtualenvs","text":"My preference is to have all the logic to my virtualenv in one place, e.g the .toml file. [ tool.poetry ] name = \"microservice-aws-app-runner\" version = \"0.1.0\" description = \"\" authors = [ \"memadsen <michael.madsen@bayer.com>\" ] [ tool.poetry.dependencies ] python = \"^3.9\" [ tool.poetry.dev-dependencies ] [ build-system ] requires = [ \"poetry-core>=1.0.0\" ] build-backend = \"poetry.core.masonry.api\" [ virtualenvs ] create = true in -project = true","title":"Check poetry .toml file for virtualenvs"},{"location":"environment/poetry_new_project/#project-dependencies","text":"Track project dependencies using poetry show --tree . Next poetry show --tree requests-toolbelt 0 .8.0 A utility belt for advanced users... \u2514\u2500\u2500 requests < 3 .0.0,> = 2 .0.1 \u251c\u2500\u2500 certifi > = 2017 .4.17 \u251c\u2500\u2500 chardet > = 3 .0.2,< 3 .1.0 \u251c\u2500\u2500 idna > = 2 .5,< 2 .7 \u2514\u2500\u2500 urllib3 < 1 .23,> = 1 .21.1 $ poetry show --latest pendulum 2 .0.4 1 .4.5 Python datetimes made easy. django 1 .11.11 2 .0.3 A high-level Python Web framework ... requests 2 .18.4 2 .18.4 Python HTTP for Humans.","title":"Project Dependencies"},{"location":"environment/poetry_new_project/#add-libraries","text":"poetry add mlrun == 1 .0.2 ## Specify constraints when adding library poetry add pendulum@^2.0.5 poetry add \"pendulum>=2.0.5\" ## --- Add from requirements.txt poetry add ` cat requirements.txt `","title":"Add libraries"},{"location":"environment/poetry_new_project/#add-local-py-modules","text":"[ tool.poetry.dependencies ] python = \"3.9.5\" merge_inference_results = { path = \"src/utils/MY_MODULE.py\" , develop = true } WUST040517@elxsj $ tree . \u251c\u2500\u2500 README.md \u251c\u2500\u2500 pyproject.toml \u2502 \u2514\u2500\u2500 utils \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 __pycache__ \u2502 \u2502 \u251c\u2500\u2500 __init__.cpython-310.pyc \u2502 \u2502 \u251c\u2500\u2500 auth.cpython-310.pyc \u2502 \u2502 \u251c\u2500\u2500 file_utils.cpython-310.pyc \u2502 \u2502 \u251c\u2500\u2500 get_vault_creds.cpython-310.pyc \u2502 \u2502 \u251c\u2500\u2500 utils.cpython-310.pyc \u2502 \u2502 \u2514\u2500\u2500 vault_app_role.cpython-310.pyc \u2502 \u251c\u2500\u2500 auth.py \u2502 \u251c\u2500\u2500 get_vault_creds.py \u2502 \u251c\u2500\u2500 merge_inference_results.py","title":"Add local py modules"},{"location":"environment/poetry_new_project/#options-to-add-libraries","text":"--dev (-D) : Add package as development dependency. --path : The path to a dependency. --optional : Add as an optional dependency. --dry-run : Outputs the operations but will not execute anything (implicitly enables \u2013verbose). --lock : Do not perform install (only update the lockfile).","title":"Options to add libraries"},{"location":"environment/poetry_new_project/#update-libraries","text":"If you just want to update a few packages and not all, you can list them as such: poetry update requests toml Note that this will not update versions for dependencies outside their version constraints specified in the pyproject.toml file. In other terms, poetry update foo will be a no-op if the version constraint specified for foo is ~2.3 or 2.3 and 2.4 is available. In order for foo to be updated, you must update the constraint, for example ^2.3 . You can do this using the add command.","title":"Update libraries"},{"location":"environment/poetry_new_project/#options","text":"--dry-run : Outputs the operations but will not execute anything (implicitly enables \u2013verbose). --no-dev : Do not install dev dependencies. --lock : Do not perform install (only update the lockfile). poetry lock --no-update This makes it possible to remove a dependency from pyproject.toml and update the lock file without upgrading dependencies.","title":"Options"},{"location":"environment/poetry_new_project/#dependency-group","text":"poetry add --group dev httpx","title":"Dependency Group"},{"location":"environment/poetry_new_project/#current-configuration","text":"List the current configuration. $ poetry config --list cache-dir = \"/Users/elxsj/Library/Caches/pypoetry\" experimental.new-installer = true experimental.system-git-client = false installer.max-workers = null installer.no-binary = null installer.parallel = true virtualenvs.create = true virtualenvs.in-project = null virtualenvs.options.always-copy = false virtualenvs.options.no-pip = false virtualenvs.options.no-setuptools = false virtualenvs.options.system-site-packages = false virtualenvs.path = \"{cache-dir}/virtualenvs\" # /Users/elxsj/Library/Caches/pypoetry/virtualenvs virtualenvs.prefer-active-python = false virtualenvs.prompt = \"{project_name}-py{python_version}\" Check the Path and Executable for this poetry environment: $ poetry env info System Platform: darwin OS: posix Python: 3 .10.0 Path: /Users/elxsj/.pyenv/versions/3.10.0 Executable: /Users/elxsj/.pyenv/versions/3.10.0/bin/python3.10","title":"Current Configuration"},{"location":"environment/poetry_new_project/#modify-poetry-virtual-environment-path","text":"Changes to the python version (using pyenv) require update to the path used for poetry virtual environment.","title":"Modify Poetry virtual environment path"},{"location":"environment/poetry_new_project/#python-version-using-pyenv","text":"$ pyenv version 3 .9.5 ( set by ~/MY_PROJECT/.python-version )","title":"Python Version using pyenv"},{"location":"environment/poetry_new_project/#set-python-version-for-poetry","text":"$ pyenv local 3 .9.5 $ poetry env use 3 .9.5 ## Successfull output looks like this: Recreating virtualenv MY_PROJECT in ~/MY_PROJECT/.venv Using virtualenv: ~/MY_PROJECT/.venv","title":"Set python version for poetry"},{"location":"environment/poetry_new_project/#poetry-export","text":"If you have a Poetry project, you can create a requirements.txt file from your poetry.lock file: poetry export --output requirements.txt Export without hashes to speed up install. poetry export --without-hashes --format = requirements.txt > requirements.txt","title":"Poetry Export"},{"location":"environment/poetry_new_project/#poetry-commands","text":"Create a new project, create a virtual envrionment and add and remove dependencies using these commands: Command Description poetry new [package-name] Start a new Python Project. poetry init Create a pyproject.toml file interactively. poetry install Install the packages inside the pyproject.toml file. poetry add [package-name] Add a package to a Virtual Environment. poetry add -D [package-name] Add a dev package to a Virtual Environment. poetry remove [package-name] Remove a package from a Virtual Environment. poetry remove -D [package-name] Remove a dev package from a Virtual Environment. poetry self:update Update poetry to the latest stable version.","title":"Poetry Commands"},{"location":"environment/poetry_new_project/#environment","text":"poetry env list List all environments poetry env remove env-name Remove environment","title":"Environment"},{"location":"environment/poetry_new_project/#troubleshooting","text":"Some things to try if poetry has unusual errors. poetry cache clear --all . Clear cache rm - f ./ poetry . lock poetry install -- remove - untracked","title":"Troubleshooting"},{"location":"environment/poetry_new_project/#references","text":"Poetry github https://github.com/python-poetry/poetry","title":"References"},{"location":"environment/proj_environment/","text":"Setup Environment for New Project \u00b6 At the start of this project clone the environment from git and setup the virtual environment. These are one time configurations, only required to run 1x at the beginning of a new project. How to use: Open VSCode or IDE of choice Configure environment For a new project we have to configure a local environment. Skip this if not required. Local Environment Setup \u00b6 Next setup python version in project directory cd ~/ pyenv install 3 .8.10 ## install python version for this project pyenv local 3 .8.10 ## make this available locally in this directory Initialize poetry for this (existing) project. poetry init ## follow the prompt to setup new project poetry install ## install packages and create virtualenv Set vscode python interpreter in VSCode. \u00b6 The python interpreter path must be set using the poetry env path. $ poetry env info --path Save the path from this output, it should look like this: /Users/elxsj/Library/Caches/pypoetry/virtualenvs/gh-pages-_kCswBDT-py3.10 Each IDE has a similar process to setting the python interpreter path. In VSCode, we set the python interpreter path by navigating to VSCode command pallete, e.g. View >> command pallete and search for python interpreter. Insert the path saved in the last step and open a new shell to verify the poetry environment is working. Manually invoke this environment in the project; source /Users/elxsj/Library/Caches/pypoetry/virtualenvs/gh-pages-_kCswBDT-py3.10/bin/activate Next, confirm the virualenvs is declared in the .toml file for the poetry environment. [ virtualenvs ] create = true in -project = true Install python libraries from requirements.txt make install Docker \u00b6 Build docker container from laptop. The build step takes ~15 minutes the first time. Run the contatiner to make sure the predictions are working as expected. docker build -t local-fastapi:v0.1 -f Dockerfile_pip . ## docker run -d --name mycontainer -p 80:80 myimage docker run --rm --name local-fastapi -p 8000 :8000 local-fastapi docker run -d --name hf_ghcr -p 8000 :8000 local-fastapi GH Actions \u00b6 Github Actions uses GH container registry to store the docker image.","title":"Environment Setup"},{"location":"environment/proj_environment/#setup-environment-for-new-project","text":"At the start of this project clone the environment from git and setup the virtual environment. These are one time configurations, only required to run 1x at the beginning of a new project. How to use: Open VSCode or IDE of choice Configure environment For a new project we have to configure a local environment. Skip this if not required.","title":"Setup Environment for New Project"},{"location":"environment/proj_environment/#local-environment-setup","text":"Next setup python version in project directory cd ~/ pyenv install 3 .8.10 ## install python version for this project pyenv local 3 .8.10 ## make this available locally in this directory Initialize poetry for this (existing) project. poetry init ## follow the prompt to setup new project poetry install ## install packages and create virtualenv","title":"Local Environment Setup"},{"location":"environment/proj_environment/#set-vscode-python-interpreter-in-vscode","text":"The python interpreter path must be set using the poetry env path. $ poetry env info --path Save the path from this output, it should look like this: /Users/elxsj/Library/Caches/pypoetry/virtualenvs/gh-pages-_kCswBDT-py3.10 Each IDE has a similar process to setting the python interpreter path. In VSCode, we set the python interpreter path by navigating to VSCode command pallete, e.g. View >> command pallete and search for python interpreter. Insert the path saved in the last step and open a new shell to verify the poetry environment is working. Manually invoke this environment in the project; source /Users/elxsj/Library/Caches/pypoetry/virtualenvs/gh-pages-_kCswBDT-py3.10/bin/activate Next, confirm the virualenvs is declared in the .toml file for the poetry environment. [ virtualenvs ] create = true in -project = true Install python libraries from requirements.txt make install","title":"Set vscode python interpreter in VSCode."},{"location":"environment/proj_environment/#docker","text":"Build docker container from laptop. The build step takes ~15 minutes the first time. Run the contatiner to make sure the predictions are working as expected. docker build -t local-fastapi:v0.1 -f Dockerfile_pip . ## docker run -d --name mycontainer -p 80:80 myimage docker run --rm --name local-fastapi -p 8000 :8000 local-fastapi docker run -d --name hf_ghcr -p 8000 :8000 local-fastapi","title":"Docker"},{"location":"environment/proj_environment/#gh-actions","text":"Github Actions uses GH container registry to store the docker image.","title":"GH Actions"},{"location":"environment/pyenv_cheatsheet/","text":"PYENV \u00b6 Install pyenv \u00b6 brew install pyenv brew install pyenv pyenv-virtualenv Next we update bash_profile export PATH = \" $HOME /.pyenv/bin: $PATH \" eval \" $( pyenv init - ) \" eval \" $( pyenv virtualenv-init - ) \" eval \" $( pyenv init -path ) \" eval \" $( pyenv virtualenv-init - ) \" alias brew = 'env PATH=\"${PATH//$(pyenv root)\\/shims:/}\" brew' Restart shell so changes take effect: exec \"$SHELL\" Tutorials \u00b6 Working With Multiple Environments https://realpython.com/intro-to-pyenv/#working-with-multiple-environments Managing multiple python versions with pyenv https://realpython.com/intro-to-pyenv/ https://wilsonmar.github.io/pyenv/ https://switowski.com/blog/pyenv https://unop.uk/python-pyenv-shims-not-in-path-fix/ List all available python versions \u00b6 You have many versions of Python to choose from. The list of all versions is long, however to see only the available CPython 3.6 through 3.8, you can do this: pyenv install --list | grep \" 3\\.[678]\" Create pyenv local project for aws-sso \u00b6 pyenv local changes the Python version only for the current folder and all the subfolders. That\u2019s exactly what you want for your project - you want to use a different Python version in this folder without changing the global one. pyenv local command creates a .python-version file in the current directory and puts the version number inside. When pyenv tries to determine what Python version it should use, it will search for that file in the current folder and all the parent folders. If it finds one, it uses the version specified in that file. And if it gets all the way up to your home folder without finding the .python-version , it will use the global version. cd ~/aws-sso pyenv install 3.8.10 pyenv local 3.8.10 This will create a .python-version file in the folder indicating the current local Python version for the project. Also, if you run python -V in that folder, you will see the local version, and not the global one. verify python version \u00b6 python -V 3 .8.10 Verify all versions installed to laptop \u00b6 ls ~/.pyenv/versions 3 .8.10 3 .9.0 pyenv versions system * 3 .8.10 ( set by /Users/elxsj/aws_sso/.python-version ) 3 .9.0 pyenv versions pyenv shell 3 .8.10 pyenv shell pyenv which pip3 /Users/elxsj/.pyenv/versions/3.8.10/bin/pip3 ENV var \u00b6 Verify $PYENV_VERSION environment variable: echo $PYENV_VERSION pip freeze > requirements.txt # list libraries installed pip install -r requirements.txt # import libraries","title":"Pyenv"},{"location":"environment/pyenv_cheatsheet/#pyenv","text":"","title":"PYENV"},{"location":"environment/pyenv_cheatsheet/#install-pyenv","text":"brew install pyenv brew install pyenv pyenv-virtualenv Next we update bash_profile export PATH = \" $HOME /.pyenv/bin: $PATH \" eval \" $( pyenv init - ) \" eval \" $( pyenv virtualenv-init - ) \" eval \" $( pyenv init -path ) \" eval \" $( pyenv virtualenv-init - ) \" alias brew = 'env PATH=\"${PATH//$(pyenv root)\\/shims:/}\" brew' Restart shell so changes take effect: exec \"$SHELL\"","title":"Install pyenv"},{"location":"environment/pyenv_cheatsheet/#tutorials","text":"Working With Multiple Environments https://realpython.com/intro-to-pyenv/#working-with-multiple-environments Managing multiple python versions with pyenv https://realpython.com/intro-to-pyenv/ https://wilsonmar.github.io/pyenv/ https://switowski.com/blog/pyenv https://unop.uk/python-pyenv-shims-not-in-path-fix/","title":"Tutorials"},{"location":"environment/pyenv_cheatsheet/#list-all-available-python-versions","text":"You have many versions of Python to choose from. The list of all versions is long, however to see only the available CPython 3.6 through 3.8, you can do this: pyenv install --list | grep \" 3\\.[678]\"","title":"List all available python versions"},{"location":"environment/pyenv_cheatsheet/#create-pyenv-local-project-for-aws-sso","text":"pyenv local changes the Python version only for the current folder and all the subfolders. That\u2019s exactly what you want for your project - you want to use a different Python version in this folder without changing the global one. pyenv local command creates a .python-version file in the current directory and puts the version number inside. When pyenv tries to determine what Python version it should use, it will search for that file in the current folder and all the parent folders. If it finds one, it uses the version specified in that file. And if it gets all the way up to your home folder without finding the .python-version , it will use the global version. cd ~/aws-sso pyenv install 3.8.10 pyenv local 3.8.10 This will create a .python-version file in the folder indicating the current local Python version for the project. Also, if you run python -V in that folder, you will see the local version, and not the global one.","title":"Create pyenv local project for aws-sso"},{"location":"environment/pyenv_cheatsheet/#verify-python-version","text":"python -V 3 .8.10","title":"verify python version"},{"location":"environment/pyenv_cheatsheet/#verify-all-versions-installed-to-laptop","text":"ls ~/.pyenv/versions 3 .8.10 3 .9.0 pyenv versions system * 3 .8.10 ( set by /Users/elxsj/aws_sso/.python-version ) 3 .9.0 pyenv versions pyenv shell 3 .8.10 pyenv shell pyenv which pip3 /Users/elxsj/.pyenv/versions/3.8.10/bin/pip3","title":"Verify all versions installed to laptop"},{"location":"environment/pyenv_cheatsheet/#env-var","text":"Verify $PYENV_VERSION environment variable: echo $PYENV_VERSION pip freeze > requirements.txt # list libraries installed pip install -r requirements.txt # import libraries","title":"ENV var"},{"location":"environment/pyproject_toml/","text":"Simple pyproject.toml \u00b6 An example .toml for use in poetry virtual environments. [ tool.poetry ] name = \"huggingface_ghcr\" version = \"0.1.0\" description = \"\" authors = [\"memadsen <michael.madsen@bayer.com>\"] [tool.poetry.dependencies] python = \"^3.8\" transformers = \"4.20.1\" fastapi = \"0.78.0\" uvicorn = {extras = [\"standard\"], version = \"^0.18.3\"} tensorflow = \"2.10.0\" [tool.poetry.dev-dependencies] [build-system] requires = [\"poetry-core>=1.0.0\"] build-backend = \"poetry.core.masonry.api\" [virtualenvs] create = true in-project = true","title":"Poetry toml"},{"location":"environment/pyproject_toml/#simple-pyprojecttoml","text":"An example .toml for use in poetry virtual environments. [ tool.poetry ] name = \"huggingface_ghcr\" version = \"0.1.0\" description = \"\" authors = [\"memadsen <michael.madsen@bayer.com>\"] [tool.poetry.dependencies] python = \"^3.8\" transformers = \"4.20.1\" fastapi = \"0.78.0\" uvicorn = {extras = [\"standard\"], version = \"^0.18.3\"} tensorflow = \"2.10.0\" [tool.poetry.dev-dependencies] [build-system] requires = [\"poetry-core>=1.0.0\"] build-backend = \"poetry.core.masonry.api\" [virtualenvs] create = true in-project = true","title":"Simple pyproject.toml"},{"location":"environment/python/","text":"python path \u00b6 Where does python live on my laptop? which python3 /usr/local/bin/python3 ## ---- This is a symbolic link ## ---- Locate actual python3 executable ls -al /usr/local/bin/python3 lrwxr-xr-x 1 elxsj admin 40 Oct 10 20 :15 /usr/local/bin/python3 -> ../Cellar/python@3.10/3.10.7/bin/python3 Application Main \u00b6 def main (): print ( \"hello, world\" ) if __name__ == '__main__' main ()","title":"Python"},{"location":"environment/python/#python-path","text":"Where does python live on my laptop? which python3 /usr/local/bin/python3 ## ---- This is a symbolic link ## ---- Locate actual python3 executable ls -al /usr/local/bin/python3 lrwxr-xr-x 1 elxsj admin 40 Oct 10 20 :15 /usr/local/bin/python3 -> ../Cellar/python@3.10/3.10.7/bin/python3","title":"python path"},{"location":"environment/python/#application-main","text":"def main (): print ( \"hello, world\" ) if __name__ == '__main__' main ()","title":"Application Main"},{"location":"environment/vscode/","text":"settings \u00b6 In project root navigate to .vscode directory and open settings.json { \"python.pythonPath\" : \"/Users/elxsj/Library/Caches/pypoetry/virtualenvs/poetry-pyenv-uqIKaYYi-py3.10\" }","title":"VS Code"},{"location":"environment/vscode/#settings","text":"In project root navigate to .vscode directory and open settings.json { \"python.pythonPath\" : \"/Users/elxsj/Library/Caches/pypoetry/virtualenvs/poetry-pyenv-uqIKaYYi-py3.10\" }","title":"settings"},{"location":"gh_actions/broken_links_report/","text":"Report Broken URL Links \u00b6 Technical documentation requires URL links. These links provide essential support for robust programmatic methods. Broken URL links are easy to test and fix. This is a great use case for unit testing. Unit testing evaluates atomic elements of code; the objective is to isolate discrete parts of the code for testing. The devdocs static HTML site is a great use case for unit testing. There are hundreds of files and each file can contain multiple URLs. Manual editing to fix will never happen, so automation summarizes the work for efficiency. Also, project management tools are not productive for this problem. Summary \u00b6 Enable github repo to test for broken url links Review report Read the report for our team devdocs github repo: https://github.com/bayer-int/smol-cls-cloud-docs navigate to issues Open the issue with the report, showing something similar to \"Link Checker Report\". The summary shows the total URL links reviewed \"Total\" (1,166) and \"Errors\" (115). Each markdown file is displayed with URL errors. The network error should also be displayed. Developers can search for their individual files and make corrections.","title":"Report Broken URL Links"},{"location":"gh_actions/broken_links_report/#report-broken-url-links","text":"Technical documentation requires URL links. These links provide essential support for robust programmatic methods. Broken URL links are easy to test and fix. This is a great use case for unit testing. Unit testing evaluates atomic elements of code; the objective is to isolate discrete parts of the code for testing. The devdocs static HTML site is a great use case for unit testing. There are hundreds of files and each file can contain multiple URLs. Manual editing to fix will never happen, so automation summarizes the work for efficiency. Also, project management tools are not productive for this problem.","title":"Report Broken URL Links"},{"location":"gh_actions/broken_links_report/#summary","text":"Enable github repo to test for broken url links Review report Read the report for our team devdocs github repo: https://github.com/bayer-int/smol-cls-cloud-docs navigate to issues Open the issue with the report, showing something similar to \"Link Checker Report\". The summary shows the total URL links reviewed \"Total\" (1,166) and \"Errors\" (115). Each markdown file is displayed with URL errors. The network error should also be displayed. Developers can search for their individual files and make corrections.","title":"Summary"},{"location":"gh_actions/build-push-image/","text":"Github Actions Build and Push Image to Repository Package Manager \u00b6 No PAT required. This github action uses the GITHUB_TOKEN , available to each repository. # Checkout the files from the Git repository. # Login to the ghcr.io container registry. # Setup Docker # Get metadata for use later in Docker. This avoids having to do manual work to set up the tags and labels for the Docker images. # Finally, build the image and push it. The build and push has two steps name: Docker Build & Publish to GitHub Container Registry on: push: branches: - 'main' # anything under a build/ folder will be used as testing the build processes. # - 'build/*' tags: - 'v*' workflow_dispatch: inputs: git-ref: description: Git Ref (Optional) required: false env: REGISTRY: ghcr.io IMAGE_NAME: ${{ github.repository }} jobs: build-and-push-docker-image: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - name: Log into registry ${{ env.REGISTRY }} uses: docker/login-action@v1 with: registry: ${{ env.REGISTRY }} username: ${{ github.actor }} password: ${{ secrets.GITHUB_TOKEN }} - name: Setup Docker buildx uses: docker/setup-buildx-action@v1 - name: Extract Docker metadata id: meta uses: docker/metadata-action@v2 with: images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }} # - name: Build and Push Versioned Docker Image # id: build-and-push # uses: docker/build-push-action@v2 # if: ${{ github.ref != 'refs/heads/main' }} # with: # context: ./build-push-image # push: true # tags: ${{ steps.meta.outputs.tags }} # labels: ${{ steps.meta.outputs.labels }} - name: Build and Push Latest Docker Image id: build-and-push-latest uses: docker/build-push-action@v2 if: ${{ github.ref == 'refs/heads/main' }} with: context: ./build-push-image push: true tags: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest labels: ${{ steps.meta.outputs.labels }}","title":"Build and Push Image to ghcr"},{"location":"gh_actions/build-push-image/#github-actions-build-and-push-image-to-repository-package-manager","text":"No PAT required. This github action uses the GITHUB_TOKEN , available to each repository. # Checkout the files from the Git repository. # Login to the ghcr.io container registry. # Setup Docker # Get metadata for use later in Docker. This avoids having to do manual work to set up the tags and labels for the Docker images. # Finally, build the image and push it. The build and push has two steps name: Docker Build & Publish to GitHub Container Registry on: push: branches: - 'main' # anything under a build/ folder will be used as testing the build processes. # - 'build/*' tags: - 'v*' workflow_dispatch: inputs: git-ref: description: Git Ref (Optional) required: false env: REGISTRY: ghcr.io IMAGE_NAME: ${{ github.repository }} jobs: build-and-push-docker-image: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - name: Log into registry ${{ env.REGISTRY }} uses: docker/login-action@v1 with: registry: ${{ env.REGISTRY }} username: ${{ github.actor }} password: ${{ secrets.GITHUB_TOKEN }} - name: Setup Docker buildx uses: docker/setup-buildx-action@v1 - name: Extract Docker metadata id: meta uses: docker/metadata-action@v2 with: images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }} # - name: Build and Push Versioned Docker Image # id: build-and-push # uses: docker/build-push-action@v2 # if: ${{ github.ref != 'refs/heads/main' }} # with: # context: ./build-push-image # push: true # tags: ${{ steps.meta.outputs.tags }} # labels: ${{ steps.meta.outputs.labels }} - name: Build and Push Latest Docker Image id: build-and-push-latest uses: docker/build-push-action@v2 if: ${{ github.ref == 'refs/heads/main' }} with: context: ./build-push-image push: true tags: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest labels: ${{ steps.meta.outputs.labels }}","title":"Github Actions Build and Push Image to Repository Package Manager"},{"location":"gh_actions/cachetest/","text":"Caching Docker builds in github actions: Which approach is fastest? \u00b6 https://dev.to/dtinth/caching-docker-builds-in-github-actions-which-approach-is-the-fastest-a-research-18ei Abstract: \u00b6 In this post, I experimented with 6 different approaches for caching Docker builds in GitHub Actions to speed up the build process and compared the results. After trying out every approach, 10 times each, the results show that using GitHub Packages\u2019 Docker registry as a build cache, as opposed to GitHub Actions\u2019 built-in cache, yields the highest performance gain. Unlike self-hosted runners like Jenkins, most cloud-hosted build runners are stateless, providing us with a pristine environment each run. We cannot keep files from the previous runs around; anything that needs to be persisted must be externalized. GitHub Actions has a built-in cache to help do this. But there are many ways of creating that cache ( docker save and docker load first comes to mind). Will the performance gains outweight the overhead caused by saving and loading caches? Are there more approaches other than using GitHub Action\u2019s built-in cache? That\u2019s what this research is about. Dockerfile \u00b6 FROM node:14.21.2 RUN yarn create react-app my-react-app RUN cd my-react-app && yarn build RUN npm install -g @vue/cli && ( yes | vue create my-vue-app --default ) RUN cd my-vue-app && yarn build RUN mkdir -p my-tests && cd my-tests && yarn add playwright # test # test # test # test # test # test # test # test # test # test","title":"Cache Testing"},{"location":"gh_actions/cachetest/#caching-docker-builds-in-github-actions-which-approach-is-fastest","text":"https://dev.to/dtinth/caching-docker-builds-in-github-actions-which-approach-is-the-fastest-a-research-18ei","title":"Caching Docker builds in github actions: Which approach is fastest?"},{"location":"gh_actions/cachetest/#abstract","text":"In this post, I experimented with 6 different approaches for caching Docker builds in GitHub Actions to speed up the build process and compared the results. After trying out every approach, 10 times each, the results show that using GitHub Packages\u2019 Docker registry as a build cache, as opposed to GitHub Actions\u2019 built-in cache, yields the highest performance gain. Unlike self-hosted runners like Jenkins, most cloud-hosted build runners are stateless, providing us with a pristine environment each run. We cannot keep files from the previous runs around; anything that needs to be persisted must be externalized. GitHub Actions has a built-in cache to help do this. But there are many ways of creating that cache ( docker save and docker load first comes to mind). Will the performance gains outweight the overhead caused by saving and loading caches? Are there more approaches other than using GitHub Action\u2019s built-in cache? That\u2019s what this research is about.","title":"Abstract:"},{"location":"gh_actions/cachetest/#dockerfile","text":"FROM node:14.21.2 RUN yarn create react-app my-react-app RUN cd my-react-app && yarn build RUN npm install -g @vue/cli && ( yes | vue create my-vue-app --default ) RUN cd my-vue-app && yarn build RUN mkdir -p my-tests && cd my-tests && yarn add playwright # test # test # test # test # test # test # test # test # test # test","title":"Dockerfile"},{"location":"gh_actions/dependabot/","text":"Github Actions \u00b6 Dependabot \u00b6 Create pull requests to keep dependencies up-to-date. version: 2 updates: - package-ecosystem: \"github-actions\" directory: \"/\" schedule: interval: \"weekly\" labels: \"github-actions-dependencies\" assignees: - \"memadsen\" - package-ecosystem: \"pip\" # poetry directory: \"/\" schedule: interval: \"weekly\" labels: \"python-dependencies\" assignees: - \"memadsen\"","title":"Requirements Dependabot"},{"location":"gh_actions/dependabot/#github-actions","text":"","title":"Github Actions"},{"location":"gh_actions/dependabot/#dependabot","text":"Create pull requests to keep dependencies up-to-date. version: 2 updates: - package-ecosystem: \"github-actions\" directory: \"/\" schedule: interval: \"weekly\" labels: \"github-actions-dependencies\" assignees: - \"memadsen\" - package-ecosystem: \"pip\" # poetry directory: \"/\" schedule: interval: \"weekly\" labels: \"python-dependencies\" assignees: - \"memadsen\"","title":"Dependabot"},{"location":"gh_actions/docker_build_push/","text":"Docker Build and Push Image \u00b6 Use github actions to build and push an image to github packages. Source code has an example. The background displayed in these repos is out of scope with the utility of github actions and docker. In this document I reduce the problem to focus on using github actions for docker development (building and debugging docker containers). Artifactory and ECR are optimal however I simplify the problem by ignoring these options and use the repository for hosting the image. hf_ghcr repo in bayer-int huggingface_ghcr repo in memadsen Github Action \u00b6 To authenticate to a GitHub Packages registry within a GitHub Actions workflow, you can use: GITHUB_TOKEN to publish packages associated with the workflow repository. a personal access token (classic) with at least read:packages scope to install packages associated with other private repositories (which GITHUB_TOKEN can't access). Authenticate to github, on behalf of github actions using GITHUB_TOKEN . This allows us to write our docker container to github. Github Actions documentation on token authentication name : docker_build_push on : workflow_dispatch : jobs : build : runs-on: : ubuntu-latest steps : - uses : actions/checkout@v2 - name : docker build python run : | docker build ./python -t name: CI on: workflow_dispatch: jobs: # This workflow contains a single job called \"build\" build: runs-on: ubuntu-latest # Steps represent a sequence of tasks that will be executed as part of the job steps: # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it - uses: actions/checkout@v3 - name: Set up Docker Buildx uses: docker/setup-buildx-action@v1 - name: Authenticate to GitHub container registry uses: docker/login-action@v1.10.0 with: registry: ghcr.io username: ${{ github.actor }} password: ${{ secrets.GITHUB_TOKEN }} #password: ${{ github.token }} - name: Lowercase the repo name and username run: echo \"REPO=${GITHUB_REPOSITORY,,}\" >>${GITHUB_ENV} - name: Build and push container image to registry uses: docker/build-push-action@v2 with: push: true tags: ghcr.io/${{ env.REPO }}:${{ github.sha }} file: ./Dockerfile_pip Dockerfile \u00b6 FROM python:3.9 WORKDIR /code COPY ./requirements.txt /code/requirements.txt RUN pip install --no-cache-dir --upgrade -r /code/requirements.txt COPY ./app /code/app CMD [\"uvicorn\", \"app.main:app\", \"--proxy-headers\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"] Python 3.7.5 dockerfile \u00b6 FROM python:3.7.5-slim # Set up and activate virtual environment ENV VIRTUAL_ENV \"/venv\" RUN python -m venv $VIRTUAL_ENV ENV PATH \"$VIRTUAL_ENV/bin:$PATH\" # Python commands run inside the virtual environment RUN python -m pip install \\ parse \\ realpython-reader Python Latest dockerfile \u00b6 Code for hello.py is print (\"Hello World\") Code for a.py is print (\"Overriden Hello\") #Deriving the latest base image FROM python:latest #Labels as key value pair LABEL Maintainer=\"michael.madsen@bayer.com\" ADD hello.py /home/hello.py ADD a.py /home/a.py CMD [\"/home/hello.py\"] ENTRYPOINT [\"python\"]","title":"Docker Build and Push"},{"location":"gh_actions/docker_build_push/#docker-build-and-push-image","text":"Use github actions to build and push an image to github packages. Source code has an example. The background displayed in these repos is out of scope with the utility of github actions and docker. In this document I reduce the problem to focus on using github actions for docker development (building and debugging docker containers). Artifactory and ECR are optimal however I simplify the problem by ignoring these options and use the repository for hosting the image. hf_ghcr repo in bayer-int huggingface_ghcr repo in memadsen","title":"Docker Build and Push Image"},{"location":"gh_actions/docker_build_push/#github-action","text":"To authenticate to a GitHub Packages registry within a GitHub Actions workflow, you can use: GITHUB_TOKEN to publish packages associated with the workflow repository. a personal access token (classic) with at least read:packages scope to install packages associated with other private repositories (which GITHUB_TOKEN can't access). Authenticate to github, on behalf of github actions using GITHUB_TOKEN . This allows us to write our docker container to github. Github Actions documentation on token authentication name : docker_build_push on : workflow_dispatch : jobs : build : runs-on: : ubuntu-latest steps : - uses : actions/checkout@v2 - name : docker build python run : | docker build ./python -t name: CI on: workflow_dispatch: jobs: # This workflow contains a single job called \"build\" build: runs-on: ubuntu-latest # Steps represent a sequence of tasks that will be executed as part of the job steps: # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it - uses: actions/checkout@v3 - name: Set up Docker Buildx uses: docker/setup-buildx-action@v1 - name: Authenticate to GitHub container registry uses: docker/login-action@v1.10.0 with: registry: ghcr.io username: ${{ github.actor }} password: ${{ secrets.GITHUB_TOKEN }} #password: ${{ github.token }} - name: Lowercase the repo name and username run: echo \"REPO=${GITHUB_REPOSITORY,,}\" >>${GITHUB_ENV} - name: Build and push container image to registry uses: docker/build-push-action@v2 with: push: true tags: ghcr.io/${{ env.REPO }}:${{ github.sha }} file: ./Dockerfile_pip","title":"Github Action"},{"location":"gh_actions/docker_build_push/#dockerfile","text":"FROM python:3.9 WORKDIR /code COPY ./requirements.txt /code/requirements.txt RUN pip install --no-cache-dir --upgrade -r /code/requirements.txt COPY ./app /code/app CMD [\"uvicorn\", \"app.main:app\", \"--proxy-headers\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]","title":"Dockerfile"},{"location":"gh_actions/docker_build_push/#python-375-dockerfile","text":"FROM python:3.7.5-slim # Set up and activate virtual environment ENV VIRTUAL_ENV \"/venv\" RUN python -m venv $VIRTUAL_ENV ENV PATH \"$VIRTUAL_ENV/bin:$PATH\" # Python commands run inside the virtual environment RUN python -m pip install \\ parse \\ realpython-reader","title":"Python 3.7.5 dockerfile"},{"location":"gh_actions/docker_build_push/#python-latest-dockerfile","text":"Code for hello.py is print (\"Hello World\") Code for a.py is print (\"Overriden Hello\") #Deriving the latest base image FROM python:latest #Labels as key value pair LABEL Maintainer=\"michael.madsen@bayer.com\" ADD hello.py /home/hello.py ADD a.py /home/a.py CMD [\"/home/hello.py\"] ENTRYPOINT [\"python\"]","title":"Python Latest dockerfile"},{"location":"gh_actions/ecr_actions_workflow/","text":"CICD Pipeline for Docker Images \u00b6 This is a new repository using DevOps best practices and Python. A project scaffold is setup to use cloud microservices in a CICD workflow. This scaffold can be modified to fit a typical data science project. Access this code at (bayer-int/smol-cls-ecr)[https://github.com/bayer-int/smol-cls-ecr] Summary \u00b6 Build production docker images using CICD pipeline (GH Actions), lint docker/python and push to AWS ECR. Authentication using OIDC is preferred, because there are no keys/secrets to manage. GH Actions is used to build a docker image and directly pushed to ECR from GH Actions. MLOps Best Practices \u00b6 My web application is a microservice flowchart LR CICD(CICD) --> Container(Container) Microservice(Microservice) Container --> Microservice fxn(Function) <--> Web(Web) Microservice --> fxn Scaffold \u00b6 My preferred process for every new project. This GitHub repo has all the ingredients for a building a container based API deployed to AWS: Makefile , requirements.txt , virtual environment, directory structure, application files, application test files and Dockerfile . flowchart LR GH(GitHub Checkout) --> code(Codebase <br> _______ <br> - App Scripts <br> - App Directories <br> - Test App Logic <br> - Virtual Environment <br> - Library Requirements <br> - Container) The Makefile and GH Actions .yml orchestrate all the logic. This minimizes debugging and collaboration/ramping-up. The GH Actions runner implements directly from the Makefile . In addition the Makefile can be used for early development in local environment. Production code should run exactly like local code but in practice something usually breaks and minor manual changes are required; typically there is a bias towards production. Project Makefile : \u00b6 install: # Install Py lib pip install --upgrade pip && \\ pip install -r requirements.txt #cat requirements.txt | xargs poetry add post-install: python -m textblob.download_corpora git: # Update Git Repo bash sync_git.sh format: # format code using black black *.py mylib/*.py lint: # format code using pylint or flake8 pylint --disable = R,C *.py mylib/*.py test: # test code python -m pytest -vv --cov = mylib --cov = main test_*.py build: #build container docker build -t deploy-fastapi . run: #run docker docker run -p 127 .0.0.1:8080:8080 deploy-fastapi deploy: aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 722540083300 .dkr.ecr.us-east-1.amazonaws.com docker build -t smol-cls-ecr . ## This is the ECR name docker tag smol-cls-ecr:latest 722540083300 .dkr.ecr.us-east-1.amazonaws.com/smol-cls-ecr:latest docker push 722540083300 .dkr.ecr.us-east-1.amazonaws.com/smol-cls-ecr:latest all: install post-install lint test deploy Development \u00b6 Write code in local environment, use GH Actions to test and then deploy to AWS. Workflow \u00b6 OIDC authenticate \u00b6 Allow GH Actions to authenticate to AWS using OIDC. Build docker image using GH Actions and push to ECR. In your cloud provider, create an OIDC trust between your cloud role and your GitHub workflow(s) that need access to the cloud. Every time your job runs, GitHub's OIDC Provider auto-generates an OIDC token. This token contains multiple claims to establish a security-hardened and verifiable identity about the specific workflow that is trying to authenticate. You could include a step or action in your job to request this token from GitHub's OIDC provider, and present it to the cloud provider. Once the cloud provider successfully validates the claims presented in the token, it then provides a short-lived cloud access token that is available only for the duration of the job. For more information see the github docs for security hardening production deployments link: https://docs.github.com/en/actions/deployment/security-hardening-your-deployments/about-security-hardening-with-openid-connect AWS Create Service catalog product \u00b6 TODO Launch this CF stack using CLI. This project is currently provisioned using the AWS console for CF and IAM. Start Work: Navigate to AWS Console. From the AWS console >> Service Catalog >> Products the github-oidc-provider will show up like this: Next, build CloudFormation stack for cloud engineering product: github oidc provider . In CloudFormation parameters set bayer-int and repo name smol-cls-cicd Update repo name for each new project. Next set CF parameters. parameter content required GitHub org bayer-int true OIDCProviderArn When catalog item was already used once. Use arn of generated OIDC Provider. false Repository Name Technical name of the repository, as seen in the url. e.g smol-cls-cicd true AWS IAM setup \u00b6 Trusted entitites policy is attached during CloudFormation build. It should look like this: { \"Version\" : \"2008-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"Federated\" : \"arn:aws:iam::722540083300:oidc-provider/token.actions.githubusercontent.com\" , \"AWS\" : \"arn:aws:iam::722540083300:role/cloudops\" }, \"Action\" : \"sts:AssumeRoleWithWebIdentity\" , \"Condition\" : { \"StringLike\" : { \"token.actions.githubusercontent.com:sub\" : \"repo:bayer-int/smol-cls-mwaa-cicd:*\" } } }, { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"arn:aws:iam::722540083300:role/cloudops\" }, \"Action\" : \"sts:AssumeRole\" } ] } Attach ECR policy to OIDC role \u00b6 Attach existing policies directly. In \"find policies\" search for AmazonEC2ContainerRegistryFullAccess and check it which will give this User Account permission to push on Private ECR. GitHub Actions \u00b6 Add file for GH Actions and document workflow events. A simple project has the following directories: \u251c\u2500\u2500 .github \u2502 \u2514\u2500\u2500 workflows \u2502 \u2514\u2500\u2500 deploy_ecr.yml ## GitHub Actions \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 Dockerfile ## Dockerfile in project root \u251c\u2500\u2500 Makefile ## Makefile instructs my GH Actions \u251c\u2500\u2500 README.md \u251c\u2500\u2500 img \u251c\u2500\u2500 main.py \u251c\u2500\u2500 mylib ## python logic \u251c\u2500\u2500 requirements.txt ## virtual environment \u251c\u2500\u2500 sync_git.sh ## optional sync git \u2514\u2500\u2500 test_main.py Include GitHub actions permissions \u00b6 ## This allows your GitHub Actions job to save the temporary ## credentials it gets when authenticating with the Cloud Provider. permissions : contents : 'read' id-token : 'write' Use the aws-actions/configure-aws-credentials action. - name : Configure AWS Credentials uses : aws-actions/configure-aws-credentials@v1 with : role-to-assume : arn:aws:iam::123456789100:role/github-actions-role aws-region : eu-east-1 Test GH Actions server runs as expected \u00b6 Push code to git Verify docker image is in ECR Reference \u00b6 This project builds from the OIDC auth method documented in Bayer go docs. To review the details go to Authenticate to Cloud - Bayer go docs End \u00b6","title":"ECR Actions"},{"location":"gh_actions/ecr_actions_workflow/#cicd-pipeline-for-docker-images","text":"This is a new repository using DevOps best practices and Python. A project scaffold is setup to use cloud microservices in a CICD workflow. This scaffold can be modified to fit a typical data science project. Access this code at (bayer-int/smol-cls-ecr)[https://github.com/bayer-int/smol-cls-ecr]","title":"CICD Pipeline for Docker Images"},{"location":"gh_actions/ecr_actions_workflow/#summary","text":"Build production docker images using CICD pipeline (GH Actions), lint docker/python and push to AWS ECR. Authentication using OIDC is preferred, because there are no keys/secrets to manage. GH Actions is used to build a docker image and directly pushed to ECR from GH Actions.","title":"Summary"},{"location":"gh_actions/ecr_actions_workflow/#mlops-best-practices","text":"My web application is a microservice flowchart LR CICD(CICD) --> Container(Container) Microservice(Microservice) Container --> Microservice fxn(Function) <--> Web(Web) Microservice --> fxn","title":"MLOps Best Practices"},{"location":"gh_actions/ecr_actions_workflow/#scaffold","text":"My preferred process for every new project. This GitHub repo has all the ingredients for a building a container based API deployed to AWS: Makefile , requirements.txt , virtual environment, directory structure, application files, application test files and Dockerfile . flowchart LR GH(GitHub Checkout) --> code(Codebase <br> _______ <br> - App Scripts <br> - App Directories <br> - Test App Logic <br> - Virtual Environment <br> - Library Requirements <br> - Container) The Makefile and GH Actions .yml orchestrate all the logic. This minimizes debugging and collaboration/ramping-up. The GH Actions runner implements directly from the Makefile . In addition the Makefile can be used for early development in local environment. Production code should run exactly like local code but in practice something usually breaks and minor manual changes are required; typically there is a bias towards production.","title":"Scaffold"},{"location":"gh_actions/ecr_actions_workflow/#project-makefile","text":"install: # Install Py lib pip install --upgrade pip && \\ pip install -r requirements.txt #cat requirements.txt | xargs poetry add post-install: python -m textblob.download_corpora git: # Update Git Repo bash sync_git.sh format: # format code using black black *.py mylib/*.py lint: # format code using pylint or flake8 pylint --disable = R,C *.py mylib/*.py test: # test code python -m pytest -vv --cov = mylib --cov = main test_*.py build: #build container docker build -t deploy-fastapi . run: #run docker docker run -p 127 .0.0.1:8080:8080 deploy-fastapi deploy: aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 722540083300 .dkr.ecr.us-east-1.amazonaws.com docker build -t smol-cls-ecr . ## This is the ECR name docker tag smol-cls-ecr:latest 722540083300 .dkr.ecr.us-east-1.amazonaws.com/smol-cls-ecr:latest docker push 722540083300 .dkr.ecr.us-east-1.amazonaws.com/smol-cls-ecr:latest all: install post-install lint test deploy","title":"Project Makefile:"},{"location":"gh_actions/ecr_actions_workflow/#development","text":"Write code in local environment, use GH Actions to test and then deploy to AWS.","title":"Development"},{"location":"gh_actions/ecr_actions_workflow/#workflow","text":"","title":"Workflow"},{"location":"gh_actions/ecr_actions_workflow/#oidc-authenticate","text":"Allow GH Actions to authenticate to AWS using OIDC. Build docker image using GH Actions and push to ECR. In your cloud provider, create an OIDC trust between your cloud role and your GitHub workflow(s) that need access to the cloud. Every time your job runs, GitHub's OIDC Provider auto-generates an OIDC token. This token contains multiple claims to establish a security-hardened and verifiable identity about the specific workflow that is trying to authenticate. You could include a step or action in your job to request this token from GitHub's OIDC provider, and present it to the cloud provider. Once the cloud provider successfully validates the claims presented in the token, it then provides a short-lived cloud access token that is available only for the duration of the job. For more information see the github docs for security hardening production deployments link: https://docs.github.com/en/actions/deployment/security-hardening-your-deployments/about-security-hardening-with-openid-connect","title":"OIDC authenticate"},{"location":"gh_actions/ecr_actions_workflow/#aws-create-service-catalog-product","text":"TODO Launch this CF stack using CLI. This project is currently provisioned using the AWS console for CF and IAM. Start Work: Navigate to AWS Console. From the AWS console >> Service Catalog >> Products the github-oidc-provider will show up like this: Next, build CloudFormation stack for cloud engineering product: github oidc provider . In CloudFormation parameters set bayer-int and repo name smol-cls-cicd Update repo name for each new project. Next set CF parameters. parameter content required GitHub org bayer-int true OIDCProviderArn When catalog item was already used once. Use arn of generated OIDC Provider. false Repository Name Technical name of the repository, as seen in the url. e.g smol-cls-cicd true","title":"AWS Create Service catalog product"},{"location":"gh_actions/ecr_actions_workflow/#aws-iam-setup","text":"Trusted entitites policy is attached during CloudFormation build. It should look like this: { \"Version\" : \"2008-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"Federated\" : \"arn:aws:iam::722540083300:oidc-provider/token.actions.githubusercontent.com\" , \"AWS\" : \"arn:aws:iam::722540083300:role/cloudops\" }, \"Action\" : \"sts:AssumeRoleWithWebIdentity\" , \"Condition\" : { \"StringLike\" : { \"token.actions.githubusercontent.com:sub\" : \"repo:bayer-int/smol-cls-mwaa-cicd:*\" } } }, { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"arn:aws:iam::722540083300:role/cloudops\" }, \"Action\" : \"sts:AssumeRole\" } ] }","title":"AWS IAM setup"},{"location":"gh_actions/ecr_actions_workflow/#attach-ecr-policy-to-oidc-role","text":"Attach existing policies directly. In \"find policies\" search for AmazonEC2ContainerRegistryFullAccess and check it which will give this User Account permission to push on Private ECR.","title":"Attach ECR policy to OIDC role"},{"location":"gh_actions/ecr_actions_workflow/#github-actions","text":"Add file for GH Actions and document workflow events. A simple project has the following directories: \u251c\u2500\u2500 .github \u2502 \u2514\u2500\u2500 workflows \u2502 \u2514\u2500\u2500 deploy_ecr.yml ## GitHub Actions \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 Dockerfile ## Dockerfile in project root \u251c\u2500\u2500 Makefile ## Makefile instructs my GH Actions \u251c\u2500\u2500 README.md \u251c\u2500\u2500 img \u251c\u2500\u2500 main.py \u251c\u2500\u2500 mylib ## python logic \u251c\u2500\u2500 requirements.txt ## virtual environment \u251c\u2500\u2500 sync_git.sh ## optional sync git \u2514\u2500\u2500 test_main.py","title":"GitHub Actions"},{"location":"gh_actions/ecr_actions_workflow/#include-github-actions-permissions","text":"## This allows your GitHub Actions job to save the temporary ## credentials it gets when authenticating with the Cloud Provider. permissions : contents : 'read' id-token : 'write' Use the aws-actions/configure-aws-credentials action. - name : Configure AWS Credentials uses : aws-actions/configure-aws-credentials@v1 with : role-to-assume : arn:aws:iam::123456789100:role/github-actions-role aws-region : eu-east-1","title":"Include GitHub actions permissions"},{"location":"gh_actions/ecr_actions_workflow/#test-gh-actions-server-runs-as-expected","text":"Push code to git Verify docker image is in ECR","title":"Test GH Actions server runs as expected"},{"location":"gh_actions/ecr_actions_workflow/#reference","text":"This project builds from the OIDC auth method documented in Bayer go docs. To review the details go to Authenticate to Cloud - Bayer go docs","title":"Reference"},{"location":"gh_actions/ecr_actions_workflow/#end","text":"","title":"End"},{"location":"gh_actions/env_var/","text":"Github Actions Environment Variables \u00b6 Example jobs.<job_id>.if \u00b6 Only run job for specific repository. This example uses if to control when the production-deploy job can run. It will only run if the repository is named MY_REPO and is within the bayer-int organization. Otherwise, the job will be marked as skipped. github actions workflow syntax docs github actions default environment variables name : example-workflow on : [ push ] jobs : production-deploy : if : github.repository == 'bayer-int/MY_REPO' runs-on : ubuntu-latest steps : - uses : actions/checkout@v3 - uses : actions/setup-node@v3 with : node-version : '14' - run : echo 'Hello World' Contexts \u00b6 Contexts are a way to access information about workflow runs, variables, runner environments, jobs, and steps. Each context is an object that contains properties, which can be strings or other objects. github actions contexts docs Example usage of vars context \u00b6 This example workflow shows how configuration variables set at the repository, environment, or organization levels are automatically available using the vars context. on : workflow_dispatch : env : # Setting an environment variable with the value of a configuration variable env_var : ${{ vars.ENV_CONTEXT_VAR }} jobs : display-variables : name : ${{ vars.JOB_NAME }} # You can use configuration variables with the `vars` context for dynamic jobs if : ${{ vars.USE_VARIABLES == 'true' }} runs-on : ${{ vars.RUNNER }} environment : ${{ vars.ENVIRONMENT_STAGE }} steps : - name : Use variables run : | echo \"repository variable : ${{ vars.REPOSITORY_VAR }}\" echo \"organization variable : ${{ vars.ORGANIZATION_VAR }}\" echo \"overridden variable : ${{ vars.OVERRIDE_VAR }}\" echo \"variable from shell environment : $env_var\"","title":"Env Vars"},{"location":"gh_actions/env_var/#github-actions-environment-variables","text":"","title":"Github Actions Environment Variables"},{"location":"gh_actions/env_var/#example-jobsjob_idif","text":"Only run job for specific repository. This example uses if to control when the production-deploy job can run. It will only run if the repository is named MY_REPO and is within the bayer-int organization. Otherwise, the job will be marked as skipped. github actions workflow syntax docs github actions default environment variables name : example-workflow on : [ push ] jobs : production-deploy : if : github.repository == 'bayer-int/MY_REPO' runs-on : ubuntu-latest steps : - uses : actions/checkout@v3 - uses : actions/setup-node@v3 with : node-version : '14' - run : echo 'Hello World'","title":"Example jobs.&lt;job_id&gt;.if"},{"location":"gh_actions/env_var/#contexts","text":"Contexts are a way to access information about workflow runs, variables, runner environments, jobs, and steps. Each context is an object that contains properties, which can be strings or other objects. github actions contexts docs","title":"Contexts"},{"location":"gh_actions/env_var/#example-usage-of-vars-context","text":"This example workflow shows how configuration variables set at the repository, environment, or organization levels are automatically available using the vars context. on : workflow_dispatch : env : # Setting an environment variable with the value of a configuration variable env_var : ${{ vars.ENV_CONTEXT_VAR }} jobs : display-variables : name : ${{ vars.JOB_NAME }} # You can use configuration variables with the `vars` context for dynamic jobs if : ${{ vars.USE_VARIABLES == 'true' }} runs-on : ${{ vars.RUNNER }} environment : ${{ vars.ENVIRONMENT_STAGE }} steps : - name : Use variables run : | echo \"repository variable : ${{ vars.REPOSITORY_VAR }}\" echo \"organization variable : ${{ vars.ORGANIZATION_VAR }}\" echo \"overridden variable : ${{ vars.OVERRIDE_VAR }}\" echo \"variable from shell environment : $env_var\"","title":"Example usage of vars context"},{"location":"gh_actions/gh_actions_cheatsheet/","text":"GitHub Actions Basic Template \u00b6 Template for using GitHub actions. This file lives in the workflows directory, <my_repo/><.github/workflows/gh_action.yml> My directory tree showing the workflows dir, relative to the project root. Actions Template \u00b6 . \u251c\u2500\u2500 docs \u251c\u2500\u2500 .github \u2502 \u2514\u2500\u2500 workflows \u2502 \u2514\u2500\u2500 gh_action.yml name : OIDC auth Push ECR from GH Actions docker build ## Trigger #on: [push] on : push : paths : - 's3_bucket/**' - '.github/**' workflow_dispatch : permissions : contents : 'read' id-token : 'write' jobs : test : name : github oidc sync to AWS runs-on : ubuntu-latest steps : - name : Git clone the repository uses : actions/checkout@v2 ## Requires team_users.txt with github users, e.g. memadsen - name : User list to authorize GH Action build run : | # if user not in list, exit while read line; do if [ ${{ github.actor }} == \"$line\" ]; then echo \"----- ${{ github.actor }} is authorized user -----\" ALLOWED_USER=${{ github.actor }} break fi done < \".github/team_users.txt\" if [ -z \"$ALLOWED_USER\"]; then echo \"----- Job triggered by unauthorized user -----\" exit 1 fi ## OIDC Auth ## Requires AWS IAM Role ARN - name : Configure AWS Credentials uses : aws-actions/configure-aws-credentials@v1 with : role-to-assume : arn:aws:iam::073416988478:role/SC-073416988478-pp-jgiui7c5d7xx2-Role-SLI0NYCKDL9J aws-region : us-east-1 - name : STS verify AWS connect to OIDC auth run : aws sts get-caller-identity - name : s3 sync and rm S3 files not in local run : | aws s3 sync s3_bucket/ s3://mulesoft-fastapi/local_folder/ --delete - name : Update AWS Lambda run : | aws lambda update-function-code \\ --function-name mulesoft-fastapi \\ --s3-bucket mulesoft-fastapi \\ --s3-key local_folder/deployment-package.zip \\ --publish Event Triggers \u00b6 Event to run job. Trigger using a regex pattern. on : push : tags : - 'v*' You can make a advanced rule that only triggers on v1 and up or excludes -alpha releases, if that matters to you. Trigger on a push to main or a tag on : push : branches : - main tags : Trigger on a push to main and a tag. on : push : branches : - main tags : jobs : build-deploy : if : startsWith(github.ref, 'refs/tags/') steps : # ... There is no on.tag or on.tags option, but there is on.release - see below.","title":"GH Actions Cheatsheet"},{"location":"gh_actions/gh_actions_cheatsheet/#github-actions-basic-template","text":"Template for using GitHub actions. This file lives in the workflows directory, <my_repo/><.github/workflows/gh_action.yml> My directory tree showing the workflows dir, relative to the project root.","title":"GitHub Actions Basic Template"},{"location":"gh_actions/gh_actions_cheatsheet/#actions-template","text":". \u251c\u2500\u2500 docs \u251c\u2500\u2500 .github \u2502 \u2514\u2500\u2500 workflows \u2502 \u2514\u2500\u2500 gh_action.yml name : OIDC auth Push ECR from GH Actions docker build ## Trigger #on: [push] on : push : paths : - 's3_bucket/**' - '.github/**' workflow_dispatch : permissions : contents : 'read' id-token : 'write' jobs : test : name : github oidc sync to AWS runs-on : ubuntu-latest steps : - name : Git clone the repository uses : actions/checkout@v2 ## Requires team_users.txt with github users, e.g. memadsen - name : User list to authorize GH Action build run : | # if user not in list, exit while read line; do if [ ${{ github.actor }} == \"$line\" ]; then echo \"----- ${{ github.actor }} is authorized user -----\" ALLOWED_USER=${{ github.actor }} break fi done < \".github/team_users.txt\" if [ -z \"$ALLOWED_USER\"]; then echo \"----- Job triggered by unauthorized user -----\" exit 1 fi ## OIDC Auth ## Requires AWS IAM Role ARN - name : Configure AWS Credentials uses : aws-actions/configure-aws-credentials@v1 with : role-to-assume : arn:aws:iam::073416988478:role/SC-073416988478-pp-jgiui7c5d7xx2-Role-SLI0NYCKDL9J aws-region : us-east-1 - name : STS verify AWS connect to OIDC auth run : aws sts get-caller-identity - name : s3 sync and rm S3 files not in local run : | aws s3 sync s3_bucket/ s3://mulesoft-fastapi/local_folder/ --delete - name : Update AWS Lambda run : | aws lambda update-function-code \\ --function-name mulesoft-fastapi \\ --s3-bucket mulesoft-fastapi \\ --s3-key local_folder/deployment-package.zip \\ --publish","title":"Actions Template"},{"location":"gh_actions/gh_actions_cheatsheet/#event-triggers","text":"Event to run job. Trigger using a regex pattern. on : push : tags : - 'v*' You can make a advanced rule that only triggers on v1 and up or excludes -alpha releases, if that matters to you. Trigger on a push to main or a tag on : push : branches : - main tags : Trigger on a push to main and a tag. on : push : branches : - main tags : jobs : build-deploy : if : startsWith(github.ref, 'refs/tags/') steps : # ... There is no on.tag or on.tags option, but there is on.release - see below.","title":"Event Triggers"},{"location":"gh_actions/gh_actions_poetry/","text":"Install poetry in GH actions workflow. steps: - uses: actions/setup-python@v3 with: python-version: \"3.10\" - run: python -m pip install --upgrade poetry - run: poetry install - run: poetry run mkdocs build","title":"GH Actions Poetry"},{"location":"gh_actions/gh_actions_tutorial/","text":"GitHub Actions \u00b6 Github actions is a virtual machine used to do work. Each workflow is configured by the YAML file located in .github/workflows . Workflow triggers can be time or event based. Unlike self-hosted runners like Jenkins, most cloud-hosted build runners are stateless, providing a pristine runtime environment. We cannot keep files from the previous runs around; anything that needs to be persisted must be externalized. Github Actions - Architecture \u00b6 GitHub Actions are formed by a set of components. There are six main components of GitHub Actions: Workflows: Automated procedure added to the repository, and is the actual Action itself Events: An activity that triggers a workflow; these can be based on events such as push or pull requests, but they can also be scheduled using the crontab syntax Jobs: A group of one or more steps that are executed inside a runner Steps: These are tasks from a job that can be used to run commands Actions: The standalone commands from the steps Runners: A server that has the GHA runner application installed. The runner is a server provided by GitHub to run your workflows (also known as Actions). They are deployed and are terminated after your automation is completed. Although there are some limits regarding this service, users do not pay anything to use it, even with a free GitHub account. A useful cheatsheet for syntax to GitHub action workflows Network security testing on GitHub Actions Security Hardening with OIDC \u00b6 Security hardening with short lived tokens from cloud service. GitHub Actions workflows can access a cloud provider (such as AWS, Azure, GCP, or HashiCorp Vault) in order to deploy software or use the cloud's services. Github's OIDC provider overview OIDC tokens are good security practice and reduce tech debt for developers. - No cloud secrets - authentication and authorization management - rotating credentials Github Actions documentation on OIDC End \u00b6","title":"GH Actions Description"},{"location":"gh_actions/gh_actions_tutorial/#github-actions","text":"Github actions is a virtual machine used to do work. Each workflow is configured by the YAML file located in .github/workflows . Workflow triggers can be time or event based. Unlike self-hosted runners like Jenkins, most cloud-hosted build runners are stateless, providing a pristine runtime environment. We cannot keep files from the previous runs around; anything that needs to be persisted must be externalized.","title":"GitHub Actions"},{"location":"gh_actions/gh_actions_tutorial/#github-actions-architecture","text":"GitHub Actions are formed by a set of components. There are six main components of GitHub Actions: Workflows: Automated procedure added to the repository, and is the actual Action itself Events: An activity that triggers a workflow; these can be based on events such as push or pull requests, but they can also be scheduled using the crontab syntax Jobs: A group of one or more steps that are executed inside a runner Steps: These are tasks from a job that can be used to run commands Actions: The standalone commands from the steps Runners: A server that has the GHA runner application installed. The runner is a server provided by GitHub to run your workflows (also known as Actions). They are deployed and are terminated after your automation is completed. Although there are some limits regarding this service, users do not pay anything to use it, even with a free GitHub account. A useful cheatsheet for syntax to GitHub action workflows Network security testing on GitHub Actions","title":"Github Actions - Architecture"},{"location":"gh_actions/gh_actions_tutorial/#security-hardening-with-oidc","text":"Security hardening with short lived tokens from cloud service. GitHub Actions workflows can access a cloud provider (such as AWS, Azure, GCP, or HashiCorp Vault) in order to deploy software or use the cloud's services. Github's OIDC provider overview OIDC tokens are good security practice and reduce tech debt for developers. - No cloud secrets - authentication and authorization management - rotating credentials Github Actions documentation on OIDC","title":"Security Hardening with OIDC"},{"location":"gh_actions/gh_actions_tutorial/#end","text":"","title":"End"},{"location":"gh_actions/push_file_remote/","text":"Update Remote Repository \u00b6 A centralized repository is used for documenting data science governance. Information on CSP, best practices and announcements. This is the \"source of truth\" to provide guidance to CLS SMol teams, working on data science projects. Update projects at scale \u00b6 Documentation updates need to scale across projects. Github is the standard for source code management. Templates provide boilerplate to ensure alignment on sharing data and ML-governance. Project source code is updated automagically from the ML governance documentation. Project files are independent of updates from ML-Governance. gitGraph commit id: \"1\" branch ML-Governance commit id: \"2\" commit id: \"3\" commit id: \"4\" checkout main commit id: \"5\" commit id: \"6\" merge ML-Governance id: \"Merge ML Docs\" tag: \"V 1.0\" type: HIGHLIGHT commit id: \"7\" commit id: \"8\" Implementation \u00b6 Generate PAT in my personal repo and scope for GH Actions. Navigate to my personal repo and open settings \u2192 Developer settings: Next generate a personal access token and save it somewhere safe: In another tab open the source repo used to write files, bayer-int/smol-cls-docs . Open the Security \u2192 Secrets \u2192 Actions and add the secret. GH Actions Workflow \u00b6 The workflow to push files to another repo. name : Push File on : push : branches : - main workflow_dispatch : jobs : copy-file : runs-on : ubuntu-latest steps : - name : Checkout uses : actions/checkout@v2 - name : Pushes test file uses : dmnemec/copy_file_to_another_repo_action@main env : API_TOKEN_GITHUB : ${{ secrets.PUSH_FILE }} with : source_file : 'poetry.lock' destination_repo : 'bayer-int/hf_ghcr' destination_folder : 'test-dir' user_email : 'michael.madsen@bayer.com' user_name : 'memadsen' commit_message : 'A custom message for the commit'","title":"Push Remote File"},{"location":"gh_actions/push_file_remote/#update-remote-repository","text":"A centralized repository is used for documenting data science governance. Information on CSP, best practices and announcements. This is the \"source of truth\" to provide guidance to CLS SMol teams, working on data science projects.","title":"Update Remote Repository"},{"location":"gh_actions/push_file_remote/#update-projects-at-scale","text":"Documentation updates need to scale across projects. Github is the standard for source code management. Templates provide boilerplate to ensure alignment on sharing data and ML-governance. Project source code is updated automagically from the ML governance documentation. Project files are independent of updates from ML-Governance. gitGraph commit id: \"1\" branch ML-Governance commit id: \"2\" commit id: \"3\" commit id: \"4\" checkout main commit id: \"5\" commit id: \"6\" merge ML-Governance id: \"Merge ML Docs\" tag: \"V 1.0\" type: HIGHLIGHT commit id: \"7\" commit id: \"8\"","title":"Update projects at scale"},{"location":"gh_actions/push_file_remote/#implementation","text":"Generate PAT in my personal repo and scope for GH Actions. Navigate to my personal repo and open settings \u2192 Developer settings: Next generate a personal access token and save it somewhere safe: In another tab open the source repo used to write files, bayer-int/smol-cls-docs . Open the Security \u2192 Secrets \u2192 Actions and add the secret.","title":"Implementation"},{"location":"gh_actions/push_file_remote/#gh-actions-workflow","text":"The workflow to push files to another repo. name : Push File on : push : branches : - main workflow_dispatch : jobs : copy-file : runs-on : ubuntu-latest steps : - name : Checkout uses : actions/checkout@v2 - name : Pushes test file uses : dmnemec/copy_file_to_another_repo_action@main env : API_TOKEN_GITHUB : ${{ secrets.PUSH_FILE }} with : source_file : 'poetry.lock' destination_repo : 'bayer-int/hf_ghcr' destination_folder : 'test-dir' user_email : 'michael.madsen@bayer.com' user_name : 'memadsen' commit_message : 'A custom message for the commit'","title":"GH Actions Workflow"},{"location":"gh_actions/read_project_file/","text":"Github Actions Read Files in Project \u00b6 Configure github actions to access files locally. Team User List \u00b6 User list limits user access to the github action build. Add file in .github/team_users.txt and add github user, e.g. memadsen. Now, only memadsen is authorized to build on github actions. jobs : build : runs-on : ubuntu-latest # Steps represent a sequence of tasks that will be executed as part of the job steps : - name : Checks-out repository under $GITHUB_WORKSPACE, so your job can access it uses : actions/checkout@v3 ## Requires team_users.txt with github users, e.g. memadsen - name : User list to authorize GH Action build run : | # if user not in list, exit while read line; do if [ ${{ github.actor }} == \"$line\" ]; then echo \"----- ${{ github.actor }} is authorized user -----\" ALLOWED_USER=${{ github.actor }} break fi done < \".github/team_users.txt\" if [ -z \"$ALLOWED_USER\"]; then echo \"----- Job triggered by unauthorized user -----\" exit 1 fi Github metadata and environment variables \u00b6 name : Context Example on : [ push ] jobs : build : name : Inspect context runs-on : ubuntu-latest steps : - name : Inspect context run : | echo 'Github Repository ------ \\n ${{ github.repository }} ' echo 'Github use ---------- \\n ${{ github.actor }}'' build : runs-on : ubuntu-latest # Steps represent a sequence of tasks that will be executed as part of the job steps : # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it - uses : actions/checkout@v2 # Runs commands using the runners shell - name : Run the build php script run : | cd build php build.php # This file reads a few json files and then creates a set of html files in the doc folder cd ../ - name : Commit files # transfer the new html files back into the repository run : | git config --local user.name \"jpadfield\" git add ./docs git commit -m \"Updating the repository GitHub html pages in the docs folder\" - name : Push changes # push the output folder to your repo uses : ad-m/github-push-action@master with : github_token : ${{ secrets.GITHUB_TOKEN }} force : true Check if File exists \u00b6 Check (e.g. with git diff --exit-code ) if there actually are any changes, and skip the commit if not. Definitely better than committing deletion first and then (possibly identical) new files. Example: - id : commit_files name : Commit files run : git config --local user.name actions-user git config --local user.email \"actions@github.com\" if ! git diff --exit-code; then git add your/files/here git commit -am \"GH Action Files added $(date)\" git push -f origin main fi","title":"Read Project File"},{"location":"gh_actions/read_project_file/#github-actions-read-files-in-project","text":"Configure github actions to access files locally.","title":"Github Actions Read Files in Project"},{"location":"gh_actions/read_project_file/#team-user-list","text":"User list limits user access to the github action build. Add file in .github/team_users.txt and add github user, e.g. memadsen. Now, only memadsen is authorized to build on github actions. jobs : build : runs-on : ubuntu-latest # Steps represent a sequence of tasks that will be executed as part of the job steps : - name : Checks-out repository under $GITHUB_WORKSPACE, so your job can access it uses : actions/checkout@v3 ## Requires team_users.txt with github users, e.g. memadsen - name : User list to authorize GH Action build run : | # if user not in list, exit while read line; do if [ ${{ github.actor }} == \"$line\" ]; then echo \"----- ${{ github.actor }} is authorized user -----\" ALLOWED_USER=${{ github.actor }} break fi done < \".github/team_users.txt\" if [ -z \"$ALLOWED_USER\"]; then echo \"----- Job triggered by unauthorized user -----\" exit 1 fi","title":"Team User List"},{"location":"gh_actions/read_project_file/#github-metadata-and-environment-variables","text":"name : Context Example on : [ push ] jobs : build : name : Inspect context runs-on : ubuntu-latest steps : - name : Inspect context run : | echo 'Github Repository ------ \\n ${{ github.repository }} ' echo 'Github use ---------- \\n ${{ github.actor }}'' build : runs-on : ubuntu-latest # Steps represent a sequence of tasks that will be executed as part of the job steps : # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it - uses : actions/checkout@v2 # Runs commands using the runners shell - name : Run the build php script run : | cd build php build.php # This file reads a few json files and then creates a set of html files in the doc folder cd ../ - name : Commit files # transfer the new html files back into the repository run : | git config --local user.name \"jpadfield\" git add ./docs git commit -m \"Updating the repository GitHub html pages in the docs folder\" - name : Push changes # push the output folder to your repo uses : ad-m/github-push-action@master with : github_token : ${{ secrets.GITHUB_TOKEN }} force : true","title":"Github metadata and environment variables"},{"location":"gh_actions/read_project_file/#check-if-file-exists","text":"Check (e.g. with git diff --exit-code ) if there actually are any changes, and skip the commit if not. Definitely better than committing deletion first and then (possibly identical) new files. Example: - id : commit_files name : Commit files run : git config --local user.name actions-user git config --local user.email \"actions@github.com\" if ! git diff --exit-code; then git add your/files/here git commit -am \"GH Action Files added $(date)\" git push -f origin main fi","title":"Check if File exists"},{"location":"gh_actions/runner/","text":"Self Hosted Runner \u00b6 Github Actions using self hosted runner. name : Github runner demo on : [ push ] jobs : runner-demo : runs-on : [ self-hosted , linux ] steps : - run : echo \"I'm using my self-hosted runner for this.\"","title":"Self Hosted Runner"},{"location":"gh_actions/runner/#self-hosted-runner","text":"Github Actions using self hosted runner. name : Github runner demo on : [ push ] jobs : runner-demo : runs-on : [ self-hosted , linux ] steps : - run : echo \"I'm using my self-hosted runner for this.\"","title":"Self Hosted Runner"},{"location":"gh_actions/sbom/","text":"SBOM Software Bill of Material with Docker Container and Github Actions \u00b6 Generate an SBOM from within a GitHub Actions workflow. In this way, the SBOM is shipped with the container image and is made available without having to scan the image each time. What is SBOM? \u00b6 From Wikipedia: A software bill of materials ( SBOM ) is a list of components in a piece of software . Software vendors often create products by assembling open source and commercial software components . The SBOM describes the components in a product . It is analogous to a list of ingredients on food packaging : where you might consult a label to avoid foods that may cause an allergies , SBOMs can help companies avoid consumption of software that could harm their organization . The concept of a BOM is well - established in traditional manufacturing as part of supply chain management . A manufacturer uses a BOM to track the parts it uses to create a product . If defects are later found in a specific part , the BOM makes it easy to locate affected products . Crane \u00b6 crane is a tool for interacting with remote images and registries. Install crane sudo snap install go --classic ## Install go go install github.com/google/go-containerregistry/cmd/crane@latest curl -sL \"https://github.com/google/go-containerregistry/releases/latest/download/go-containerregistry_Linux_x86_64.tar.gz\" > go-containerregistry.tar.gz curl -sL https://api.github.com/repos/konveyor/crane/releases/latest | jq -r \".assets[] | select(.name | contains(\\\"amd64-linux\\\")) | .browser_download_url\" | wget -i- Imagine you have the following Dockerfile: \u00b6 FROM alpine:3.17.0 RUN apk add --no-cache curl ca-certificates CMD [ \"curl\" , \"https://www.google.com\" ] I know that there's a vulnerability in alpine 3.17.0 in the OpenSSL library. How do I know that? I recently updated every OpenFaaS Pro component to use 3.17.1 to fix a specific vulnerability. Now a typical workflow for this Dockerfile would look like the below: name: build on: push: branches: [ master, main ] pull_request: branches: [ master, main ] permissions: actions: read checks: write contents: read packages: write jobs: publish: runs-on: ubuntu-latest steps: - uses: actions/checkout@master with: fetch-depth: 1 - name: Build Docker image run: docker build . --file Dockerfile.sbom --tag my-image-name:$(date +%s) # - name: Set up Docker Buildx # uses: docker/setup-buildx-action@v2 # - name: Login to Docker Registry # uses: docker/login-action@v2 # with: # username: ${{ github.repository_owner }} # password: ${{ secrets.GITHUB_TOKEN }} # registry: ghcr.io - name: Publish image uses: docker/build-push-action@v3 with: build-args: | GitCommit=${{ github.sha }} outputs: \"type=registry,push=true\" tags: | ghcr.io/alexellis/gha-sbom:${{ github.sha }} Upon each commit, an image is published to GitHub's Container Registry with the image name of: ghcr.io/np-completed/gha-sbom:SHA . To generate an SBOM, we just need to update the docker/build-push-action to use the sbom flag: - name: Local build id: local_build uses: docker/build-push-action@v3 with: sbom: true provenance: false View the SBOM: syft ghcr.io/np-completed/gha-sbom:840fc5b2f3c94099e5dc74434a6bbcc156407a6f \u2714 Loaded image \u2714 Parsed image \u2714 Cataloged packages [ 21 packages ] NAME VERSION TYPE alpine-baselayout 3 .4.0-r0 apk alpine-baselayout-data 3 .4.0-r0 apk alpine-keys 2 .4-r1 apk apk-tools 2 .12.10-r1 apk brotli-libs 1 .0.9-r9 apk busybox 1 .35.0 binary busybox 1 .35.0-r29 apk busybox-binsh 1 .35.0-r29 apk ca-certificates 20220614 -r4 apk ca-certificates-bundle 20220614 -r2 apk curl 7 .87.0-r1 apk libc-utils 0 .7.2-r3 apk libcrypto3 3 .0.7-r0 apk libcurl 7 .87.0-r1 apk libssl3 3 .0.7-r0 apk musl 1 .2.3-r4 apk musl-utils 1 .2.3-r4 apk nghttp2-libs 1 .51.0-r0 apk scanelf 1 .3.5-r1 apk ssl_client 1 .35.0-r29 apk zlib 1 .2.13-r0 apk grype ghcr.io/np-completed/gha-sbom:840fc5b2f3c94099e5dc74434a6bbcc156407a6f \u2714 Vulnerability DB [ no update available ] \u2714 Pulled image \u2714 Loaded image \u2714 Parsed image \u2714 Cataloged packages [ 21 packages ] \u2714 Scanned image [ 2 vulnerabilities ] NAME INSTALLED FIXED-IN TYPE VULNERABILITY SEVERITY libcrypto3 3 .0.7-r0 3 .0.7-r2 apk CVE-2022-3996 High libssl3 3 .0.7-r0 3 .0.7-r2 apk CVE-2022-3996 High The image: alpine:3.17.0 contains two High vulnerabilities, and from reading the notes, we can see that both have been fixed. We can resolve the issue by changing the Dockerfile to use alpine:latest instead, and re-running the build. syft ghcr.io/np-completed/gha-sbom:1a7f4c36bad2a506d7403fe4dd904a356bd8bbfa \u2714 Pulled image \u2714 Loaded image \u2714 Parsed image \u2714 Cataloged packages [ 21 packages ] NAME VERSION TYPE alpine-baselayout 3 .4.0-r0 apk alpine-baselayout-data 3 .4.0-r0 apk alpine-keys 2 .4-r1 apk apk-tools 2 .12.10-r1 apk brotli-libs 1 .0.9-r9 apk busybox 1 .35.0 binary busybox 1 .35.0-r29 apk busybox-binsh 1 .35.0-r29 apk ca-certificates 20220614 -r4 apk ca-certificates-bundle 20220614 -r4 apk curl 7 .87.0-r1 apk libc-utils 0 .7.2-r3 apk libcrypto3 3 .0.7-r2 apk libcurl 7 .87.0-r1 apk libssl3 3 .0.7-r2 apk musl 1 .2.3-r4 apk musl-utils 1 .2.3-r4 apk nghttp2-libs 1 .51.0-r0 apk scanelf 1 .3.5-r1 apk ssl_client 1 .35.0-r29 apk zlib 1 .2.13-r0 apk grype ghcr.io/np-completed/gha-sbom:1a7f4c36bad2a506d7403fe4dd904a356bd8bbfa \u2714 Vulnerability DB [ no update available ] \u2714 Loaded image \u2714 Parsed image \u2714 Cataloged packages [ 21 packages ] \u2714 Scanned image [ 0 vulnerabilities ] Install Grype and Syft \u00b6 Anchore provides commercial solutions for creating, managing and inspecting SBOMs, however they also have two very useful open source tools that we can try out for free. syft - a command line tool that can be used to generate an SBOM for a container image. grype - a command line tool that can be used to scan an SBOM for vulnerabilities. Grype \u00b6 Run the following command to install the latest version of Grype to the /usr/local/bin directory: wget -qO - https://raw.githubusercontent.com/anchore/grype/main/install.sh | sudo bash -s -- -b /usr/local/bin ## Verify install worked grype version Run the grype command and specify the container image as argument: $ grype ubuntu:latest \u2714 Vulnerability DB [ updated ] \u2714 Pulled image \u2714 Loaded image \u2714 Parsed image \u2714 Cataloged packages [ 101 packages ] \u2714 Scanned image [ 18 vulnerabilities ] NAME INSTALLED FIXED-IN TYPE VULNERABILITY SEVERITY bash 5 .1-6ubuntu1 deb CVE-2022-3715 Low coreutils 8 .32-4.1ubuntu1 deb CVE-2016-2781 Low gpgv 2 .2.27-3ubuntu2.1 deb CVE-2022-3219 Low libc-bin 2 .35-0ubuntu3.1 deb CVE-2016-20013 Negligible libc6 2 .35-0ubuntu3.1 deb CVE-2016-20013 Negligible libgssapi-krb5-2 1 .19.2-2 deb CVE-2022-42898 Medium libk5crypto3 1 .19.2-2 deb CVE-2022-42898 Medium Uninstall grype \u00b6 sudo rm -rf /usr/local/bin/grype and remove vulnerabilities database rm -rf ~/.cache/grype Install Syft \u00b6 Run the following command to install the latest version of Syft to the /usr/local/bin directory: wget -qO - https://raw.githubusercontent.com/anchore/syft/main/install.sh | sudo bash -s -- -b /usr/local/bin ## Verify install worked syft version Application: syft Version: 0 .68.1 JsonSchemaVersion: 6 .2.0 BuildDate: 2023 -01-25T17:46:33Z GitCommit: 4c0aef09b8d7fb78200b04416f474b90b79370de GitDescription: v0.68.1 Platform: linux/amd64 GoVersion: go1.18.10 Compiler: gc Test Syft \u00b6 Generate an SBOM for a container image: syft <image> The above output includes only software that is visible in the container (i.e., the squashed representation of the image). To include software from all image layers in the SBOM, regardless of its presence in the final image, provide --scope all-layers : syft <image> --scope all-layers","title":"Software Bill of Materials"},{"location":"gh_actions/sbom/#sbom-software-bill-of-material-with-docker-container-and-github-actions","text":"Generate an SBOM from within a GitHub Actions workflow. In this way, the SBOM is shipped with the container image and is made available without having to scan the image each time.","title":"SBOM Software Bill of Material with Docker Container and Github Actions"},{"location":"gh_actions/sbom/#what-is-sbom","text":"From Wikipedia: A software bill of materials ( SBOM ) is a list of components in a piece of software . Software vendors often create products by assembling open source and commercial software components . The SBOM describes the components in a product . It is analogous to a list of ingredients on food packaging : where you might consult a label to avoid foods that may cause an allergies , SBOMs can help companies avoid consumption of software that could harm their organization . The concept of a BOM is well - established in traditional manufacturing as part of supply chain management . A manufacturer uses a BOM to track the parts it uses to create a product . If defects are later found in a specific part , the BOM makes it easy to locate affected products .","title":"What is SBOM?"},{"location":"gh_actions/sbom/#crane","text":"crane is a tool for interacting with remote images and registries. Install crane sudo snap install go --classic ## Install go go install github.com/google/go-containerregistry/cmd/crane@latest curl -sL \"https://github.com/google/go-containerregistry/releases/latest/download/go-containerregistry_Linux_x86_64.tar.gz\" > go-containerregistry.tar.gz curl -sL https://api.github.com/repos/konveyor/crane/releases/latest | jq -r \".assets[] | select(.name | contains(\\\"amd64-linux\\\")) | .browser_download_url\" | wget -i-","title":"Crane"},{"location":"gh_actions/sbom/#imagine-you-have-the-following-dockerfile","text":"FROM alpine:3.17.0 RUN apk add --no-cache curl ca-certificates CMD [ \"curl\" , \"https://www.google.com\" ] I know that there's a vulnerability in alpine 3.17.0 in the OpenSSL library. How do I know that? I recently updated every OpenFaaS Pro component to use 3.17.1 to fix a specific vulnerability. Now a typical workflow for this Dockerfile would look like the below: name: build on: push: branches: [ master, main ] pull_request: branches: [ master, main ] permissions: actions: read checks: write contents: read packages: write jobs: publish: runs-on: ubuntu-latest steps: - uses: actions/checkout@master with: fetch-depth: 1 - name: Build Docker image run: docker build . --file Dockerfile.sbom --tag my-image-name:$(date +%s) # - name: Set up Docker Buildx # uses: docker/setup-buildx-action@v2 # - name: Login to Docker Registry # uses: docker/login-action@v2 # with: # username: ${{ github.repository_owner }} # password: ${{ secrets.GITHUB_TOKEN }} # registry: ghcr.io - name: Publish image uses: docker/build-push-action@v3 with: build-args: | GitCommit=${{ github.sha }} outputs: \"type=registry,push=true\" tags: | ghcr.io/alexellis/gha-sbom:${{ github.sha }} Upon each commit, an image is published to GitHub's Container Registry with the image name of: ghcr.io/np-completed/gha-sbom:SHA . To generate an SBOM, we just need to update the docker/build-push-action to use the sbom flag: - name: Local build id: local_build uses: docker/build-push-action@v3 with: sbom: true provenance: false View the SBOM: syft ghcr.io/np-completed/gha-sbom:840fc5b2f3c94099e5dc74434a6bbcc156407a6f \u2714 Loaded image \u2714 Parsed image \u2714 Cataloged packages [ 21 packages ] NAME VERSION TYPE alpine-baselayout 3 .4.0-r0 apk alpine-baselayout-data 3 .4.0-r0 apk alpine-keys 2 .4-r1 apk apk-tools 2 .12.10-r1 apk brotli-libs 1 .0.9-r9 apk busybox 1 .35.0 binary busybox 1 .35.0-r29 apk busybox-binsh 1 .35.0-r29 apk ca-certificates 20220614 -r4 apk ca-certificates-bundle 20220614 -r2 apk curl 7 .87.0-r1 apk libc-utils 0 .7.2-r3 apk libcrypto3 3 .0.7-r0 apk libcurl 7 .87.0-r1 apk libssl3 3 .0.7-r0 apk musl 1 .2.3-r4 apk musl-utils 1 .2.3-r4 apk nghttp2-libs 1 .51.0-r0 apk scanelf 1 .3.5-r1 apk ssl_client 1 .35.0-r29 apk zlib 1 .2.13-r0 apk grype ghcr.io/np-completed/gha-sbom:840fc5b2f3c94099e5dc74434a6bbcc156407a6f \u2714 Vulnerability DB [ no update available ] \u2714 Pulled image \u2714 Loaded image \u2714 Parsed image \u2714 Cataloged packages [ 21 packages ] \u2714 Scanned image [ 2 vulnerabilities ] NAME INSTALLED FIXED-IN TYPE VULNERABILITY SEVERITY libcrypto3 3 .0.7-r0 3 .0.7-r2 apk CVE-2022-3996 High libssl3 3 .0.7-r0 3 .0.7-r2 apk CVE-2022-3996 High The image: alpine:3.17.0 contains two High vulnerabilities, and from reading the notes, we can see that both have been fixed. We can resolve the issue by changing the Dockerfile to use alpine:latest instead, and re-running the build. syft ghcr.io/np-completed/gha-sbom:1a7f4c36bad2a506d7403fe4dd904a356bd8bbfa \u2714 Pulled image \u2714 Loaded image \u2714 Parsed image \u2714 Cataloged packages [ 21 packages ] NAME VERSION TYPE alpine-baselayout 3 .4.0-r0 apk alpine-baselayout-data 3 .4.0-r0 apk alpine-keys 2 .4-r1 apk apk-tools 2 .12.10-r1 apk brotli-libs 1 .0.9-r9 apk busybox 1 .35.0 binary busybox 1 .35.0-r29 apk busybox-binsh 1 .35.0-r29 apk ca-certificates 20220614 -r4 apk ca-certificates-bundle 20220614 -r4 apk curl 7 .87.0-r1 apk libc-utils 0 .7.2-r3 apk libcrypto3 3 .0.7-r2 apk libcurl 7 .87.0-r1 apk libssl3 3 .0.7-r2 apk musl 1 .2.3-r4 apk musl-utils 1 .2.3-r4 apk nghttp2-libs 1 .51.0-r0 apk scanelf 1 .3.5-r1 apk ssl_client 1 .35.0-r29 apk zlib 1 .2.13-r0 apk grype ghcr.io/np-completed/gha-sbom:1a7f4c36bad2a506d7403fe4dd904a356bd8bbfa \u2714 Vulnerability DB [ no update available ] \u2714 Loaded image \u2714 Parsed image \u2714 Cataloged packages [ 21 packages ] \u2714 Scanned image [ 0 vulnerabilities ]","title":"Imagine you have the following Dockerfile:"},{"location":"gh_actions/sbom/#install-grype-and-syft","text":"Anchore provides commercial solutions for creating, managing and inspecting SBOMs, however they also have two very useful open source tools that we can try out for free. syft - a command line tool that can be used to generate an SBOM for a container image. grype - a command line tool that can be used to scan an SBOM for vulnerabilities.","title":"Install Grype and Syft"},{"location":"gh_actions/sbom/#grype","text":"Run the following command to install the latest version of Grype to the /usr/local/bin directory: wget -qO - https://raw.githubusercontent.com/anchore/grype/main/install.sh | sudo bash -s -- -b /usr/local/bin ## Verify install worked grype version Run the grype command and specify the container image as argument: $ grype ubuntu:latest \u2714 Vulnerability DB [ updated ] \u2714 Pulled image \u2714 Loaded image \u2714 Parsed image \u2714 Cataloged packages [ 101 packages ] \u2714 Scanned image [ 18 vulnerabilities ] NAME INSTALLED FIXED-IN TYPE VULNERABILITY SEVERITY bash 5 .1-6ubuntu1 deb CVE-2022-3715 Low coreutils 8 .32-4.1ubuntu1 deb CVE-2016-2781 Low gpgv 2 .2.27-3ubuntu2.1 deb CVE-2022-3219 Low libc-bin 2 .35-0ubuntu3.1 deb CVE-2016-20013 Negligible libc6 2 .35-0ubuntu3.1 deb CVE-2016-20013 Negligible libgssapi-krb5-2 1 .19.2-2 deb CVE-2022-42898 Medium libk5crypto3 1 .19.2-2 deb CVE-2022-42898 Medium","title":"Grype"},{"location":"gh_actions/sbom/#uninstall-grype","text":"sudo rm -rf /usr/local/bin/grype and remove vulnerabilities database rm -rf ~/.cache/grype","title":"Uninstall grype"},{"location":"gh_actions/sbom/#install-syft","text":"Run the following command to install the latest version of Syft to the /usr/local/bin directory: wget -qO - https://raw.githubusercontent.com/anchore/syft/main/install.sh | sudo bash -s -- -b /usr/local/bin ## Verify install worked syft version Application: syft Version: 0 .68.1 JsonSchemaVersion: 6 .2.0 BuildDate: 2023 -01-25T17:46:33Z GitCommit: 4c0aef09b8d7fb78200b04416f474b90b79370de GitDescription: v0.68.1 Platform: linux/amd64 GoVersion: go1.18.10 Compiler: gc","title":"Install Syft"},{"location":"gh_actions/sbom/#test-syft","text":"Generate an SBOM for a container image: syft <image> The above output includes only software that is visible in the container (i.e., the squashed representation of the image). To include software from all image layers in the SBOM, regardless of its presence in the final image, provide --scope all-layers : syft <image> --scope all-layers","title":"Test Syft"},{"location":"gh_actions/share_docker_job/","text":"Share Artifacts \u00b6 Sharing a Docker image between two jobs, here is how I did it: jobs: docker-build: name: Docker build runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v2 - name: Build Docker image run: | docker build -t foo/bar:$GITHUB_SHA mkdir -p path/to/artifacts docker save foo/bar:$GITHUB_SHA > path/to/artifacts/docker-image.tar - name: Temporarily save Docker image uses: actions/upload-artifact@v2 with: name: docker-artifact path: path/to/artifacts retention-days: 1 docker-deploy: name: Deploy to Docker Hub runs-on: ubuntu-latest needs: docker-build steps: - name: Checkout uses: actions/checkout@v2 - name: Retrieve saved Docker image uses: actions/download-artifact@v2 with: name: docker-artifact path: path/to/artifacts - name: Docker load run: | cd path/to/artifacts docker load < docker-image.tar # docker_build_push.sh","title":"Share Artifacts"},{"location":"gh_actions/share_docker_job/#share-artifacts","text":"Sharing a Docker image between two jobs, here is how I did it: jobs: docker-build: name: Docker build runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v2 - name: Build Docker image run: | docker build -t foo/bar:$GITHUB_SHA mkdir -p path/to/artifacts docker save foo/bar:$GITHUB_SHA > path/to/artifacts/docker-image.tar - name: Temporarily save Docker image uses: actions/upload-artifact@v2 with: name: docker-artifact path: path/to/artifacts retention-days: 1 docker-deploy: name: Deploy to Docker Hub runs-on: ubuntu-latest needs: docker-build steps: - name: Checkout uses: actions/checkout@v2 - name: Retrieve saved Docker image uses: actions/download-artifact@v2 with: name: docker-artifact path: path/to/artifacts - name: Docker load run: | cd path/to/artifacts docker load < docker-image.tar # docker_build_push.sh","title":"Share Artifacts"},{"location":"gh_actions/url_test_in_cloudfront/","text":"Github Actions \u00b6 Check URL Links \u00b6 Check for broken url links in published markdown html. The json and yaml files are required. links.yml is located in .github/workflows/links.yml name : Check Markdown Links on : workflow_call jobs : markdown-link-check : runs-on : ubuntu-latest env : ZENML_DEBUG : 1 ZENML_ANALYTICS_OPT_IN : false steps : - uses : actions/checkout@master - uses : gaurav-nelson/github-action-markdown-link-check@v1 with : use-quiet-mode : 'yes' use-verbose-mode : 'no' config-file : .github/workflows/markdown_check_config.json continue-on-error : true { \"ignorePatterns\" : [ { \"pattern\" : \"^http://0.0.0.0\" }, { \"pattern\" : \"^http://127.0.0.1\" }, { \"pattern\" : \"^http://localhost\" } ] } Broken links report to issue \u00b6 name : Broken Links Report to GH Issues on : push : branches : - main repository_dispatch : workflow_dispatch : # schedule: # - cron: \"00 18 * * *\" concurrency : # New commit on branch cancels running workflows of the same branch group : ${{ github.workflow }}-${{ github.ref }} cancel-in-progress : true jobs : links : runs-on : ubuntu-latest steps : - uses : actions/checkout@v2.3.4 - name : Link Checker uses : lycheeverse/lychee-action@v1.5.1 with : # TODO TOML is causing kernel panick and fails to render report --config .github/lychee.toml # TODO --exclude-path .lychee.excludes NOT WORKING AS EXPECTED args : > --exclude-mail --no-progress --timeout 45 './**/*.md' './**/*.html' # --cache --max-cache-age 1d -- **/*.md *.md **/*.html # --verbose # --accept=200,403,429 # --exclude-mail **/*.html **/*.md **/*.txt **/*.json --exclude-file .lychee.excludes # --exclude-all-private --insecure # --max-retries 4 # -- **/*.md *.md '**/*.md' # --cache --max-cache-age 1d '**/*.md' README.md patterns/ output : out.md jobSummary : true env : GITHUB_TOKEN : ${{ secrets.actions }} - name : Create Issue From File # if: github.event_name != 'pull_request' # if: steps.lychee.outputs.exit_code != 0 uses : peter-evans/create-issue-from-file@v4 with : title : Link Checker Report content-filepath : out.md # token: ${{ secrets.actions }} #content-filepath: ./link-checker/out.md labels : | report links automated issue issue-number : 15 Use file to ignore URLs. Add .lycheeingnore to project root directory: file://* http://127.0.0.1:8000/ ON Event \u00b6 ## Manual Trigger from Github UI on : workflow_dispatch : ## Push Trigger on : push : paths : - 'docs/**' - 'mkdocs.yml' - '.github/workflows/main.yml' - 'pyproject.toml' branches : - main pull_request : # The branches below must be a subset of the branches above ## types: [opened, synchronize, reopened] branches : [ \"main\" ] concurrency : # New commit on branch cancels running workflows of the same branch group : ${{ github.workflow }}-${{ github.ref }} cancel-in-progress : true ## Workflow Call on : workflow_call Reusable workflows \u00b6 TODO Workflow for Github Actions \u00b6 TODO Using Github Actions to push to AWS ECR with Credentials \u00b6 https://aws.plainenglish.io/build-a-docker-image-and-publish-it-to-aws-ecr-using-github-actions-f20accd774c3 Using Github Actions OpenID Connect to push to AWS ECR without Credentials \u00b6 https://blog.tedivm.com/guides/2021/10/github-actions-push-to-aws-ecr-without-credentials-oidc/ Pushing the Container \u00b6 WIth all that out of the way here\u2019s full action to build and deploy an image to ECR. In this we\u2019re chaining together a variety of published actions from other vendors- actions/checkout to actually pull the repository. docker/setup-qemu-action to install an emulation layer to build multiplatform images. docker/setup-buildx-action to use the docker buildx system, again for multiplatform images. aws-actions/configure-aws-credentials to log into AWS. aws-actions/amazon-ecr-login to log into AWS ECR. docker/metadata-action to create a bunch of tags for our docker container. docker/build-push-action to push the container.","title":"Github Actions"},{"location":"gh_actions/url_test_in_cloudfront/#github-actions","text":"","title":"Github Actions"},{"location":"gh_actions/url_test_in_cloudfront/#check-url-links","text":"Check for broken url links in published markdown html. The json and yaml files are required. links.yml is located in .github/workflows/links.yml name : Check Markdown Links on : workflow_call jobs : markdown-link-check : runs-on : ubuntu-latest env : ZENML_DEBUG : 1 ZENML_ANALYTICS_OPT_IN : false steps : - uses : actions/checkout@master - uses : gaurav-nelson/github-action-markdown-link-check@v1 with : use-quiet-mode : 'yes' use-verbose-mode : 'no' config-file : .github/workflows/markdown_check_config.json continue-on-error : true { \"ignorePatterns\" : [ { \"pattern\" : \"^http://0.0.0.0\" }, { \"pattern\" : \"^http://127.0.0.1\" }, { \"pattern\" : \"^http://localhost\" } ] }","title":"Check URL Links"},{"location":"gh_actions/url_test_in_cloudfront/#broken-links-report-to-issue","text":"name : Broken Links Report to GH Issues on : push : branches : - main repository_dispatch : workflow_dispatch : # schedule: # - cron: \"00 18 * * *\" concurrency : # New commit on branch cancels running workflows of the same branch group : ${{ github.workflow }}-${{ github.ref }} cancel-in-progress : true jobs : links : runs-on : ubuntu-latest steps : - uses : actions/checkout@v2.3.4 - name : Link Checker uses : lycheeverse/lychee-action@v1.5.1 with : # TODO TOML is causing kernel panick and fails to render report --config .github/lychee.toml # TODO --exclude-path .lychee.excludes NOT WORKING AS EXPECTED args : > --exclude-mail --no-progress --timeout 45 './**/*.md' './**/*.html' # --cache --max-cache-age 1d -- **/*.md *.md **/*.html # --verbose # --accept=200,403,429 # --exclude-mail **/*.html **/*.md **/*.txt **/*.json --exclude-file .lychee.excludes # --exclude-all-private --insecure # --max-retries 4 # -- **/*.md *.md '**/*.md' # --cache --max-cache-age 1d '**/*.md' README.md patterns/ output : out.md jobSummary : true env : GITHUB_TOKEN : ${{ secrets.actions }} - name : Create Issue From File # if: github.event_name != 'pull_request' # if: steps.lychee.outputs.exit_code != 0 uses : peter-evans/create-issue-from-file@v4 with : title : Link Checker Report content-filepath : out.md # token: ${{ secrets.actions }} #content-filepath: ./link-checker/out.md labels : | report links automated issue issue-number : 15 Use file to ignore URLs. Add .lycheeingnore to project root directory: file://* http://127.0.0.1:8000/","title":"Broken links report to issue"},{"location":"gh_actions/url_test_in_cloudfront/#on-event","text":"## Manual Trigger from Github UI on : workflow_dispatch : ## Push Trigger on : push : paths : - 'docs/**' - 'mkdocs.yml' - '.github/workflows/main.yml' - 'pyproject.toml' branches : - main pull_request : # The branches below must be a subset of the branches above ## types: [opened, synchronize, reopened] branches : [ \"main\" ] concurrency : # New commit on branch cancels running workflows of the same branch group : ${{ github.workflow }}-${{ github.ref }} cancel-in-progress : true ## Workflow Call on : workflow_call","title":"ON Event"},{"location":"gh_actions/url_test_in_cloudfront/#reusable-workflows","text":"TODO","title":"Reusable workflows"},{"location":"gh_actions/url_test_in_cloudfront/#workflow-for-github-actions","text":"TODO","title":"Workflow for Github Actions"},{"location":"gh_actions/url_test_in_cloudfront/#using-github-actions-to-push-to-aws-ecr-with-credentials","text":"https://aws.plainenglish.io/build-a-docker-image-and-publish-it-to-aws-ecr-using-github-actions-f20accd774c3","title":"Using Github Actions to push to AWS ECR with Credentials"},{"location":"gh_actions/url_test_in_cloudfront/#using-github-actions-openid-connect-to-push-to-aws-ecr-without-credentials","text":"https://blog.tedivm.com/guides/2021/10/github-actions-push-to-aws-ecr-without-credentials-oidc/","title":"Using Github Actions OpenID Connect to push to AWS ECR without Credentials"},{"location":"gh_actions/url_test_in_cloudfront/#pushing-the-container","text":"WIth all that out of the way here\u2019s full action to build and deploy an image to ECR. In this we\u2019re chaining together a variety of published actions from other vendors- actions/checkout to actually pull the repository. docker/setup-qemu-action to install an emulation layer to build multiplatform images. docker/setup-buildx-action to use the docker buildx system, again for multiplatform images. aws-actions/configure-aws-credentials to log into AWS. aws-actions/amazon-ecr-login to log into AWS ECR. docker/metadata-action to create a bunch of tags for our docker container. docker/build-push-action to push the container.","title":"Pushing the Container"},{"location":"gh_actions/checks/actions/","text":"Actions Connection Check \u00b6 What is this check for? \u00b6 Make sure the runner has access to actions service for GitHub.com or GitHub Enterprise Server For GitHub.com The runner needs to access https://api.github.com for downloading actions. The runner needs to access https://vstoken.actions.githubusercontent.com/_apis/.../ for requesting an access token. The runner needs to access https://pipelines.actions.githubusercontent.com/_apis/.../ for receiving workflow jobs. These can by tested by running the following curl commands from your self-hosted runner machine: ``` curl -v https://api.github.com/api/v3/zen curl -v https://vstoken.actions.githubusercontent.com/_apis/health curl -v https://pipelines.actions.githubusercontent.com/_apis/health ``` For GitHub Enterprise Server The runner needs to access https://[hostname]/api/v3 for downloading actions. The runner needs to access https://[hostname]/_services/vstoken/_apis/.../ for requesting an access token. The runner needs to access https://[hostname]/_services/pipelines/_apis/.../ for receiving workflow jobs. These can by tested by running the following curl commands from your self-hosted runner machine, replacing [hostname] with the hostname of your appliance, for instance github.example.com : ``` curl - v https : //[ hostname ]/ api / v3 / zen curl - v https : //[ hostname ]/ _services / vstoken / _apis / health curl - v https : //[ hostname ]/ _services / pipelines / _apis / health ``` A common cause of this these connectivity issues is if your to GitHub Enterprise Server appliance is using [ the self-signed certificate that is enabled the first time ] ( https : // docs . github . com / en / enterprise - server / admin / configuration / configuring - network - settings / configuring - tls ) your appliance is started . As self - signed certificates are not trusted by web browsers and Git clients , these clients ( including the GitHub Actions runner ) will report certificate warnings . We recommend [ upload a certificate signed by a trusted authority ] ( https : // docs . github . com / en / enterprise - server / admin / configuration / configuring - network - settings / configuring - tls ) to GitHub Enterprise Server , or enabling the built - in ] [ Let's Encrypt support ] ( https : // docs . github . com / en / enterprise - server / admin / configuration / configuring - network - settings / configuring - tls ). What is checked? \u00b6 DNS lookup for api.github.com or myGHES.com using dotnet Ping api.github.com or myGHES.com using dotnet Make HTTP GET to https://api.github.com or https://myGHES.com/api/v3 using dotnet, check response headers contains X-GitHub-Request-Id DNS lookup for vstoken.actions.githubusercontent.com using dotnet Ping vstoken.actions.githubusercontent.com using dotnet Make HTTP GET to https://vstoken.actions.githubusercontent.com/_apis/health or https://myGHES.com/_services/vstoken/_apis/health using dotnet, check response headers contains x-vss-e2eid DNS lookup for pipelines.actions.githubusercontent.com using dotnet Ping pipelines.actions.githubusercontent.com using dotnet Make HTTP GET to https://pipelines.actions.githubusercontent.com/_apis/health or https://myGHES.com/_services/pipelines/_apis/health using dotnet, check response headers contains x-vss-e2eid Make HTTP POST to https://pipelines.actions.githubusercontent.com/_apis/health or https://myGHES.com/_services/pipelines/_apis/health using dotnet, check response headers contains x-vss-e2eid How to fix the issue? \u00b6 1. Check the common network issue \u00b6 Please check the network doc 2. SSL certificate related issue \u00b6 If you are seeing System.Net.Http.HttpRequestException: The SSL connection could not be established, see inner exception. in the log, it means the runner can't connect to Actions service due to SSL handshake failure. Please check the SSL cert doc Still not working? \u00b6 Contact GitHub Support if you have further questuons, or log an issue at https://github.com/actions/runner if you think it's a runner issue.","title":"Actions"},{"location":"gh_actions/checks/actions/#actions-connection-check","text":"","title":"Actions Connection Check"},{"location":"gh_actions/checks/actions/#what-is-this-check-for","text":"Make sure the runner has access to actions service for GitHub.com or GitHub Enterprise Server For GitHub.com The runner needs to access https://api.github.com for downloading actions. The runner needs to access https://vstoken.actions.githubusercontent.com/_apis/.../ for requesting an access token. The runner needs to access https://pipelines.actions.githubusercontent.com/_apis/.../ for receiving workflow jobs. These can by tested by running the following curl commands from your self-hosted runner machine: ``` curl -v https://api.github.com/api/v3/zen curl -v https://vstoken.actions.githubusercontent.com/_apis/health curl -v https://pipelines.actions.githubusercontent.com/_apis/health ``` For GitHub Enterprise Server The runner needs to access https://[hostname]/api/v3 for downloading actions. The runner needs to access https://[hostname]/_services/vstoken/_apis/.../ for requesting an access token. The runner needs to access https://[hostname]/_services/pipelines/_apis/.../ for receiving workflow jobs. These can by tested by running the following curl commands from your self-hosted runner machine, replacing [hostname] with the hostname of your appliance, for instance github.example.com : ``` curl - v https : //[ hostname ]/ api / v3 / zen curl - v https : //[ hostname ]/ _services / vstoken / _apis / health curl - v https : //[ hostname ]/ _services / pipelines / _apis / health ``` A common cause of this these connectivity issues is if your to GitHub Enterprise Server appliance is using [ the self-signed certificate that is enabled the first time ] ( https : // docs . github . com / en / enterprise - server / admin / configuration / configuring - network - settings / configuring - tls ) your appliance is started . As self - signed certificates are not trusted by web browsers and Git clients , these clients ( including the GitHub Actions runner ) will report certificate warnings . We recommend [ upload a certificate signed by a trusted authority ] ( https : // docs . github . com / en / enterprise - server / admin / configuration / configuring - network - settings / configuring - tls ) to GitHub Enterprise Server , or enabling the built - in ] [ Let's Encrypt support ] ( https : // docs . github . com / en / enterprise - server / admin / configuration / configuring - network - settings / configuring - tls ).","title":"What is this check for?"},{"location":"gh_actions/checks/actions/#what-is-checked","text":"DNS lookup for api.github.com or myGHES.com using dotnet Ping api.github.com or myGHES.com using dotnet Make HTTP GET to https://api.github.com or https://myGHES.com/api/v3 using dotnet, check response headers contains X-GitHub-Request-Id DNS lookup for vstoken.actions.githubusercontent.com using dotnet Ping vstoken.actions.githubusercontent.com using dotnet Make HTTP GET to https://vstoken.actions.githubusercontent.com/_apis/health or https://myGHES.com/_services/vstoken/_apis/health using dotnet, check response headers contains x-vss-e2eid DNS lookup for pipelines.actions.githubusercontent.com using dotnet Ping pipelines.actions.githubusercontent.com using dotnet Make HTTP GET to https://pipelines.actions.githubusercontent.com/_apis/health or https://myGHES.com/_services/pipelines/_apis/health using dotnet, check response headers contains x-vss-e2eid Make HTTP POST to https://pipelines.actions.githubusercontent.com/_apis/health or https://myGHES.com/_services/pipelines/_apis/health using dotnet, check response headers contains x-vss-e2eid","title":"What is checked?"},{"location":"gh_actions/checks/actions/#how-to-fix-the-issue","text":"","title":"How to fix the issue?"},{"location":"gh_actions/checks/actions/#1-check-the-common-network-issue","text":"Please check the network doc","title":"1. Check the common network issue"},{"location":"gh_actions/checks/actions/#2-ssl-certificate-related-issue","text":"If you are seeing System.Net.Http.HttpRequestException: The SSL connection could not be established, see inner exception. in the log, it means the runner can't connect to Actions service due to SSL handshake failure. Please check the SSL cert doc","title":"2. SSL certificate related issue"},{"location":"gh_actions/checks/actions/#still-not-working","text":"Contact GitHub Support if you have further questuons, or log an issue at https://github.com/actions/runner if you think it's a runner issue.","title":"Still not working?"},{"location":"gh_actions/checks/auth/","text":"Runner Authentication and Authorization \u00b6 Goals \u00b6 Support runner installs in untrusted domains. The account that configures or runs the runner process is not relevant for accessing GitHub resources. Accessing GitHub resources is done with a per-job token which expires when job completes. The token is granted to trusted parts of the system including the runner, actions and script steps specified by the workflow author as trusted. All OAuth tokens that come from the Token Service that the runner uses to access Actions Service resources are the same. It's just the scope and expiration of the token that may vary. Configuration \u00b6 Configuring a self-hosted runner is covered here in the documentation . Configuration is done with the user being authenticated via a time-limited, GitHub runner registration token. Your credentials are never used for registering the runner with the service. During configuration, an RSA public/private key pair is created, the private key is stored in file on disk. On Windows, the content is protected with DPAPI (machine level encrypted - runner only valid on that machine) and on Linux/OSX with chmod permissions. Using your credentials, the runner is registered with the service by sending the public key to the service which adds that runner to the pool and stores the public key, the Token Service will generate a clientId associated with the public key. Start and Listen \u00b6 After configuring the runner, the runner can be started interactively ( ./run.cmd or ./run.sh ) or as a service. On start, the runner listener process loads the RSA private key (on Windows decrypting with machine key DPAPI), and asks the Token Service for an OAuth token which is signed with the RSA private key. The server then responds with an OAuth token that grants permission to access the message queue (HTTP long poll), allowing the runner to acquire the messages it will eventually run. Run a workflow \u00b6 When a workflow is run, its labels are evaluated, it is matched to a runner and a message is placed in a queue of messages for that runner. The runner then starts listening for jobs via the message queue HTTP long poll. The message is encrypted with the runner's public key, stored during runner configuration. A workflow is queued as a result of a triggered event . Workflows can be scheduled to run at specific UTC times using POSIX cron syntax. An OAuth token is generated, granting limited access to the host in Actions Service associated with the github.com repository/organization. The lifetime of the OAuth token is the lifetime of the run or at most the job timeout (default: 6 hours) , plus 10 additional minutes. Accessing GitHub resources \u00b6 The job message sent to the runner contains the OAuth token to talk back to the Actions Service. The runner listener parent process will spawn a runner worker process for that job and send it the job message over IPC. The token is never persisted. Each action is run as a unique subprocess. The encrypted access token will be provided as an environment variable in each action subprocess. The token is registered with the runner as a secret and scrubbed from the logs as they are written. Authentication in a workflow run to github.com can be accomplished by using the GITHUB_TOKEN ) secret. This token expires after 60 minutes. Please note that this token is different from the OAuth token that the runner uses to talk to the Actions Service. Hosted runner authentication \u00b6 Hosted runner authentication differs from self-hosted authentication in that runners do not undergo a registration process, but instead, the hosted runners get the OAuth token directly by reading the .credentials file. The scope of this particular token is limited for a given workflow job execution, and the token is revoked as soon as the job is finished.","title":"Auth"},{"location":"gh_actions/checks/auth/#runner-authentication-and-authorization","text":"","title":"Runner Authentication and Authorization"},{"location":"gh_actions/checks/auth/#goals","text":"Support runner installs in untrusted domains. The account that configures or runs the runner process is not relevant for accessing GitHub resources. Accessing GitHub resources is done with a per-job token which expires when job completes. The token is granted to trusted parts of the system including the runner, actions and script steps specified by the workflow author as trusted. All OAuth tokens that come from the Token Service that the runner uses to access Actions Service resources are the same. It's just the scope and expiration of the token that may vary.","title":"Goals"},{"location":"gh_actions/checks/auth/#configuration","text":"Configuring a self-hosted runner is covered here in the documentation . Configuration is done with the user being authenticated via a time-limited, GitHub runner registration token. Your credentials are never used for registering the runner with the service. During configuration, an RSA public/private key pair is created, the private key is stored in file on disk. On Windows, the content is protected with DPAPI (machine level encrypted - runner only valid on that machine) and on Linux/OSX with chmod permissions. Using your credentials, the runner is registered with the service by sending the public key to the service which adds that runner to the pool and stores the public key, the Token Service will generate a clientId associated with the public key.","title":"Configuration"},{"location":"gh_actions/checks/auth/#start-and-listen","text":"After configuring the runner, the runner can be started interactively ( ./run.cmd or ./run.sh ) or as a service. On start, the runner listener process loads the RSA private key (on Windows decrypting with machine key DPAPI), and asks the Token Service for an OAuth token which is signed with the RSA private key. The server then responds with an OAuth token that grants permission to access the message queue (HTTP long poll), allowing the runner to acquire the messages it will eventually run.","title":"Start and Listen"},{"location":"gh_actions/checks/auth/#run-a-workflow","text":"When a workflow is run, its labels are evaluated, it is matched to a runner and a message is placed in a queue of messages for that runner. The runner then starts listening for jobs via the message queue HTTP long poll. The message is encrypted with the runner's public key, stored during runner configuration. A workflow is queued as a result of a triggered event . Workflows can be scheduled to run at specific UTC times using POSIX cron syntax. An OAuth token is generated, granting limited access to the host in Actions Service associated with the github.com repository/organization. The lifetime of the OAuth token is the lifetime of the run or at most the job timeout (default: 6 hours) , plus 10 additional minutes.","title":"Run a workflow"},{"location":"gh_actions/checks/auth/#accessing-github-resources","text":"The job message sent to the runner contains the OAuth token to talk back to the Actions Service. The runner listener parent process will spawn a runner worker process for that job and send it the job message over IPC. The token is never persisted. Each action is run as a unique subprocess. The encrypted access token will be provided as an environment variable in each action subprocess. The token is registered with the runner as a secret and scrubbed from the logs as they are written. Authentication in a workflow run to github.com can be accomplished by using the GITHUB_TOKEN ) secret. This token expires after 60 minutes. Please note that this token is different from the OAuth token that the runner uses to talk to the Actions Service.","title":"Accessing GitHub resources"},{"location":"gh_actions/checks/auth/#hosted-runner-authentication","text":"Hosted runner authentication differs from self-hosted authentication in that runners do not undergo a registration process, but instead, the hosted runners get the OAuth token directly by reading the .credentials file. The scope of this particular token is limited for a given workflow job execution, and the token is revoked as soon as the job is finished.","title":"Hosted runner authentication"},{"location":"gh_actions/checks/git/","text":"Git Connection Check \u00b6 What is this check for? \u00b6 Make sure git can access GitHub.com or your GitHub Enterprise Server. What is checked? \u00b6 The test is done by executing # For GitHub.com git ls-remote --exit-code https://github.com/actions/checkout HEAD # For GitHub Enterprise Server git ls-remote --exit-code https://ghes.me/actions/checkout HEAD The test also set environment variable GIT_TRACE=1 and GIT_CURL_VERBOSE=1 before running git ls-remote , this will make git to produce debug log for better debug any potential issues. How to fix the issue? \u00b6 1. Check global and system git config \u00b6 If you are having issues connecting to the server, check your global and system git config for any unexpected authentication headers. You might be seeing an error like: fatal: unable to access 'https://github.com/actions/checkout/': The requested URL returned error: 400 The following commands can be used to check for unexpected authentication headers: $ git config --global --list | grep extraheader http.extraheader=AUTHORIZATION: unexpected_auth_header $ git config --system --list | grep extraheader The following command can be used to remove the above value: git config --global --unset http.extraheader 2. Check the common network issue \u00b6 Please check the network doc 3. SSL certificate related issue \u00b6 If you are seeing SSL Certificate problem: in the log, it means the git can't connect to the GitHub server due to SSL handshake failure. Please check the SSL cert doc Still not working? \u00b6 Contact GitHub customer service or log an issue at https://github.com/actions/runner if you think it's a runner issue.","title":"Git"},{"location":"gh_actions/checks/git/#git-connection-check","text":"","title":"Git Connection Check"},{"location":"gh_actions/checks/git/#what-is-this-check-for","text":"Make sure git can access GitHub.com or your GitHub Enterprise Server.","title":"What is this check for?"},{"location":"gh_actions/checks/git/#what-is-checked","text":"The test is done by executing # For GitHub.com git ls-remote --exit-code https://github.com/actions/checkout HEAD # For GitHub Enterprise Server git ls-remote --exit-code https://ghes.me/actions/checkout HEAD The test also set environment variable GIT_TRACE=1 and GIT_CURL_VERBOSE=1 before running git ls-remote , this will make git to produce debug log for better debug any potential issues.","title":"What is checked?"},{"location":"gh_actions/checks/git/#how-to-fix-the-issue","text":"","title":"How to fix the issue?"},{"location":"gh_actions/checks/git/#1-check-global-and-system-git-config","text":"If you are having issues connecting to the server, check your global and system git config for any unexpected authentication headers. You might be seeing an error like: fatal: unable to access 'https://github.com/actions/checkout/': The requested URL returned error: 400 The following commands can be used to check for unexpected authentication headers: $ git config --global --list | grep extraheader http.extraheader=AUTHORIZATION: unexpected_auth_header $ git config --system --list | grep extraheader The following command can be used to remove the above value: git config --global --unset http.extraheader","title":"1. Check global and system git config"},{"location":"gh_actions/checks/git/#2-check-the-common-network-issue","text":"Please check the network doc","title":"2. Check the common network issue"},{"location":"gh_actions/checks/git/#3-ssl-certificate-related-issue","text":"If you are seeing SSL Certificate problem: in the log, it means the git can't connect to the GitHub server due to SSL handshake failure. Please check the SSL cert doc","title":"3. SSL certificate related issue"},{"location":"gh_actions/checks/git/#still-not-working","text":"Contact GitHub customer service or log an issue at https://github.com/actions/runner if you think it's a runner issue.","title":"Still not working?"},{"location":"gh_actions/checks/internet/","text":"Internet Connection Check \u00b6 What is this check for? \u00b6 Make sure the runner has access to https://api.github.com The runner needs to access https://api.github.com to download any actions from the marketplace. Even the runner is configured to GitHub Enterprise Server, the runner can still download actions from GitHub.com with GitHub Connect What is checked? \u00b6 DNS lookup for api.github.com using dotnet Ping api.github.com using dotnet Make HTTP GET to https://api.github.com using dotnet, check response headers contains X-GitHub-Request-Id How to fix the issue? \u00b6 1. Check the common network issue \u00b6 Please check the network doc Still not working? \u00b6 Contact GitHub customer service or log an issue at https://github.com/actions/runner if you think it's a runner issue.","title":"Internet"},{"location":"gh_actions/checks/internet/#internet-connection-check","text":"","title":"Internet Connection Check"},{"location":"gh_actions/checks/internet/#what-is-this-check-for","text":"Make sure the runner has access to https://api.github.com The runner needs to access https://api.github.com to download any actions from the marketplace. Even the runner is configured to GitHub Enterprise Server, the runner can still download actions from GitHub.com with GitHub Connect","title":"What is this check for?"},{"location":"gh_actions/checks/internet/#what-is-checked","text":"DNS lookup for api.github.com using dotnet Ping api.github.com using dotnet Make HTTP GET to https://api.github.com using dotnet, check response headers contains X-GitHub-Request-Id","title":"What is checked?"},{"location":"gh_actions/checks/internet/#how-to-fix-the-issue","text":"","title":"How to fix the issue?"},{"location":"gh_actions/checks/internet/#1-check-the-common-network-issue","text":"Please check the network doc","title":"1. Check the common network issue"},{"location":"gh_actions/checks/internet/#still-not-working","text":"Contact GitHub customer service or log an issue at https://github.com/actions/runner if you think it's a runner issue.","title":"Still not working?"},{"location":"gh_actions/checks/network/","text":"Common Network Related Issues \u00b6 Common things that can cause the runner to not working properly \u00b6 A bug in the runner or the dotnet framework that causes the actions runner to be unable to make Http requests in a certain network environment. A Proxy or Firewall may block certain HTTP method, such as blocking all POST and PUT calls which the runner will use to upload logs. A Proxy or Firewall may only allows requests with certain user-agent to pass through and the actions runner user-agent is not in the allow list. A Proxy try to decrypt and exam HTTPS traffic for security purpose but cause the actions-runner to fail to finish SSL handshake due to the lack of trusting proxy's CA. The SSL handshake may fail if the client and server do not support the same TLS version, or the same cipher suites. A Proxy may try to modify the HTTPS request (like add or change some http headers) and causes the request become incompatible with the Actions Service (ASP.NetCore), Ex: Nginx Firewall rules that block action runner from accessing certain hosts, ex: *.github.com , *.actions.githubusercontent.com , etc Identify and solve these problems \u00b6 The key is to figure out where is the problem, the network environment, or the actions runner? Use a 3 rd party tool to make the same requests as the runner did would be a good start point. Use nslookup to check DNS Use ping to check Ping Use traceroute , tracepath , or tracert to check the network route between the runner and the Actions service Use curl -v to check the network stack, good for verifying default certificate/proxy settings. Use Invoke-WebRequest from pwsh ( PowerShell Core ) to check the dotnet network stack, good for verifying bugs in the dotnet framework. If the 3 rd party tool is also experiencing the same error as the runner does, then you might want to contact your network administrator for help. Otherwise, contact GitHub customer support or log an issue at https://github.com/actions/runner Troubleshooting: Why can't I configure a runner? \u00b6 If you are having trouble connecting, try these steps: Validate you can reach our endpoints from your web browser. If not, double check your local network connection For hosted Github: https://api.github.com/ https://vstoken.actions.githubusercontent.com/_apis/health https://pipelines.actions.githubusercontent.com/_apis/health For GHES/GHAE https://myGHES.com/_services/vstoken/_apis/health https://myGHES.com/_services/pipelines/_apis/health https://myGHES.com/api/v3 Validate you can reach those endpoints in powershell core The runner runs on .net core, lets validate the local settings for that stack Open up pwsh Run the command using the urls above Invoke-WebRequest {url} If not, get a packet trace using a tool like wireshark and start looking at the TLS handshake. If you see a Client Hello followed by a Server RST: You may need to configure your TLS settings to use the correct version You should support TLS version 1.2 or later You may need to configure your TLS settings to have up to date cipher suites, this may be solved by system updates and patches. Most notably, on windows server 2012 make sure the tls cipher suite update is installed Your firewall, proxy or network configuration may be blocking the connection You will want to reach out to whoever is in charge of your network with these pcap files to further troubleshoot If you see a failure later in the handshake: Try the fix in the SSLCert Fix","title":"Network"},{"location":"gh_actions/checks/network/#common-network-related-issues","text":"","title":"Common Network Related Issues"},{"location":"gh_actions/checks/network/#common-things-that-can-cause-the-runner-to-not-working-properly","text":"A bug in the runner or the dotnet framework that causes the actions runner to be unable to make Http requests in a certain network environment. A Proxy or Firewall may block certain HTTP method, such as blocking all POST and PUT calls which the runner will use to upload logs. A Proxy or Firewall may only allows requests with certain user-agent to pass through and the actions runner user-agent is not in the allow list. A Proxy try to decrypt and exam HTTPS traffic for security purpose but cause the actions-runner to fail to finish SSL handshake due to the lack of trusting proxy's CA. The SSL handshake may fail if the client and server do not support the same TLS version, or the same cipher suites. A Proxy may try to modify the HTTPS request (like add or change some http headers) and causes the request become incompatible with the Actions Service (ASP.NetCore), Ex: Nginx Firewall rules that block action runner from accessing certain hosts, ex: *.github.com , *.actions.githubusercontent.com , etc","title":"Common things that can cause the runner to not working properly"},{"location":"gh_actions/checks/network/#identify-and-solve-these-problems","text":"The key is to figure out where is the problem, the network environment, or the actions runner? Use a 3 rd party tool to make the same requests as the runner did would be a good start point. Use nslookup to check DNS Use ping to check Ping Use traceroute , tracepath , or tracert to check the network route between the runner and the Actions service Use curl -v to check the network stack, good for verifying default certificate/proxy settings. Use Invoke-WebRequest from pwsh ( PowerShell Core ) to check the dotnet network stack, good for verifying bugs in the dotnet framework. If the 3 rd party tool is also experiencing the same error as the runner does, then you might want to contact your network administrator for help. Otherwise, contact GitHub customer support or log an issue at https://github.com/actions/runner","title":"Identify and solve these problems"},{"location":"gh_actions/checks/network/#troubleshooting-why-cant-i-configure-a-runner","text":"If you are having trouble connecting, try these steps: Validate you can reach our endpoints from your web browser. If not, double check your local network connection For hosted Github: https://api.github.com/ https://vstoken.actions.githubusercontent.com/_apis/health https://pipelines.actions.githubusercontent.com/_apis/health For GHES/GHAE https://myGHES.com/_services/vstoken/_apis/health https://myGHES.com/_services/pipelines/_apis/health https://myGHES.com/api/v3 Validate you can reach those endpoints in powershell core The runner runs on .net core, lets validate the local settings for that stack Open up pwsh Run the command using the urls above Invoke-WebRequest {url} If not, get a packet trace using a tool like wireshark and start looking at the TLS handshake. If you see a Client Hello followed by a Server RST: You may need to configure your TLS settings to use the correct version You should support TLS version 1.2 or later You may need to configure your TLS settings to have up to date cipher suites, this may be solved by system updates and patches. Most notably, on windows server 2012 make sure the tls cipher suite update is installed Your firewall, proxy or network configuration may be blocking the connection You will want to reach out to whoever is in charge of your network with these pcap files to further troubleshoot If you see a failure later in the handshake: Try the fix in the SSLCert Fix","title":"Troubleshooting: Why can't I configure a runner?"},{"location":"gh_actions/checks/nodejs/","text":"Node.js Connection Check \u00b6 What is this check for? \u00b6 Make sure the built-in node.js has access to GitHub.com or GitHub Enterprise Server. The runner carries its own copy of node.js executable under <runner_root>/externals/node16/ . All javascript base Actions will get executed by the built-in node at <runner_root>/externals/node16/ . Not the node from $PATH What is checked? \u00b6 Make HTTPS GET to https://api.github.com or https://myGHES.com/api/v3 using node.js, make sure it gets 200 response code. How to fix the issue? \u00b6 1. Check the common network issue \u00b6 Please check the network doc 2. SSL certificate related issue \u00b6 If you are seeing Https request failed due to SSL cert issue in the log, it means the node.js can't connect to the GitHub server due to SSL handshake failure. Please check the SSL cert doc Still not working? \u00b6 Contact GitHub customer service or log an issue at https://github.com/actions/runner if you think it's a runner issue.","title":"Nodejs"},{"location":"gh_actions/checks/nodejs/#nodejs-connection-check","text":"","title":"Node.js Connection Check"},{"location":"gh_actions/checks/nodejs/#what-is-this-check-for","text":"Make sure the built-in node.js has access to GitHub.com or GitHub Enterprise Server. The runner carries its own copy of node.js executable under <runner_root>/externals/node16/ . All javascript base Actions will get executed by the built-in node at <runner_root>/externals/node16/ . Not the node from $PATH","title":"What is this check for?"},{"location":"gh_actions/checks/nodejs/#what-is-checked","text":"Make HTTPS GET to https://api.github.com or https://myGHES.com/api/v3 using node.js, make sure it gets 200 response code.","title":"What is checked?"},{"location":"gh_actions/checks/nodejs/#how-to-fix-the-issue","text":"","title":"How to fix the issue?"},{"location":"gh_actions/checks/nodejs/#1-check-the-common-network-issue","text":"Please check the network doc","title":"1. Check the common network issue"},{"location":"gh_actions/checks/nodejs/#2-ssl-certificate-related-issue","text":"If you are seeing Https request failed due to SSL cert issue in the log, it means the node.js can't connect to the GitHub server due to SSL handshake failure. Please check the SSL cert doc","title":"2. SSL certificate related issue"},{"location":"gh_actions/checks/nodejs/#still-not-working","text":"Contact GitHub customer service or log an issue at https://github.com/actions/runner if you think it's a runner issue.","title":"Still not working?"},{"location":"gh_actions/checks/ssl_certs/","text":"SSL Certificate Related Issues \u00b6 You might run into an SSL certificate error when your GitHub Enterprise Server is using a self-signed SSL server certificate or a web proxy within your network is decrypting HTTPS traffic for a security audit. As long as your certificate is generated properly, most of the issues should be fixed after your trust the certificate properly on the runner machine. github repo actions/runner Different OS might have extra requirements on SSL certificate, Ex: macOS requires ExtendedKeyUsage https://support.apple.com/en-us/HT210176 Don't skip SSL cert validation \u00b6 !!! DO NOT SKIP SSL CERT VALIDATION !!! !!! IT IS A BAD SECURITY PRACTICE !!! Download SSL certificate chain \u00b6 Depends on how your SSL server certificate gets configured, you might need to download the whole certificate chain from a machine that has trusted the SSL certificate's CA. Approach 1: Download certificate chain using a browser (Chrome, Firefox, IT), you can google for more example, here is what I found Approach 2: Download certificate chain using OpenSSL, you can google for more example, here is what I found Approach 3: Ask your network administrator or the owner of the CA certificate to send you a copy of it Trust CA certificate for the Runner \u00b6 The actions runner is a dotnet core application which will follow how dotnet load SSL CA certificates on each OS. You can get full details documentation at here In short: - Windows: Load from Windows certificate store. - Linux: Load from OpenSSL CA cert bundle. - macOS: Load from macOS KeyChain. To let the runner trusts your CA certificate, you will need to: 1. Save your SSL certificate chain which includes the root CA and all intermediate CAs into a .pem file. 2. Use OpenSSL to convert .pem file to a proper format for different OS, here is some doc with sample commands 3. Trust CA on different OS: - Windows: https://docs.microsoft.com/en-us/skype-sdk/sdn/articles/installing-the-trusted-root-certificate - macOS: - Linux: Refer to the distribution documentation 1. RedHat: https://www.redhat.com/sysadmin/ca-certificates-cli 2. Ubuntu: http://manpages.ubuntu.com/manpages/focal/man8/update-ca-certificates.8.html 3. Google search: \"trust ca certificate on [linux distribution]\" 4. If all approaches failed, set environment variable SSL_CERT_FILE to the CA bundle .pem file we get. > To verify cert gets installed properly on Linux, you can try use curl -v https://sitewithsslissue.com and pwsh -Command \\\"Invoke-WebRequest -Uri https://sitewithsslissue.com\\\" Trust CA certificate for Git CLI \u00b6 Git uses various CA bundle file depends on your operation system. - Git packaged the CA bundle file within the Git installation on Windows - Git use OpenSSL certificate CA bundle file on Linux and macOS You can check where Git check CA file by running: export GIT_CURL_VERBOSE = 1 git ls-remote https://github.com/actions/runner HEAD You should see something like: * Couldn't find host github.com in the .netrc file; using defaults * Trying 140.82.114.4... * TCP_NODELAY set * Connected to github.com (140.82.114.4) port 443 (#0) * ALPN, offering h2 * ALPN, offering http/1.1 * successfully set certificate verify locations: * CAfile: /etc/ssl/cert.pem CApath: none * SSL connection using TLSv1.2 / ECDHE-RSA-AES128-GCM-SHA256 This tells me /etc/ssl/cert.pem is where it read trusted CA certificates. To let Git trusts your CA certificate, you will need to: 1. Save your SSL certificate chain which includes the root CA and all intermediate CAs into a .pem file. 2. Set http.sslCAInfo Git config or GIT_SSL_CAINFO environment variable to the full path of the .pem file Git Doc I would recommend using http.sslCAInfo since it can be scope to certain hosts that need the extra trusted CA. Ex: git config --global http.https://myghes.com/.sslCAInfo /extra/ca/cert.pem This will make Git use the /extra/ca/cert.pem only when communicates with https://myghes.com and keep using the default CA bundle with others. Trust CA certificate for Node.js \u00b6 Node.js has compiled a snapshot of the Mozilla CA store that is fixed at each version of Node.js' release time. To let Node.js trusts your CA certificate, you will need to: 1. Save your SSL certificate chain which includes the root CA and all intermediate CAs into a .pem file. 2. Set environment variable NODE_EXTRA_CA_CERTS which point to the file. ex: export NODE_EXTRA_CA_CERTS=/full/path/to/cacert.pem or set NODE_EXTRA_CA_CERTS=C:\\full\\path\\to\\cacert.pem","title":"SSL Certs"},{"location":"gh_actions/checks/ssl_certs/#ssl-certificate-related-issues","text":"You might run into an SSL certificate error when your GitHub Enterprise Server is using a self-signed SSL server certificate or a web proxy within your network is decrypting HTTPS traffic for a security audit. As long as your certificate is generated properly, most of the issues should be fixed after your trust the certificate properly on the runner machine. github repo actions/runner Different OS might have extra requirements on SSL certificate, Ex: macOS requires ExtendedKeyUsage https://support.apple.com/en-us/HT210176","title":"SSL Certificate Related Issues"},{"location":"gh_actions/checks/ssl_certs/#dont-skip-ssl-cert-validation","text":"!!! DO NOT SKIP SSL CERT VALIDATION !!! !!! IT IS A BAD SECURITY PRACTICE !!!","title":"Don't skip SSL cert validation"},{"location":"gh_actions/checks/ssl_certs/#download-ssl-certificate-chain","text":"Depends on how your SSL server certificate gets configured, you might need to download the whole certificate chain from a machine that has trusted the SSL certificate's CA. Approach 1: Download certificate chain using a browser (Chrome, Firefox, IT), you can google for more example, here is what I found Approach 2: Download certificate chain using OpenSSL, you can google for more example, here is what I found Approach 3: Ask your network administrator or the owner of the CA certificate to send you a copy of it","title":"Download SSL certificate chain"},{"location":"gh_actions/checks/ssl_certs/#trust-ca-certificate-for-the-runner","text":"The actions runner is a dotnet core application which will follow how dotnet load SSL CA certificates on each OS. You can get full details documentation at here In short: - Windows: Load from Windows certificate store. - Linux: Load from OpenSSL CA cert bundle. - macOS: Load from macOS KeyChain. To let the runner trusts your CA certificate, you will need to: 1. Save your SSL certificate chain which includes the root CA and all intermediate CAs into a .pem file. 2. Use OpenSSL to convert .pem file to a proper format for different OS, here is some doc with sample commands 3. Trust CA on different OS: - Windows: https://docs.microsoft.com/en-us/skype-sdk/sdn/articles/installing-the-trusted-root-certificate - macOS: - Linux: Refer to the distribution documentation 1. RedHat: https://www.redhat.com/sysadmin/ca-certificates-cli 2. Ubuntu: http://manpages.ubuntu.com/manpages/focal/man8/update-ca-certificates.8.html 3. Google search: \"trust ca certificate on [linux distribution]\" 4. If all approaches failed, set environment variable SSL_CERT_FILE to the CA bundle .pem file we get. > To verify cert gets installed properly on Linux, you can try use curl -v https://sitewithsslissue.com and pwsh -Command \\\"Invoke-WebRequest -Uri https://sitewithsslissue.com\\\"","title":"Trust CA certificate for the Runner"},{"location":"gh_actions/checks/ssl_certs/#trust-ca-certificate-for-git-cli","text":"Git uses various CA bundle file depends on your operation system. - Git packaged the CA bundle file within the Git installation on Windows - Git use OpenSSL certificate CA bundle file on Linux and macOS You can check where Git check CA file by running: export GIT_CURL_VERBOSE = 1 git ls-remote https://github.com/actions/runner HEAD You should see something like: * Couldn't find host github.com in the .netrc file; using defaults * Trying 140.82.114.4... * TCP_NODELAY set * Connected to github.com (140.82.114.4) port 443 (#0) * ALPN, offering h2 * ALPN, offering http/1.1 * successfully set certificate verify locations: * CAfile: /etc/ssl/cert.pem CApath: none * SSL connection using TLSv1.2 / ECDHE-RSA-AES128-GCM-SHA256 This tells me /etc/ssl/cert.pem is where it read trusted CA certificates. To let Git trusts your CA certificate, you will need to: 1. Save your SSL certificate chain which includes the root CA and all intermediate CAs into a .pem file. 2. Set http.sslCAInfo Git config or GIT_SSL_CAINFO environment variable to the full path of the .pem file Git Doc I would recommend using http.sslCAInfo since it can be scope to certain hosts that need the extra trusted CA. Ex: git config --global http.https://myghes.com/.sslCAInfo /extra/ca/cert.pem This will make Git use the /extra/ca/cert.pem only when communicates with https://myghes.com and keep using the default CA bundle with others.","title":"Trust CA certificate for Git CLI"},{"location":"gh_actions/checks/ssl_certs/#trust-ca-certificate-for-nodejs","text":"Node.js has compiled a snapshot of the Mozilla CA store that is fixed at each version of Node.js' release time. To let Node.js trusts your CA certificate, you will need to: 1. Save your SSL certificate chain which includes the root CA and all intermediate CAs into a .pem file. 2. Set environment variable NODE_EXTRA_CA_CERTS which point to the file. ex: export NODE_EXTRA_CA_CERTS=/full/path/to/cacert.pem or set NODE_EXTRA_CA_CERTS=C:\\full\\path\\to\\cacert.pem","title":"Trust CA certificate for Node.js"},{"location":"git/check_out_multiple_repos/","text":"Check out all the projects in all the orgs my team uses. We plan to zip it all up and put it in s3 for a few years in case something comes up and we need some old archived repo. #!/usr/bin/env bash # run \"gh auth login\" first to login to ghe orgs =( ORG1, ORG2 ) # todo maybe checkout cloud-foundry org too (to get cf-deploy) checkoutProject () { ghe_org = $1 ghe_project = $2 # just in case we're running this multiple times rm -rf ${ ghe_project } git clone \"git@github.com: ${ ghe_org } / ${ ghe_project } .git\" cd ${ ghe_project } for branch in ` git branch -a | grep remotes | grep -v HEAD | grep -v master ` ; do git branch --track ${ branch #remotes/origin/ } $branch done cd .. } for org in ${ orgs [@] } ; do gh repo list $org --json owner,name --template '{{range .}}{{.owner.login}}{{\" \"}}{{.name}}{{\"\\n\"}}{{end}}' | while read line ; do checkoutProject $line ; done done","title":"Check out multiple repos"},{"location":"git/git-bayer-github-proxy/","text":"Access to GitHub.com Bayer Organization \u00b6 The following links will take you to the documentation on how to get started with GitHub.com & Bayer. Setting up GitHub.com for use with Bayer: https://docs.cloud.bayer.com/devops/github/login/ GitHub.com Conventions: https://docs.cloud.bayer.com/devops/github/conventions/ GitHub Actions: https://docs.cloud.bayer.com/devops/github/actions/# https://docs.github.com/en/actions/learn-github-actions/understanding-github-actions https://docs.cloud.bayer.com/devops/github/actions/cloud/#steps-in-aws-account Setting Proxy Variables to work with GitHub.com \u00b6 The following settings will allow you to set your proxy variables for working with GitHub.com Bayer Cloud Documentation: https://docs.cloud.bayer.com/devops/gitlab/#proxy-settings-for-git-client-to-access-from-mycloudpro-pc Step One: Open Git Bash Set the following proxy variables git config --global http.proxy 'http://your1CWID@10.185.190.100:8080' git config --global https.proxy 'https://your1CWID@10.185.190.100:8080' Set Bayer Proxy for GitHub in Rstudio You will want to open Rstudio and then go to Tools -> Terminal -> New Terminal \u00b6 Then you will want to set the proxies the same way you did above by copying the code and entering it in the terminal now open in Rstudio. You can now type the following command and a list of our settings in the global git config file can be seen. git config -l Once this has been completed you should be able to connect to GitHub.com (GHC) or GitHub Engineering (GHE). When connecting you should see a pop up window similar to the one show below. You will want to utilize a Personal Access Token (PAT) to work with either account. \u00b6","title":"Git bayer github proxy"},{"location":"git/git-bayer-github-proxy/#access-to-githubcom-bayer-organization","text":"The following links will take you to the documentation on how to get started with GitHub.com & Bayer. Setting up GitHub.com for use with Bayer: https://docs.cloud.bayer.com/devops/github/login/ GitHub.com Conventions: https://docs.cloud.bayer.com/devops/github/conventions/ GitHub Actions: https://docs.cloud.bayer.com/devops/github/actions/# https://docs.github.com/en/actions/learn-github-actions/understanding-github-actions https://docs.cloud.bayer.com/devops/github/actions/cloud/#steps-in-aws-account","title":"Access to GitHub.com Bayer Organization"},{"location":"git/git-bayer-github-proxy/#setting-proxy-variables-to-work-with-githubcom","text":"The following settings will allow you to set your proxy variables for working with GitHub.com Bayer Cloud Documentation: https://docs.cloud.bayer.com/devops/gitlab/#proxy-settings-for-git-client-to-access-from-mycloudpro-pc Step One: Open Git Bash Set the following proxy variables git config --global http.proxy 'http://your1CWID@10.185.190.100:8080' git config --global https.proxy 'https://your1CWID@10.185.190.100:8080' Set Bayer Proxy for GitHub in Rstudio You will want to open Rstudio and then go to Tools -> Terminal -> New Terminal","title":"Setting Proxy Variables to work with GitHub.com"},{"location":"git/git-bayer-github-proxy/#_1","text":"Then you will want to set the proxies the same way you did above by copying the code and entering it in the terminal now open in Rstudio. You can now type the following command and a list of our settings in the global git config file can be seen. git config -l Once this has been completed you should be able to connect to GitHub.com (GHC) or GitHub Engineering (GHE). When connecting you should see a pop up window similar to the one show below. You will want to utilize a Personal Access Token (PAT) to work with either account.","title":""},{"location":"git/git-bayer-github-proxy/#_2","text":"","title":""},{"location":"git/git_branch_v1/","text":"Git Branches \u00b6 Create a new branch and merge into main. git checkout -b <branch> The -b is a convenience flag , to run git branch before running git checkout <branch> git pull git checkout -b mike ## create new branch ## Make some changes to mike branch git add . git commit git push --set-upstream origin mike Change to remote branch \u00b6 Create a local branch np , which tracks the remote np and switch to it git checkout -b np origin/np Remotes \u00b6 Show github repo remotes (URL) git remote -v origin https://github.com/aws-samples/amazon-sagemaker-studio-vpc-networkfirewall.git ( fetch ) origin https://github.com/aws-samples/amazon-sagemaker-studio-vpc-networkfirewall.git ( push ) Change remotes \u00b6 To push repo to a new remote, clone repo and change remote path. git push git@github.com:prfrl/amazon-sagemaker-studio-vpc-networkfirewall.git Delete branch \u00b6 Delete local branch git branch -d <branch> Delete branch in remote git push origin -d <branch> Delete local branches not in remote \u00b6 Delete branches that are in local repo but have been removed from remote. First, check that branches of interest are not in this list, e.g. main/master and important feature branches. $ git fetch -p && git branch -vv | awk '/: gone]/{print $1}' - [ deleted ] ( none ) -> origin/dependabot/github_actions/actions/setup-python-3 - [ deleted ] ( none ) -> origin/dependabot/github_actions/actions/setup-python-4 - [ deleted ] ( none ) -> origin/dependabot/pip/mkdocs-1.3.1 - [ deleted ] ( none ) -> origin/dependabot/pip/mkdocs-1.4.0 - [ deleted ] ( none ) -> origin/dependabot/pip/mkdocs-1.4.1 - [ deleted ] ( none ) -> origin/dependabot/pip/mkdocs-git-revision-date-localized-plugin-1.1.0 - [ deleted ] ( none ) -> origin/dependabot/pip/mkdocs-material-8.2.15 - [ deleted ] ( none ) -> origin/dependabot/pip/mkdocs-material-8.2.5 - [ deleted ] ( none ) -> origin/dependabot/pip/mkdocs-material-8.3.5 If the list looks as expected, delete local branches. git fetch -p && git branch -vv | awk '/: gone]/{print $1}' | xargs git branch -d Works by pruning your tracking branches then deleting the local ones that show they are \"gone\" in git branch -vv To finish, verify all important local branches are intact. git branch -a $ git branch -a * main remotes/origin/HEAD -> origin/main remotes/origin/dependabot/github_actions/actions/checkout-3.1.0 remotes/origin/dependabot/github_actions/actions/setup-python-4.1.0 remotes/origin/dependabot/pip/mkdocs-material-8.5.7 remotes/origin/main Compare branches \u00b6 git diff <my branch_v1> <my branch_v2> Update branch from remote \u00b6 git stash ( optional, to save local changes which differs from the remote repository if any ) git checkout my_local_branch Checkout branch to work on git pull Pull from remote Additionally you can checkout a new local branch and reset it to the remote branches last commit. git checkout -b \uff1cbranchname\uff1e git reset --hard origin/\uff1cbranchname\uff1e Update branch from main \u00b6 When git status shows \"Your branch is ahead of 'origin/main' by 2 commits\", this measn If your local changes are bad then just remove them locally and reset to the state of remote using git reset git reset \u00b6 git checkout <branch> Optional, You may already be in the branch git pull -s recursive -X theirs Take remote branch changes and replace with their changes if conflict arise. Here if you do git status you will get something like this your branch is ahead of 'origin/master' by 3 commits. git reset --hard origin/<branch> git fetch ## Cheatsheet Push branch to remote `git push origin <branch>` Delete branch `git branch -d <branch>` # replace local changes In case you did something wrong, which for sure never happens ;), you can replace local changes using the command `git checkout -- <filename>` this replaces the changes in your working tree with the last content in HEAD. Changes already added to the index, as well as new files, will be kept. If you instead want to drop all your local changes and commits, fetch the latest history from the server and point your local master branch at it like this ```shell git fetch origin git reset --hard origin/master","title":"Branch V1"},{"location":"git/git_branch_v1/#git-branches","text":"Create a new branch and merge into main. git checkout -b <branch> The -b is a convenience flag , to run git branch before running git checkout <branch> git pull git checkout -b mike ## create new branch ## Make some changes to mike branch git add . git commit git push --set-upstream origin mike","title":"Git Branches"},{"location":"git/git_branch_v1/#change-to-remote-branch","text":"Create a local branch np , which tracks the remote np and switch to it git checkout -b np origin/np","title":"Change to remote branch"},{"location":"git/git_branch_v1/#remotes","text":"Show github repo remotes (URL) git remote -v origin https://github.com/aws-samples/amazon-sagemaker-studio-vpc-networkfirewall.git ( fetch ) origin https://github.com/aws-samples/amazon-sagemaker-studio-vpc-networkfirewall.git ( push )","title":"Remotes"},{"location":"git/git_branch_v1/#change-remotes","text":"To push repo to a new remote, clone repo and change remote path. git push git@github.com:prfrl/amazon-sagemaker-studio-vpc-networkfirewall.git","title":"Change remotes"},{"location":"git/git_branch_v1/#delete-branch","text":"Delete local branch git branch -d <branch> Delete branch in remote git push origin -d <branch>","title":"Delete branch"},{"location":"git/git_branch_v1/#delete-local-branches-not-in-remote","text":"Delete branches that are in local repo but have been removed from remote. First, check that branches of interest are not in this list, e.g. main/master and important feature branches. $ git fetch -p && git branch -vv | awk '/: gone]/{print $1}' - [ deleted ] ( none ) -> origin/dependabot/github_actions/actions/setup-python-3 - [ deleted ] ( none ) -> origin/dependabot/github_actions/actions/setup-python-4 - [ deleted ] ( none ) -> origin/dependabot/pip/mkdocs-1.3.1 - [ deleted ] ( none ) -> origin/dependabot/pip/mkdocs-1.4.0 - [ deleted ] ( none ) -> origin/dependabot/pip/mkdocs-1.4.1 - [ deleted ] ( none ) -> origin/dependabot/pip/mkdocs-git-revision-date-localized-plugin-1.1.0 - [ deleted ] ( none ) -> origin/dependabot/pip/mkdocs-material-8.2.15 - [ deleted ] ( none ) -> origin/dependabot/pip/mkdocs-material-8.2.5 - [ deleted ] ( none ) -> origin/dependabot/pip/mkdocs-material-8.3.5 If the list looks as expected, delete local branches. git fetch -p && git branch -vv | awk '/: gone]/{print $1}' | xargs git branch -d Works by pruning your tracking branches then deleting the local ones that show they are \"gone\" in git branch -vv To finish, verify all important local branches are intact. git branch -a $ git branch -a * main remotes/origin/HEAD -> origin/main remotes/origin/dependabot/github_actions/actions/checkout-3.1.0 remotes/origin/dependabot/github_actions/actions/setup-python-4.1.0 remotes/origin/dependabot/pip/mkdocs-material-8.5.7 remotes/origin/main","title":"Delete local branches not in remote"},{"location":"git/git_branch_v1/#compare-branches","text":"git diff <my branch_v1> <my branch_v2>","title":"Compare branches"},{"location":"git/git_branch_v1/#update-branch-from-remote","text":"git stash ( optional, to save local changes which differs from the remote repository if any ) git checkout my_local_branch Checkout branch to work on git pull Pull from remote Additionally you can checkout a new local branch and reset it to the remote branches last commit. git checkout -b \uff1cbranchname\uff1e git reset --hard origin/\uff1cbranchname\uff1e","title":"Update branch from remote"},{"location":"git/git_branch_v1/#update-branch-from-main","text":"When git status shows \"Your branch is ahead of 'origin/main' by 2 commits\", this measn If your local changes are bad then just remove them locally and reset to the state of remote using git reset","title":"Update branch from main"},{"location":"git/git_branch_v1/#git-reset","text":"git checkout <branch> Optional, You may already be in the branch git pull -s recursive -X theirs Take remote branch changes and replace with their changes if conflict arise. Here if you do git status you will get something like this your branch is ahead of 'origin/master' by 3 commits. git reset --hard origin/<branch> git fetch ## Cheatsheet Push branch to remote `git push origin <branch>` Delete branch `git branch -d <branch>` # replace local changes In case you did something wrong, which for sure never happens ;), you can replace local changes using the command `git checkout -- <filename>` this replaces the changes in your working tree with the last content in HEAD. Changes already added to the index, as well as new files, will be kept. If you instead want to drop all your local changes and commits, fetch the latest history from the server and point your local master branch at it like this ```shell git fetch origin git reset --hard origin/master","title":"git reset"},{"location":"git/git_branching/","text":"Branching Workflow for Continuous Delivery \u00b6 This simple workflow has two guiding principles: master is always production-like and deployable. rebase during feature development, explicit (non fast-forward) merge when done. The n commits from this branch will be rebased and added to base branch Pulling change-sets using rebase rewrites the history of the branch you\u2019re working on and keeps your changes on top. Merge vs Rebase \u00b6 Merge branch to main Create a feature branch and make some file edits. gitGraph commit id: \"1\" commit id: \"2\" branch feature commit id: \"A\" commit id: \"B\" checkout main commit id: \"3\" commit id: \"4\" Next, merge the main branch into the feature branch. gitGraph commit id: \"1\" commit id: \"2\" branch feature commit id: \"A\" commit id: \"B\" checkout main commit id: \"3\" commit id: \"4\" merge feature tag: \"Merge Commit\" This creates a new \u201cmerge commit\u201d in the feature branch that ties together the histories of both branches, giving you a branch structure that looks like this: What is a rebase? Preserving the order of change-sets. Create a feature branch and make some file edits. gitGraph commit id: \"1\" commit id: \"2\" branch feature commit id: \"A\" commit id: \"B\" checkout main commit id: \"3\" commit id: \"4\" Rebase moves feature branch to the tip of the main branch. gitGraph commit id: \"1\" commit id: \"2\" branch feature commit id: \"A\" commit id: \"B\" checkout main commit id: \"3\" commit id: \"4\" merge feature Commit history maintains a linear record. gitGraph commit id: \"1\" commit id: \"2\" commit id: \"3\" commit id: \"4\" commit id: \"A\" type: HIGHLIGHT commit id: \"B\" type: HIGHLIGHT Start by pulling down the latest changes from master This is done easily with the common git commands: git checkout master git fetch origin git merge master Branch off to isolate the feature or bug-fix work in a branch Now create a branch for the feature or bug-fix: git checkout -b PRJ-123-awesome-feature The branch name structure I show here is just the one we use, but you can pick any convention you feel comfortable with. Now you can work on the feature Work on the feature as long as needed. Make sure your commits are meaningful and do not cluster separate changes together. To keep your feature branch fresh and up to date with the latest changes in master, use rebase Every once in a while during the development update the feature branch with the latest changes in master. You can do this with: git fetch origin git rebase origin/master In the (somewhat less common) case where other people are also working on the same shared remote feature branch, also rebase changes coming from it: git rebase origin/PRJ-123-awesome-feature At this point solve any conflicts that come out of the rebase. Resolving conflicts during the rebase allows you to have always clean merges at the end of the feature development. It also keeps your feature branch history clean and focused without spurious noise. When ready for feedback push your branch remotely and create a pull request When it\u2019s time to share your work and solicit feedback you can push your branch remotely with: git push -u origin PRJ-123-awesome-feature (if the branch is already set as 'upstream' and your remote is called 'origin', 'git push' is enough) Now you can create a pull request. After the initial push you can keep pushing updates to the remote branch multiple times throughout. This can happen in response to feedback, or because you\u2019re not done with the development of the feature. Perform a final rebase cleanup after the pull request has been approved After the review is done, it\u2019s good to perform a final cleanup and scrub of the feature branch commit history to remove spurious commits that are not providing relevant information. In some cases \u2013 if your team is experienced and they can handle it \u2013 you can rebase also during development, but I strongly advise against it.: git rebase -i origin/master (At this point if you have rewritten the history of a published branch and provided that no one else will commit to it or use it, you might need to push your changes using the \u2013force flag). When development is complete record an explicit merge When finished with the development of the feature branch and reviewers have reviewed your work, merge using the flag \u2013no-ff. This will preserve the context of the work and will make it easy to revert the whole feature if needed. Here are the commands: git checkout master git pull origin master git merge --no-ff PRJ-123-awesome-feature If you followed the advice above and you have used rebase to keep your feature branch up to date, the actual merge commit will not include any changes; this is cool! The merge commit becomes just a marker that stores the context about the feature branch. Good documentation comparing git rebase vs git merge Fix Conflicts \u00b6 Take remote branch changes and replace with their changes if conflict arise. Here if you do git status you will get something like this your branch is ahead of 'origin/master' by 3 commits. git checkout test git pull git checkout master git pull git merge --no-ff --no-commit test Test merge before commit, avoid a fast-forward commit by --no-ff, If conflict is encountered, we can run git status to check details about the conflicts and try to solve git status Once we solve the conflicts, or if there is no conflict, we commit and push them git commit -m 'merge test branch' git push But this way will lose the changes history logged in test branch, and it would make master branch to be hard for other developers to understand the history of the project. So the best method is we have to use rebase instead of merge (suppose, when in this time, we have solved the branch conflicts). Following is one simple sample, for advanced operations, please refer to http://git-scm.com/book/en/v2/Git-Branching-Rebasing git checkout master git pull git checkout test git pull git rebase -i master git checkout master git merge test Yep, when you have uppers done, all the Test branch's commits will be moved onto the head of Master branch. The major benefit of rebasing is that you get a linear and much cleaner project history. The only thing you need to avoid is: never use rebase on public branch, like master branch. Never do operations like the following: git checkout master git rebase -i test git checkout main git pull origin main Find merge base hash git merge-base origin main # 106f9f893e1ac36e55711fc870f6a30a60a9febc git merge --squash test git commit git push origin master 3 way merge \u00b6 echo $USER branch name dev_ ${ USER } # Create new branch git checkout -b dev main # Edit some files echo \"insert text here\" > scratches/a_file.txt git add <file> git commit -m \"Start a feature\" # Edit some files git add <file> git commit -m \"Finish a feature\" # compare branch to main git request-pull main ./ # Optional # push branch to remote git push -u origin <branch> # Develop the main branch git checkout main # Edit some files git add <file> git commit -m \"Make some super-stable changes to main\" # Merge in the new-feature branch git merge new-feature # To write a commit message and get out of VI, follow these steps: # press i (i for insert) # write your merge message # press esc (escape) # write :wq (write & quit) # then press enter # optional # uncomment to delete local branch # git branch -d dev Note that it\u2019s impossible for Git to perform a fast-forward merge, as there is no way to move main up to new-feature without backtracking.","title":"Branch"},{"location":"git/git_branching/#branching-workflow-for-continuous-delivery","text":"This simple workflow has two guiding principles: master is always production-like and deployable. rebase during feature development, explicit (non fast-forward) merge when done. The n commits from this branch will be rebased and added to base branch Pulling change-sets using rebase rewrites the history of the branch you\u2019re working on and keeps your changes on top.","title":"Branching Workflow for Continuous Delivery"},{"location":"git/git_branching/#merge-vs-rebase","text":"Merge branch to main Create a feature branch and make some file edits. gitGraph commit id: \"1\" commit id: \"2\" branch feature commit id: \"A\" commit id: \"B\" checkout main commit id: \"3\" commit id: \"4\" Next, merge the main branch into the feature branch. gitGraph commit id: \"1\" commit id: \"2\" branch feature commit id: \"A\" commit id: \"B\" checkout main commit id: \"3\" commit id: \"4\" merge feature tag: \"Merge Commit\" This creates a new \u201cmerge commit\u201d in the feature branch that ties together the histories of both branches, giving you a branch structure that looks like this: What is a rebase? Preserving the order of change-sets. Create a feature branch and make some file edits. gitGraph commit id: \"1\" commit id: \"2\" branch feature commit id: \"A\" commit id: \"B\" checkout main commit id: \"3\" commit id: \"4\" Rebase moves feature branch to the tip of the main branch. gitGraph commit id: \"1\" commit id: \"2\" branch feature commit id: \"A\" commit id: \"B\" checkout main commit id: \"3\" commit id: \"4\" merge feature Commit history maintains a linear record. gitGraph commit id: \"1\" commit id: \"2\" commit id: \"3\" commit id: \"4\" commit id: \"A\" type: HIGHLIGHT commit id: \"B\" type: HIGHLIGHT Start by pulling down the latest changes from master This is done easily with the common git commands: git checkout master git fetch origin git merge master Branch off to isolate the feature or bug-fix work in a branch Now create a branch for the feature or bug-fix: git checkout -b PRJ-123-awesome-feature The branch name structure I show here is just the one we use, but you can pick any convention you feel comfortable with. Now you can work on the feature Work on the feature as long as needed. Make sure your commits are meaningful and do not cluster separate changes together. To keep your feature branch fresh and up to date with the latest changes in master, use rebase Every once in a while during the development update the feature branch with the latest changes in master. You can do this with: git fetch origin git rebase origin/master In the (somewhat less common) case where other people are also working on the same shared remote feature branch, also rebase changes coming from it: git rebase origin/PRJ-123-awesome-feature At this point solve any conflicts that come out of the rebase. Resolving conflicts during the rebase allows you to have always clean merges at the end of the feature development. It also keeps your feature branch history clean and focused without spurious noise. When ready for feedback push your branch remotely and create a pull request When it\u2019s time to share your work and solicit feedback you can push your branch remotely with: git push -u origin PRJ-123-awesome-feature (if the branch is already set as 'upstream' and your remote is called 'origin', 'git push' is enough) Now you can create a pull request. After the initial push you can keep pushing updates to the remote branch multiple times throughout. This can happen in response to feedback, or because you\u2019re not done with the development of the feature. Perform a final rebase cleanup after the pull request has been approved After the review is done, it\u2019s good to perform a final cleanup and scrub of the feature branch commit history to remove spurious commits that are not providing relevant information. In some cases \u2013 if your team is experienced and they can handle it \u2013 you can rebase also during development, but I strongly advise against it.: git rebase -i origin/master (At this point if you have rewritten the history of a published branch and provided that no one else will commit to it or use it, you might need to push your changes using the \u2013force flag). When development is complete record an explicit merge When finished with the development of the feature branch and reviewers have reviewed your work, merge using the flag \u2013no-ff. This will preserve the context of the work and will make it easy to revert the whole feature if needed. Here are the commands: git checkout master git pull origin master git merge --no-ff PRJ-123-awesome-feature If you followed the advice above and you have used rebase to keep your feature branch up to date, the actual merge commit will not include any changes; this is cool! The merge commit becomes just a marker that stores the context about the feature branch. Good documentation comparing git rebase vs git merge","title":"Merge vs Rebase"},{"location":"git/git_branching/#fix-conflicts","text":"Take remote branch changes and replace with their changes if conflict arise. Here if you do git status you will get something like this your branch is ahead of 'origin/master' by 3 commits. git checkout test git pull git checkout master git pull git merge --no-ff --no-commit test Test merge before commit, avoid a fast-forward commit by --no-ff, If conflict is encountered, we can run git status to check details about the conflicts and try to solve git status Once we solve the conflicts, or if there is no conflict, we commit and push them git commit -m 'merge test branch' git push But this way will lose the changes history logged in test branch, and it would make master branch to be hard for other developers to understand the history of the project. So the best method is we have to use rebase instead of merge (suppose, when in this time, we have solved the branch conflicts). Following is one simple sample, for advanced operations, please refer to http://git-scm.com/book/en/v2/Git-Branching-Rebasing git checkout master git pull git checkout test git pull git rebase -i master git checkout master git merge test Yep, when you have uppers done, all the Test branch's commits will be moved onto the head of Master branch. The major benefit of rebasing is that you get a linear and much cleaner project history. The only thing you need to avoid is: never use rebase on public branch, like master branch. Never do operations like the following: git checkout master git rebase -i test git checkout main git pull origin main Find merge base hash git merge-base origin main # 106f9f893e1ac36e55711fc870f6a30a60a9febc git merge --squash test git commit git push origin master","title":"Fix Conflicts"},{"location":"git/git_branching/#3-way-merge","text":"echo $USER branch name dev_ ${ USER } # Create new branch git checkout -b dev main # Edit some files echo \"insert text here\" > scratches/a_file.txt git add <file> git commit -m \"Start a feature\" # Edit some files git add <file> git commit -m \"Finish a feature\" # compare branch to main git request-pull main ./ # Optional # push branch to remote git push -u origin <branch> # Develop the main branch git checkout main # Edit some files git add <file> git commit -m \"Make some super-stable changes to main\" # Merge in the new-feature branch git merge new-feature # To write a commit message and get out of VI, follow these steps: # press i (i for insert) # write your merge message # press esc (escape) # write :wq (write & quit) # then press enter # optional # uncomment to delete local branch # git branch -d dev Note that it\u2019s impossible for Git to perform a fast-forward merge, as there is no way to move main up to new-feature without backtracking.","title":"3 way merge"},{"location":"git/git_checkout/","text":"Git Checkout \u00b6 git-checkout 1. Switch branches or 2. restore working tree files If you modify a file but haven't staged the change, then git checkout will reverse the modifications... a quick and easy way to cancel changes to a file. You remain in the same branch. git checkout (as you noted) switches branches. git checkout Selects the current branch git checkout <filename> reverses modifications to unstaged file git branch -d Deletes the obsolete branch git restore takes care of operations that change file","title":"Git Checkout"},{"location":"git/git_checkout/#git-checkout","text":"git-checkout 1. Switch branches or 2. restore working tree files If you modify a file but haven't staged the change, then git checkout will reverse the modifications... a quick and easy way to cancel changes to a file. You remain in the same branch. git checkout (as you noted) switches branches. git checkout Selects the current branch git checkout <filename> reverses modifications to unstaged file git branch -d Deletes the obsolete branch git restore takes care of operations that change file","title":"Git Checkout"},{"location":"git/git_command_list/","text":"Command comparison \u00b6 previous command new command git checkout git switch git checkout N/A (use git status) git checkout -b [ ] git switch -c [ ] git checkout -B [ ] git switch -C [ ] git checkout --orphan git switch --orphan git checkout --orphan N/A (use git switch then git switch --orphan ) git checkout [--detach] git switch --detach git checkout --detach [ ] git switch --detach [ ] git checkout [--] \u2026 git restore [--] \u2026 git checkout --pathspec-from-file= git restore --pathspec-from-file= git checkout [--] \u2026 git restore -s [--] \u2026 git checkout --pathspec-from-file= git restore -s --pathspec-from-file= git checkout -p [ ] [--] [ \u2026] git restore -p [-s ] [--] [ \u2026]","title":"Command comparison"},{"location":"git/git_command_list/#command-comparison","text":"previous command new command git checkout git switch git checkout N/A (use git status) git checkout -b [ ] git switch -c [ ] git checkout -B [ ] git switch -C [ ] git checkout --orphan git switch --orphan git checkout --orphan N/A (use git switch then git switch --orphan ) git checkout [--detach] git switch --detach git checkout --detach [ ] git switch --detach [ ] git checkout [--] \u2026 git restore [--] \u2026 git checkout --pathspec-from-file= git restore --pathspec-from-file= git checkout [--] \u2026 git restore -s [--] \u2026 git checkout --pathspec-from-file= git restore -s --pathspec-from-file= git checkout -p [ ] [--] [ \u2026] git restore -p [-s ] [--] [ \u2026]","title":"Command comparison"},{"location":"git/git_diff/","text":"Show changes before git Pull \u00b6 Preview changes before merging git diff <source_branch> <target_branch> git pull == git fetch && git merge The git fetch updates your so-called \"remote-tracking branches\" - typically these are ones that look like origin/master , github/experiment , etc. that you see with git branch -r . These are like a cache of the state of branches in the remote repository that are updated when you do git fetch (or a successful git push ). So, suppose you've got a remote called origin that refers to your GitHub repository, you would do: git diff List remote \u00b6 git ls-remote origin -h refs/heads/master will list the current head on the remote -- you can compare it to a previous value or see if you have the SHA in your local repo. git status \u00b6 git status -uno will tell you whether the branch you are tracking is ahead, behind or has diverged. If it says nothing, the local and remote are the same. git reset \u00b6 git pull --rebase origin/main This resets my (local) copy of master (which I assume is screwed up) to the correct point, as represented by (remote) origin/master. git remote \u00b6 git remote update git remote -v get fetch origin ... and then do: git diff master origin/master git log -p HEAD.. origin/main to show each patch git diff HEAD\u2026 origin/main to show a single diff Discard Changes \u00b6 git reset --hard discard changes in staging and working area Sync Remote with Staging \u00b6 git rev-parse --abbrev-ref HEAD Check if on master. I want to check so that I have the master branch checked out. Since thats the branch i want to deploy from. This would return master if on master branch. I could do a git branch but that would return a list of branches with the currently selected marked with a *. So this saves me from string manipulations. Putting together a shell script that checks if you have master branch checked out could look like this: Check for changes on remote (origin) Git repo. Scenario I cloned from a repository and did some commits of my own to my local repository. In the meantime, my colleagues made commits to the remote repository. Now, I want to: Check whether there are any new commits from other people on the remote repository, i.e. origin? Say there were 3 new commits on the remote repository since my last pull, I would like to diff the remote repository's commits, i.e. HEAD~3 with HEAD~2, HEAD~2 with HEAD~1 and HEAD~1 with HEAD. After knowing what changed remotely, I want to get the latest commits from the others. My findings so far For step 2: I know the caret notation HEAD^, HEAD^^, etc. and the tilde notation HEAD~2, HEAD~3, etc. For step 3: That is, I guess, just a git pull. git fetch origin This will update your remote branch to the latest version. For a difference against remote you could use: git diff origin/master And after checking that if you want to accept that changes you could use: git merge origin/master git diff \u00b6 git diff diff --git a/.github/workflows/publish_mkdocs.yml b/.github/workflows/publish_mkdocs.yml ## A old version of file ## B new version of file index 4a377fd..79b6c71 100644 ## Meta data about two files compared @@ -3,18 +3,19 @@ name: \"Publish MKDocs\" ## -3,18 means from A version file, extracting 18 lines starting from line 3","title":"Diff"},{"location":"git/git_diff/#show-changes-before-git-pull","text":"Preview changes before merging git diff <source_branch> <target_branch> git pull == git fetch && git merge The git fetch updates your so-called \"remote-tracking branches\" - typically these are ones that look like origin/master , github/experiment , etc. that you see with git branch -r . These are like a cache of the state of branches in the remote repository that are updated when you do git fetch (or a successful git push ). So, suppose you've got a remote called origin that refers to your GitHub repository, you would do: git diff","title":"Show changes before git Pull"},{"location":"git/git_diff/#list-remote","text":"git ls-remote origin -h refs/heads/master will list the current head on the remote -- you can compare it to a previous value or see if you have the SHA in your local repo.","title":"List remote"},{"location":"git/git_diff/#git-status","text":"git status -uno will tell you whether the branch you are tracking is ahead, behind or has diverged. If it says nothing, the local and remote are the same.","title":"git status"},{"location":"git/git_diff/#git-reset","text":"git pull --rebase origin/main This resets my (local) copy of master (which I assume is screwed up) to the correct point, as represented by (remote) origin/master.","title":"git reset"},{"location":"git/git_diff/#git-remote","text":"git remote update git remote -v get fetch origin ... and then do: git diff master origin/master git log -p HEAD.. origin/main to show each patch git diff HEAD\u2026 origin/main to show a single diff","title":"git remote"},{"location":"git/git_diff/#discard-changes","text":"git reset --hard discard changes in staging and working area","title":"Discard Changes"},{"location":"git/git_diff/#sync-remote-with-staging","text":"git rev-parse --abbrev-ref HEAD Check if on master. I want to check so that I have the master branch checked out. Since thats the branch i want to deploy from. This would return master if on master branch. I could do a git branch but that would return a list of branches with the currently selected marked with a *. So this saves me from string manipulations. Putting together a shell script that checks if you have master branch checked out could look like this: Check for changes on remote (origin) Git repo. Scenario I cloned from a repository and did some commits of my own to my local repository. In the meantime, my colleagues made commits to the remote repository. Now, I want to: Check whether there are any new commits from other people on the remote repository, i.e. origin? Say there were 3 new commits on the remote repository since my last pull, I would like to diff the remote repository's commits, i.e. HEAD~3 with HEAD~2, HEAD~2 with HEAD~1 and HEAD~1 with HEAD. After knowing what changed remotely, I want to get the latest commits from the others. My findings so far For step 2: I know the caret notation HEAD^, HEAD^^, etc. and the tilde notation HEAD~2, HEAD~3, etc. For step 3: That is, I guess, just a git pull. git fetch origin This will update your remote branch to the latest version. For a difference against remote you could use: git diff origin/master And after checking that if you want to accept that changes you could use: git merge origin/master","title":"Sync Remote with Staging"},{"location":"git/git_diff/#git-diff","text":"git diff diff --git a/.github/workflows/publish_mkdocs.yml b/.github/workflows/publish_mkdocs.yml ## A old version of file ## B new version of file index 4a377fd..79b6c71 100644 ## Meta data about two files compared @@ -3,18 +3,19 @@ name: \"Publish MKDocs\" ## -3,18 means from A version file, extracting 18 lines starting from line 3","title":"git diff"},{"location":"git/git_log/","text":"Git logs \u00b6 in its simplest form, you can study repository history using.. git log This shows the commit message and SHA-1 checksum from each commit. git log commit cd1f2ec305a2c996ab6f3d17199ddc295c0dda52 ( HEAD -> main, origin/main, origin/HEAD ) Author: memadsen <michael.madsen@bayer.com> Date: Wed Oct 26 12 :06:43 2022 -0400 cicd content You can add a lot of parameters to make the log look like what you want. To see only the commits of a certain author: git log --author=bob To see a very compressed log where each commit is one line: git log --pretty=oneline Or maybe you want to see an ASCII art tree of all the branches, decorated with the names of tags and branches: git log --graph --oneline --decorate --all See only which files have changed: git log --name-status These are just a few of the possible parameters you can use. For more, see git log --help","title":"Log"},{"location":"git/git_log/#git-logs","text":"in its simplest form, you can study repository history using.. git log This shows the commit message and SHA-1 checksum from each commit. git log commit cd1f2ec305a2c996ab6f3d17199ddc295c0dda52 ( HEAD -> main, origin/main, origin/HEAD ) Author: memadsen <michael.madsen@bayer.com> Date: Wed Oct 26 12 :06:43 2022 -0400 cicd content You can add a lot of parameters to make the log look like what you want. To see only the commits of a certain author: git log --author=bob To see a very compressed log where each commit is one line: git log --pretty=oneline Or maybe you want to see an ASCII art tree of all the branches, decorated with the names of tags and branches: git log --graph --oneline --decorate --all See only which files have changed: git log --name-status These are just a few of the possible parameters you can use. For more, see git log --help","title":"Git logs"},{"location":"git/git_merge/","text":"Git Merge \u00b6 merge fast forward \u00b6 The code below creates a new branch, adds two commits to it, then integrates it into the main line with a fast-forward merge. This is a common workflow for short-lived topic branches that are used more as an isolated development than an organizational tool for longer-running features. # Start a new feature git checkout -b new-feature main # or switch to existing feature git switch new-feature # Edit some files git add <file> git commit -m \"Start a feature\" # Edit some files git add <file> git commit -m \"Finish a feature\" # Merge in the new-feature branch git checkout main # Use only one merge # git merge new-feature git merge --no-ff <branch> ## record keeping git branch -d new-feature Update working branch from another branch \u00b6 I've been working on feature1 for a month now and a lot of changes have been pushed to develop . How can I update my current branch feature1 with the latest commits from develop ? git checkout develop git pull git checkout feature/myfeature 3 way merge \u00b6 The next example is very similar, but requires a 3-way merge because main progresses while the feature is in-progress. This is a common scenario for large features or when several developers are working on a project simultaneously. # Start a new feature git checkout -b new-feature main # Edit some files git add <file> git commit -m \"Start a feature\" # Edit some files git add <file> git commit -m \"Finish a feature\" # Develop the main branch git checkout main # Edit some files git add <file> git commit -m \"Make some super-stable changes to main\" # Merge in the new-feature branch git merge new-feature # To write a commit message and get out of VI, follow these steps: # press i (i for insert) # write your merge message # press esc (escape) # write :wq (write & quit) # then press enter # optional # uncomment to delete local branch # git branch -d dev Note that it\u2019s impossible for Git to perform a fast-forward merge, as there is no way to move main up to new-feature without backtracking. For most workflows, new-feature would be a much larger feature that took a long time to develop, which would be why new commits would appear on main in the meantime. If your feature branch was actually as small as the one in the above example, you would probably be better off rebasing it onto main and doing a fast-forward merge. This prevents superfluous merge commits from cluttering up the project history. Resolving conflict \u00b6 If the two branches you're trying to merge both changed the same part of the same file, Git won't be able to figure out which version to use. When such a situation occurs, it stops right before the merge commit so that you can resolve the conflicts manually. The great part of Git's merging process is that it uses the familiar edit/stage/commit workflow to resolve merge conflicts. When you encounter a merge conflict, running the git status command shows you which files need to be resolved. For example, if both branches modified the same section of hello.py, you would see something like the following: On branch main Unmerged paths: ( use \"git add/rm ...\" as appropriate to mark resolution ) both modified: hello.py How conflicts are presented When Git encounters a conflict during a merge, It will edit the content of the affected files with visual indicators that mark both sides of the conflicted content. These visual markers are: <<<<<<<, =======, and >>>>>>>. Its helpful to search a project for these indicators during a merge to find where conflicts need to be resolved. here is some content not affected by the conflict <<<<<< < main this is conflicted text from main ======= this is conflicted text from feature branch >>>>>>> feature branch ; Generally the content before the ======= marker is the receiving branch and the part after is the merging branch. Once you've identified conflicting sections, you can go in and fix up the merge to your liking. When you're ready to finish the merge, all you have to do is run git add on the conflicted file(s) to tell Git they're resolved. Then, you run a normal git commit to generate the merge commit. It\u2019s the exact same process as committing an ordinary snapshot, which means it\u2019s easy for normal developers to manage their own merges. Note that merge conflicts will only occur in the event of a 3-way merge. It\u2019s not possible to have conflicting changes in a fast-forward merge. Example 3 way Commit \u00b6 git init echo one>1.txt git add . git commit -m 'c1' echo two>2.txt git add . git commit -m 'c2' echo three>3.txt git add . git commit -m 'C3' git branch feature git switch feature echo four>4.txt git add . git commit -m 'c4' echo five>5.txt git add . git commit -m 'c5' git switch master echo six>6.txt git add . git commit -m 'c6' git merge feature git log --oneline --all --graph","title":"Merge"},{"location":"git/git_merge/#git-merge","text":"","title":"Git Merge"},{"location":"git/git_merge/#merge-fast-forward","text":"The code below creates a new branch, adds two commits to it, then integrates it into the main line with a fast-forward merge. This is a common workflow for short-lived topic branches that are used more as an isolated development than an organizational tool for longer-running features. # Start a new feature git checkout -b new-feature main # or switch to existing feature git switch new-feature # Edit some files git add <file> git commit -m \"Start a feature\" # Edit some files git add <file> git commit -m \"Finish a feature\" # Merge in the new-feature branch git checkout main # Use only one merge # git merge new-feature git merge --no-ff <branch> ## record keeping git branch -d new-feature","title":"merge fast forward"},{"location":"git/git_merge/#update-working-branch-from-another-branch","text":"I've been working on feature1 for a month now and a lot of changes have been pushed to develop . How can I update my current branch feature1 with the latest commits from develop ? git checkout develop git pull git checkout feature/myfeature","title":"Update working branch from another branch"},{"location":"git/git_merge/#3-way-merge","text":"The next example is very similar, but requires a 3-way merge because main progresses while the feature is in-progress. This is a common scenario for large features or when several developers are working on a project simultaneously. # Start a new feature git checkout -b new-feature main # Edit some files git add <file> git commit -m \"Start a feature\" # Edit some files git add <file> git commit -m \"Finish a feature\" # Develop the main branch git checkout main # Edit some files git add <file> git commit -m \"Make some super-stable changes to main\" # Merge in the new-feature branch git merge new-feature # To write a commit message and get out of VI, follow these steps: # press i (i for insert) # write your merge message # press esc (escape) # write :wq (write & quit) # then press enter # optional # uncomment to delete local branch # git branch -d dev Note that it\u2019s impossible for Git to perform a fast-forward merge, as there is no way to move main up to new-feature without backtracking. For most workflows, new-feature would be a much larger feature that took a long time to develop, which would be why new commits would appear on main in the meantime. If your feature branch was actually as small as the one in the above example, you would probably be better off rebasing it onto main and doing a fast-forward merge. This prevents superfluous merge commits from cluttering up the project history.","title":"3 way merge"},{"location":"git/git_merge/#resolving-conflict","text":"If the two branches you're trying to merge both changed the same part of the same file, Git won't be able to figure out which version to use. When such a situation occurs, it stops right before the merge commit so that you can resolve the conflicts manually. The great part of Git's merging process is that it uses the familiar edit/stage/commit workflow to resolve merge conflicts. When you encounter a merge conflict, running the git status command shows you which files need to be resolved. For example, if both branches modified the same section of hello.py, you would see something like the following: On branch main Unmerged paths: ( use \"git add/rm ...\" as appropriate to mark resolution ) both modified: hello.py How conflicts are presented When Git encounters a conflict during a merge, It will edit the content of the affected files with visual indicators that mark both sides of the conflicted content. These visual markers are: <<<<<<<, =======, and >>>>>>>. Its helpful to search a project for these indicators during a merge to find where conflicts need to be resolved. here is some content not affected by the conflict <<<<<< < main this is conflicted text from main ======= this is conflicted text from feature branch >>>>>>> feature branch ; Generally the content before the ======= marker is the receiving branch and the part after is the merging branch. Once you've identified conflicting sections, you can go in and fix up the merge to your liking. When you're ready to finish the merge, all you have to do is run git add on the conflicted file(s) to tell Git they're resolved. Then, you run a normal git commit to generate the merge commit. It\u2019s the exact same process as committing an ordinary snapshot, which means it\u2019s easy for normal developers to manage their own merges. Note that merge conflicts will only occur in the event of a 3-way merge. It\u2019s not possible to have conflicting changes in a fast-forward merge.","title":"Resolving conflict"},{"location":"git/git_merge/#example-3-way-commit","text":"git init echo one>1.txt git add . git commit -m 'c1' echo two>2.txt git add . git commit -m 'c2' echo three>3.txt git add . git commit -m 'C3' git branch feature git switch feature echo four>4.txt git add . git commit -m 'c4' echo five>5.txt git add . git commit -m 'c5' git switch master echo six>6.txt git add . git commit -m 'c6' git merge feature git log --oneline --all --graph","title":"Example 3 way Commit"},{"location":"git/git_pull_stash/","text":"Git stash \u00b6 To overwrite your local files do: git fetch --all git reset --hard <remote>/<branch_name> git fetch --all git reset --hard origin/master Git Pull and save local changes \u00b6 Maintain current local commits It's worth noting that it is possible to maintain current local commits by creating a branch from master before resetting: git checkout master git branch new-branch-to-save-current-commits git fetch --all git reset --hard origin/master After this, all of the old commits will be kept in new-branch-to-save-current-commits. Uncommitted changes Uncommitted changes, however (even staged), will be lost. Make sure to stash and commit anything you need. For that you can run the following: git stash And then to reapply these uncommitted changes: git stash pop","title":"Pull and Stash"},{"location":"git/git_pull_stash/#git-stash","text":"To overwrite your local files do: git fetch --all git reset --hard <remote>/<branch_name> git fetch --all git reset --hard origin/master","title":"Git stash"},{"location":"git/git_pull_stash/#git-pull-and-save-local-changes","text":"Maintain current local commits It's worth noting that it is possible to maintain current local commits by creating a branch from master before resetting: git checkout master git branch new-branch-to-save-current-commits git fetch --all git reset --hard origin/master After this, all of the old commits will be kept in new-branch-to-save-current-commits. Uncommitted changes Uncommitted changes, however (even staged), will be lost. Make sure to stash and commit anything you need. For that you can run the following: git stash And then to reapply these uncommitted changes: git stash pop","title":"Git Pull and save local changes"},{"location":"git/git_rebase/","text":"git rebase \u00b6 I want to base my changes on what everyone has already done. The primary reason for rebasing is to maintain a linear project history. For example, consider a situation where the main branch has progressed since you started working on a feature branch. You want to get the latest updates to the main branch in your feature branch, but you want to keep your branch's history clean so it appears as if you've been working off the latest main branch. This gives the later benefit of a clean merge of your feature branch back into the main branch. Why do we want to maintain a \"clean history\"? The benefits of having a clean history become tangible when performing Git operations to investigate the introduction of a regression. A more real-world scenario would be: A bug is identified in the main branch. A feature that was working successfully is now broken. A developer examines the history of the main branch using git log because of the \"clean history\" the developer is quickly able to reason about the history of the project. The developer can not identify when the bug was introduced using git log so the developer executes a git bisect. Because the git history is clean, git bisect has a refined set of commits to compare when looking for the regression. The developer quickly finds the commit that introduced the bug and is able to act accordingly. Keep feature branch up to date with master and merge to master when done \u00b6 Create a feature branch based off of main and switch to it. git checkout -b feature_branch main git pull --rebase Edit files Commit git commit -a -m \"Adds new feature\" git checkout NewFeatureBranch git merge --no-ff master ## --no-ff keep git history clear git rebase <base>","title":"Rebase"},{"location":"git/git_rebase/#git-rebase","text":"I want to base my changes on what everyone has already done. The primary reason for rebasing is to maintain a linear project history. For example, consider a situation where the main branch has progressed since you started working on a feature branch. You want to get the latest updates to the main branch in your feature branch, but you want to keep your branch's history clean so it appears as if you've been working off the latest main branch. This gives the later benefit of a clean merge of your feature branch back into the main branch. Why do we want to maintain a \"clean history\"? The benefits of having a clean history become tangible when performing Git operations to investigate the introduction of a regression. A more real-world scenario would be: A bug is identified in the main branch. A feature that was working successfully is now broken. A developer examines the history of the main branch using git log because of the \"clean history\" the developer is quickly able to reason about the history of the project. The developer can not identify when the bug was introduced using git log so the developer executes a git bisect. Because the git history is clean, git bisect has a refined set of commits to compare when looking for the regression. The developer quickly finds the commit that introduced the bug and is able to act accordingly.","title":"git rebase"},{"location":"git/git_rebase/#keep-feature-branch-up-to-date-with-master-and-merge-to-master-when-done","text":"Create a feature branch based off of main and switch to it. git checkout -b feature_branch main git pull --rebase Edit files Commit git commit -a -m \"Adds new feature\" git checkout NewFeatureBranch git merge --no-ff master ## --no-ff keep git history clear git rebase <base>","title":"Keep feature branch up to date with master and merge to master when done"},{"location":"git/git_workflow/","text":"Git Basic Workflow \u00b6 Using typical git workflow. This doc shows git commands I use every day. How does git work internally? \u00b6 Git works like a file system with version history Snapshots are identified by SHA-1 hash sums Versions can also have a name (Tag) add , commit , push and pull are the most frequent commands used. git HEAD \u00b6 HEAD is a special ref that points to the commit you are currently working on - the currently checked out commit in your git working directory. HEAD usually points to the tip/head of the currently active branch, which is represented in the .git/HEAD file as follows: $ cat .git/HEAD ref: refs/heads/main I can switch branches and git HEAD will be updated. $ git switch dev $ cat .git/HEAD ref: refs/heads/dev Set up a local repository \u00b6 In local environment (laptop or cloud directory) cd YOUR_PROJECT_DIRECTORY git init git add --all git commit -m \"initial commit\" If you want to exclude files in you project directory use a .gitignore file file names will be used recursively (also in all subdirectories), start with / to select only files in the project root directory Wildcards can be used to track all files by a pattern. For example to get all python and R files use *.py and *.R select complete directories YOUR_EXCLUDE_FOLDER/ Clone from a remote repository \u00b6 git clone CLONEURL Create new repository \u00b6 ##### Option 1 echo \"# Can-I-Shop-2\" >> README.md git init ## Required for new directory git add . git commit -m \"first commit\" git remote add origin https://github.com/username/projectname.git git push -u origin master ##### Option 2 git remote add origin https://github.com/bayer-int/elxsj_test.git git branch -M master git push -u origin master Reverse git add git reset <filename> Add all files in current path git add . Add all files in project git add -A Amend commit without changing message git commit --amend --no-edit Pushing an amended commit. After pushing to remote, amend commit using th e -f option git push -f origin <my_branch> Putting some changes aside: stash \u00b6 You can't pull changes if you have uncommitted work in your project How can you get important changes if you are \"in the middle of something\"? git stash git pull git stash apply You can have multiple stashes which you can list with git stash list Remove all stash list git stash clear Using .gitignore \u00b6 Reset .gitignore part 1 \u00b6 Abbreviated steps to reset .gitignore to track new changes. First, make changes in .gitignore file. Reset cache git rm -r --cached . Add and push changes to github git add . git commit -m \"Commit message\" Reset .gitignore part 2 \u00b6 After creating a .gitignore file in your repository and setting patterns to match files which you do not want Git to track, Git starts tracking repository files and respecting the patterns set in the .gitignore file after you run the git add command (For example git add . ). The problem is that if we later make some changes to the .gitignore file and then just run the git add command again, the changes made in the .gitignore file will not take effect. Reset .gitignore part 3 \u00b6 For example if you later set in the .gitignore file that you want Git to start tracking a file which you previously set to be ignored, the file will still be untracked if you just run the git add . command. This is because, the git cache needs to be cleared. I usually just do this with the git rm -r --cached . then after I run the git add . command to apply the .gitignore changes. Fetch \u00b6 Fetch all of the remote branches and tags from the existing repository to our local index: git fetch origin branches local \u00b6 But even if all branches and tags have been fetched and exist in a local index, we still won\u2019t have a copy of them that is physically local. And a local copy is required to migrate the repository. We can check for any missing branches that we need to create a local copy of. Let\u2019s list all existing branches (local and remote) to see whether we are missing any copies locally: git branch -a We can easily tell from this output whether we have local copies of all remote branches. The remote ones are prefixed with the remotes/origin/ path, and the local ones are not. So, only our master branch is local, whereas remotes/origin/develop and remotes/origin/release/0.1 are not. That\u2019s OK \u2014 let\u2019s just create local copies: git checkout -b develop origin/develop git checkout -b release/0.1 origin/release/0.1 After creating local copies of everything, we can verify once again whether all branches with the remotes/origin/ prefix have corresponding local copies (shown without the prefix): git branch -a develop master * release/0.1 remotes/origin/develop remotes/origin/master remotes/origin/release/0.1 Now we know for sure that all branches in our repository are stored locally, and we are ready to move the repository to a new host. Switch to another branch \u00b6 git switch <branch name> Create new branch \u00b6 This is a two step process. git checkout -b <branch-name> Create new branch locally git push -u origin <branch-name> Push new branch local to remote After git push to remote, the repo on GitHub.com will show the new branch. Verify the new branch and proceed to add new code using git add > git commit and git push . Merge dev branch into master \u00b6 ## on branch \"dev\" git commit -m \" my message \" git push git checkout master # now on branch master git branch -a git pull ## ensure local master is current with remote git merge dev git push git checkout dev # switch to dev branch RM branches \u00b6 delete local branch git branch -d <branch> delete remote branch git push <remote_name> -d <remote_branch_name> ### Usually looks like this git push origin -d dev Change remote origin \u00b6 Migrate a Git repository OR to merge existing ones. This is useful if the remote needs to be mapped to local files. Change the Git origin remote to a new URL. ## Update the URL of origin remote using SSH git remote set-url origin git@github.com:username/repo.git ## My example git remote set-url origin git@github.com:bayer-int/smol-cls-mlops-scaffold.git # Test URL remote git remote show origin Pull Request \u00b6 The following code creates a new branch, makes an arbitrary change, and pushes it to new_branch: Open the git repo ( from github.com ) and click on the Fork button in the top-right corner. This creates a new copy of the repo we want to update, under your GitHub user account with a URL like: https://github.com/<UserName>/reponame clone the repo git clone <repo_2> Create a new branch git checkout -b new_branch Create a new remote for the upstream repo (github server) git remote add upstream <repo_1> Then git add, git commit and git push as usual. The entire CLI workflow follows this pattern: git clone https://github.platforms.engineering/ELXSJ/MWAA.git git checkout -b dev git remote add upstream https://github.platforms.engineering/science-at-scale/MWAA.git git add . git commit -m \"add ingress to ALB\" git branch -a git push -u origin dev Once you push the changes to your repo, the Compare & pull request button will appear in GitHub. Update local repo to latest remote tag \u00b6 Clone repo. This step was assumed to be done previously but placed here to make the example consistent. git clone https://github.com/mlrun/demos.git Use latest branch and tag from github repo Update current project ## To check out the latest Git tag, first, update ## the repository by fetching the remote tags available. git fetch --tags ## Then, retrieve the latest tag available by using the \u201cgit describe\u201d command. latestTag = $( git describe --tags ` git rev-list --tags --max-count = 1 ` ) echo $latestTag ## Finally, use the \u201cgit checkout\u201d command to checkout the latest git tag git checkout $latestTag -b latest ## Execute the \u201cgit log\u201d command to make sure that we are actually ## developing starting from the new tag. git log --oneline --graph * 8c98f7c ( HEAD -> latest, tag: v1.1.x-rc5, origin/1.1.x ) Merge pull request #295 from mlrun/1.1.x-dev | \\ | * ba1c1cb ( origin/1.1.x-dev ) Merge pull request #294 from aviaIguazio/1.1.x-dev | | \\ | | * e885976 add timeout for pipeline deploy !!! Find file I have edited. git log --pretty --author=$( git config user.email ) --name-only | sort | uniq github Stats \u00b6 Gather insights into the usage of different programming languages and their dependencies within our organization. Useful Links UI URL: https://devtools-np.monsanto.net/github-stats UI Repo: https://github.platforms.engineering/APALA4/github-stats-ui API Docs: https://devtools-np.monsanto.net/github-stats-api/v1/docs API Repo: https://github.platforms.engineering/APALA4/github-stats-api Vulnerabilities API: https://ossindex.sonatype.org/rest Github Hooks \u00b6 Many people are familair with github actions and its common to use github actions to perform code formatting or testing. Another way to do this is using git hooks . Authors \u00b6 fuad.abdallah@bayer.com michael.madsen@bayer.com References \u00b6 Some advanced topics not covered but important to know. Submodules github api You can find more information in the Pro Git book Git official docs Git visual cheat sheet Pro Git by Scott Chacon and Ben Straub is available to read online for free . A hard copy can be purchased online.","title":"Workflow"},{"location":"git/git_workflow/#git-basic-workflow","text":"Using typical git workflow. This doc shows git commands I use every day.","title":"Git Basic Workflow"},{"location":"git/git_workflow/#how-does-git-work-internally","text":"Git works like a file system with version history Snapshots are identified by SHA-1 hash sums Versions can also have a name (Tag) add , commit , push and pull are the most frequent commands used.","title":"How does git work internally?"},{"location":"git/git_workflow/#git-head","text":"HEAD is a special ref that points to the commit you are currently working on - the currently checked out commit in your git working directory. HEAD usually points to the tip/head of the currently active branch, which is represented in the .git/HEAD file as follows: $ cat .git/HEAD ref: refs/heads/main I can switch branches and git HEAD will be updated. $ git switch dev $ cat .git/HEAD ref: refs/heads/dev","title":"git HEAD"},{"location":"git/git_workflow/#set-up-a-local-repository","text":"In local environment (laptop or cloud directory) cd YOUR_PROJECT_DIRECTORY git init git add --all git commit -m \"initial commit\" If you want to exclude files in you project directory use a .gitignore file file names will be used recursively (also in all subdirectories), start with / to select only files in the project root directory Wildcards can be used to track all files by a pattern. For example to get all python and R files use *.py and *.R select complete directories YOUR_EXCLUDE_FOLDER/","title":"Set up a local repository"},{"location":"git/git_workflow/#clone-from-a-remote-repository","text":"git clone CLONEURL","title":"Clone from a remote repository"},{"location":"git/git_workflow/#create-new-repository","text":"##### Option 1 echo \"# Can-I-Shop-2\" >> README.md git init ## Required for new directory git add . git commit -m \"first commit\" git remote add origin https://github.com/username/projectname.git git push -u origin master ##### Option 2 git remote add origin https://github.com/bayer-int/elxsj_test.git git branch -M master git push -u origin master Reverse git add git reset <filename> Add all files in current path git add . Add all files in project git add -A Amend commit without changing message git commit --amend --no-edit Pushing an amended commit. After pushing to remote, amend commit using th e -f option git push -f origin <my_branch>","title":"Create new repository"},{"location":"git/git_workflow/#putting-some-changes-aside-stash","text":"You can't pull changes if you have uncommitted work in your project How can you get important changes if you are \"in the middle of something\"? git stash git pull git stash apply You can have multiple stashes which you can list with git stash list Remove all stash list git stash clear","title":"Putting some changes aside: stash"},{"location":"git/git_workflow/#using-gitignore","text":"","title":"Using .gitignore"},{"location":"git/git_workflow/#reset-gitignore-part-1","text":"Abbreviated steps to reset .gitignore to track new changes. First, make changes in .gitignore file. Reset cache git rm -r --cached . Add and push changes to github git add . git commit -m \"Commit message\"","title":"Reset .gitignore part 1"},{"location":"git/git_workflow/#reset-gitignore-part-2","text":"After creating a .gitignore file in your repository and setting patterns to match files which you do not want Git to track, Git starts tracking repository files and respecting the patterns set in the .gitignore file after you run the git add command (For example git add . ). The problem is that if we later make some changes to the .gitignore file and then just run the git add command again, the changes made in the .gitignore file will not take effect.","title":"Reset .gitignore part 2"},{"location":"git/git_workflow/#reset-gitignore-part-3","text":"For example if you later set in the .gitignore file that you want Git to start tracking a file which you previously set to be ignored, the file will still be untracked if you just run the git add . command. This is because, the git cache needs to be cleared. I usually just do this with the git rm -r --cached . then after I run the git add . command to apply the .gitignore changes.","title":"Reset .gitignore part 3"},{"location":"git/git_workflow/#fetch","text":"Fetch all of the remote branches and tags from the existing repository to our local index: git fetch origin","title":"Fetch"},{"location":"git/git_workflow/#branches-local","text":"But even if all branches and tags have been fetched and exist in a local index, we still won\u2019t have a copy of them that is physically local. And a local copy is required to migrate the repository. We can check for any missing branches that we need to create a local copy of. Let\u2019s list all existing branches (local and remote) to see whether we are missing any copies locally: git branch -a We can easily tell from this output whether we have local copies of all remote branches. The remote ones are prefixed with the remotes/origin/ path, and the local ones are not. So, only our master branch is local, whereas remotes/origin/develop and remotes/origin/release/0.1 are not. That\u2019s OK \u2014 let\u2019s just create local copies: git checkout -b develop origin/develop git checkout -b release/0.1 origin/release/0.1 After creating local copies of everything, we can verify once again whether all branches with the remotes/origin/ prefix have corresponding local copies (shown without the prefix): git branch -a develop master * release/0.1 remotes/origin/develop remotes/origin/master remotes/origin/release/0.1 Now we know for sure that all branches in our repository are stored locally, and we are ready to move the repository to a new host.","title":"branches local"},{"location":"git/git_workflow/#switch-to-another-branch","text":"git switch <branch name>","title":"Switch to another branch"},{"location":"git/git_workflow/#create-new-branch","text":"This is a two step process. git checkout -b <branch-name> Create new branch locally git push -u origin <branch-name> Push new branch local to remote After git push to remote, the repo on GitHub.com will show the new branch. Verify the new branch and proceed to add new code using git add > git commit and git push .","title":"Create new branch"},{"location":"git/git_workflow/#merge-dev-branch-into-master","text":"## on branch \"dev\" git commit -m \" my message \" git push git checkout master # now on branch master git branch -a git pull ## ensure local master is current with remote git merge dev git push git checkout dev # switch to dev branch","title":"Merge dev branch into master"},{"location":"git/git_workflow/#rm-branches","text":"delete local branch git branch -d <branch> delete remote branch git push <remote_name> -d <remote_branch_name> ### Usually looks like this git push origin -d dev","title":"RM branches"},{"location":"git/git_workflow/#change-remote-origin","text":"Migrate a Git repository OR to merge existing ones. This is useful if the remote needs to be mapped to local files. Change the Git origin remote to a new URL. ## Update the URL of origin remote using SSH git remote set-url origin git@github.com:username/repo.git ## My example git remote set-url origin git@github.com:bayer-int/smol-cls-mlops-scaffold.git # Test URL remote git remote show origin","title":"Change remote origin"},{"location":"git/git_workflow/#pull-request","text":"The following code creates a new branch, makes an arbitrary change, and pushes it to new_branch: Open the git repo ( from github.com ) and click on the Fork button in the top-right corner. This creates a new copy of the repo we want to update, under your GitHub user account with a URL like: https://github.com/<UserName>/reponame clone the repo git clone <repo_2> Create a new branch git checkout -b new_branch Create a new remote for the upstream repo (github server) git remote add upstream <repo_1> Then git add, git commit and git push as usual. The entire CLI workflow follows this pattern: git clone https://github.platforms.engineering/ELXSJ/MWAA.git git checkout -b dev git remote add upstream https://github.platforms.engineering/science-at-scale/MWAA.git git add . git commit -m \"add ingress to ALB\" git branch -a git push -u origin dev Once you push the changes to your repo, the Compare & pull request button will appear in GitHub.","title":"Pull Request"},{"location":"git/git_workflow/#update-local-repo-to-latest-remote-tag","text":"Clone repo. This step was assumed to be done previously but placed here to make the example consistent. git clone https://github.com/mlrun/demos.git Use latest branch and tag from github repo Update current project ## To check out the latest Git tag, first, update ## the repository by fetching the remote tags available. git fetch --tags ## Then, retrieve the latest tag available by using the \u201cgit describe\u201d command. latestTag = $( git describe --tags ` git rev-list --tags --max-count = 1 ` ) echo $latestTag ## Finally, use the \u201cgit checkout\u201d command to checkout the latest git tag git checkout $latestTag -b latest ## Execute the \u201cgit log\u201d command to make sure that we are actually ## developing starting from the new tag. git log --oneline --graph * 8c98f7c ( HEAD -> latest, tag: v1.1.x-rc5, origin/1.1.x ) Merge pull request #295 from mlrun/1.1.x-dev | \\ | * ba1c1cb ( origin/1.1.x-dev ) Merge pull request #294 from aviaIguazio/1.1.x-dev | | \\ | | * e885976 add timeout for pipeline deploy !!! Find file I have edited. git log --pretty --author=$( git config user.email ) --name-only | sort | uniq","title":"Update local repo to latest remote tag"},{"location":"git/git_workflow/#github-stats","text":"Gather insights into the usage of different programming languages and their dependencies within our organization. Useful Links UI URL: https://devtools-np.monsanto.net/github-stats UI Repo: https://github.platforms.engineering/APALA4/github-stats-ui API Docs: https://devtools-np.monsanto.net/github-stats-api/v1/docs API Repo: https://github.platforms.engineering/APALA4/github-stats-api Vulnerabilities API: https://ossindex.sonatype.org/rest","title":"github Stats"},{"location":"git/git_workflow/#github-hooks","text":"Many people are familair with github actions and its common to use github actions to perform code formatting or testing. Another way to do this is using git hooks .","title":"Github Hooks"},{"location":"git/git_workflow/#authors","text":"fuad.abdallah@bayer.com michael.madsen@bayer.com","title":"Authors"},{"location":"git/git_workflow/#references","text":"Some advanced topics not covered but important to know. Submodules github api You can find more information in the Pro Git book Git official docs Git visual cheat sheet Pro Git by Scott Chacon and Ben Straub is available to read online for free . A hard copy can be purchased online.","title":"References"},{"location":"git/sync_git/","text":"Sync git from local to remote \u00b6 This script updates the local and remote repository. #!/bin/bash ############################# ## May need chmod +x file ## ## RUN script from R Console # system(\"sync.sh\") # # Run from shell # bash sync.sh #sudo apt-get install --reinstall ca-certificates # ## Optional: Reset Gitignore after making changes ## git rm -r --cached . # # git config --global user.email \"foo_bar@something.com\" # git config --global user.name \" \" ############################# ## Paramaters dt = $( date '+%d/%m/%Y %H:%M:%S' ) BRANCH = $( git branch | sed -n -e 's/^\\* \\(.*\\)/\\1/p' ) ## script # echo -e \"Hi Hi Hi $(whoami) ----------------------- \" # echo \" \" # Check if remote has changed and need to pull changed = 0 git remote update && git status -uno | grep -q 'Your branch is behind' && changed = 1 if [ $changed = 1 ] ; then git pull echo \" \" echo \"-------Updated Remote successfully-------\" ; else echo \" \" echo \"-------Remote is Up-to-date-------\" fi echo \"current git branch == $BRANCH \" read -p \"Do you wish to commit changes to repo (yes/no) \" yn case $yn in yes ) echo ok, we will proceed ;; no ) echo exiting... ; exit ;; * ) echo invalid response ; exit 1 ;; esac git add . git commit -m \" $dt \u00af\\_(\u30c4)_/\u00af\" git push #-u origin \"$BRANCH\" ## Uncomment this after merging branch to HEAD # git pull # Create new branch # BRANCH=\"dev_$USER\" # echo \"my branch name is:\" # echo \"$BRANCH\" # git checkout -b \"$BRANCH\" main ## Does Branch Exist in local # if [ `git branch --list $BRANCH` ] # then # echo \"Branch name $BRANCH already exists.\" # git switch $BRANCH # else # echo \"No $BRANCH in local\" # git checkout -b \"$BRANCH\" main # fi ############### ## push local to remote ############## #echo \"Git Commit Comments: $1\"; # echo \" \" # echo \"push local changes to remote\" # echo \"----------- -- -----------\" # echo \" \" # git add . # git commit -m \"$dt \u00af\\_(\u30c4)_/\u00af\" # # #git commit -m \"$1\" # git push","title":"Sync Repo"},{"location":"git/sync_git/#sync-git-from-local-to-remote","text":"This script updates the local and remote repository. #!/bin/bash ############################# ## May need chmod +x file ## ## RUN script from R Console # system(\"sync.sh\") # # Run from shell # bash sync.sh #sudo apt-get install --reinstall ca-certificates # ## Optional: Reset Gitignore after making changes ## git rm -r --cached . # # git config --global user.email \"foo_bar@something.com\" # git config --global user.name \" \" ############################# ## Paramaters dt = $( date '+%d/%m/%Y %H:%M:%S' ) BRANCH = $( git branch | sed -n -e 's/^\\* \\(.*\\)/\\1/p' ) ## script # echo -e \"Hi Hi Hi $(whoami) ----------------------- \" # echo \" \" # Check if remote has changed and need to pull changed = 0 git remote update && git status -uno | grep -q 'Your branch is behind' && changed = 1 if [ $changed = 1 ] ; then git pull echo \" \" echo \"-------Updated Remote successfully-------\" ; else echo \" \" echo \"-------Remote is Up-to-date-------\" fi echo \"current git branch == $BRANCH \" read -p \"Do you wish to commit changes to repo (yes/no) \" yn case $yn in yes ) echo ok, we will proceed ;; no ) echo exiting... ; exit ;; * ) echo invalid response ; exit 1 ;; esac git add . git commit -m \" $dt \u00af\\_(\u30c4)_/\u00af\" git push #-u origin \"$BRANCH\" ## Uncomment this after merging branch to HEAD # git pull # Create new branch # BRANCH=\"dev_$USER\" # echo \"my branch name is:\" # echo \"$BRANCH\" # git checkout -b \"$BRANCH\" main ## Does Branch Exist in local # if [ `git branch --list $BRANCH` ] # then # echo \"Branch name $BRANCH already exists.\" # git switch $BRANCH # else # echo \"No $BRANCH in local\" # git checkout -b \"$BRANCH\" main # fi ############### ## push local to remote ############## #echo \"Git Commit Comments: $1\"; # echo \" \" # echo \"push local changes to remote\" # echo \"----------- -- -----------\" # echo \" \" # git add . # git commit -m \"$dt \u00af\\_(\u30c4)_/\u00af\" # # #git commit -m \"$1\" # git push","title":"Sync git from local to remote"},{"location":"git/setup/git_install_config/","text":"Git \u00b6 Install and configure git on local environment. Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. GIT Refresher \u00b6 Background discussion and common github commands. No advanced topics, but tutorials should be easier after becoming familiar with common git commands. The official git documentation is excellent (most diagrams in this presentation are from the official documentation) Why use version control? \u00b6 Easier experimentation - you can always go back to a working version Identification of project versions Exchange of a consistent state of your project Why use git? \u00b6 Widespread use Integrates with many applications Fast and very flexible Distributed State of files in git \u00b6 Files are initially untracked - they will be ignored by git They need to be staged to be handled by git After committing the current state is recorded Every following change can be staged and committed The git lifecycle uses the concept of a remote repository for pushing files from a local environment. Git terminology \u00b6 Repository: Snapshots of the managed files and their version history Commit: A named snapshot of all managed files in the repository Remote: A connected git repository Push: Transmit commits to a remote Pull: Get commits from a remote Merge: Integrate changes Stash: Local cache of changes Clone: create a copy of a remote Branch: a named tracking variant of files What does this have to do with GitHub or GitLab? \u00b6 GitHub or GitLab are remotes with a web interface They offer a lot of additional functionality but in the context of git they are not different from the repository you have on disk Git can be used with different workflows but often GitHub or GitLab are used as central hubs to share a project Install and Config git \u00b6 Install git for your OS \u00b6 Windows scoop install git MacOS brew install git linux apt install git-all AWS prefers to use linux-like docker images??? debian and ubuntu commands used in practice. Setup global configuration git config --global user.name \"np-completed\" git config --global user.email mmadsen1978@gmail.com list contents of my git config \u00b6 The ~/.gitconfig file will show git details. Also, the local configuration will be in your repository's .git/config file. To view system, global and local values: git config --list ## My current git config shows these details credential.helper = osxkeychain ## Personal access token user.name = np-completed user.email = 93400240 +np-completed@users.noreply.github.com init.defaultbranch = master Change the default email shown in the config file: git config --global user.email 'mmadsen1978@gmail.com' git credentials \u00b6 SSH is the ideal method to clone and update repos from command line. To debug connectivity issues, a github Personal Access Token is a technically feasible alternative to SSH. If you want to store git credentials (this will store the password unencrypted !) Not recommended in production. Use only for testing. git config --global credential.helper store If you want to cache your credentials git config credential.helper 'cache --timeout=<timeout>' SSH keygen \u00b6 While ssh is the prefered method for users to clone and push to their repos, it is still possible to clone via https. ssh-keygen -t ed25519 -C \"mmadsen1978@gmail.com\" ## Output ## Generating public/private ed25519 key pair. ## Enter file in which to save the key (/Users/np-completed/.ssh/id_ed25519): Next, add the SSH key to your GitHub account Goto your GitHub Account -> Settings Then look for SSH and GPG keys under **Account Settings -> SSH and GPG keys ** After that click on New SSH Key. Assign some meaningful name to your key Copy the content of the key and paste the key inside your GitHub account. cat /Users/elxsj/.ssh/id_ed25519.pub Proxy configuration \u00b6 US Proxy Settings \u00b6 export http_proxy = http://10.44.255.70:8080 export https_proxy = https://10.44.255.70:8080 Configure the Git client to refer to the cacerts that have the imported certificate: git config --system http.sslCAPath /path/to/cacerts From the official git docs but untested with corporate network. ## This puts password in plain text git config --global http.proxy https://PROXY_USERNAME:PROXY_PASSWORD@PROXY_SERVER:8080 Corporate HTTP/HTTPS proxies \u00b6 Misaligned proxies interfere with using git. It is technically feasible to temporarily bypass SSL but this deviates from best practices. Do not do this in production! git config --global http.sslVerify false Clone repo \u00b6 Clone repo and bypass SSL Do not do this in production! Useful to check if certificates require updating. git -c http.sslVerify = false clone <repository-name> Git branch default name. \u00b6 If you want to use master instead of main as default branch name. git config --global init.defaultBranch master Update local branch name from main to master . git branch -m main master git fetch origin git branch -u origin/master master git remote set-head origin -a Some aspects to consider setting up a repository \u00b6 Binary files can be handled by git, but large files make some operations in git slow For pure data repositories specialized software like DVC might be a better option Removing changes from the git history is very hard - think twice if you want big files in your repository! Jupyter and RMarkdown notebooks \u00b6 Jupyter and RMarkdown Notebooks embed code with output. They can get very big (file size) They may save many image files Changes on the cell (python) or hook (rmarkdown) outputs can hide changes in the code It can be very hard to merge concurrent changes Clean the output before committing github Stats \u00b6 Gather insights into the usage of different programming languages and their dependencies within our organization. Useful Links UI URL: https://devtools-np.monsanto.net/github-stats UI Repo: https://github.platforms.engineering/APALA4/github-stats-ui API Docs: https://devtools-np.monsanto.net/github-stats-api/v1/docs API Repo: https://github.platforms.engineering/APALA4/github-stats-api Vulnerabilities API: https://ossindex.sonatype.org/rest Github Hooks \u00b6 Many people are familair with github actions and its common to use github actions to perform code formatting or testing. Another way to do this is using git hooks . References \u00b6 Git official docs Git visual cheat sheet Pro Git by Scott Chacon and Ben Straub is available to read online for free . A hard copy can be purchased online.","title":"Install git"},{"location":"git/setup/git_install_config/#git","text":"Install and configure git on local environment. Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency.","title":"Git"},{"location":"git/setup/git_install_config/#git-refresher","text":"Background discussion and common github commands. No advanced topics, but tutorials should be easier after becoming familiar with common git commands. The official git documentation is excellent (most diagrams in this presentation are from the official documentation)","title":"GIT Refresher"},{"location":"git/setup/git_install_config/#why-use-version-control","text":"Easier experimentation - you can always go back to a working version Identification of project versions Exchange of a consistent state of your project","title":"Why use version control?"},{"location":"git/setup/git_install_config/#why-use-git","text":"Widespread use Integrates with many applications Fast and very flexible Distributed","title":"Why use git?"},{"location":"git/setup/git_install_config/#state-of-files-in-git","text":"Files are initially untracked - they will be ignored by git They need to be staged to be handled by git After committing the current state is recorded Every following change can be staged and committed The git lifecycle uses the concept of a remote repository for pushing files from a local environment.","title":"State of files in git"},{"location":"git/setup/git_install_config/#git-terminology","text":"Repository: Snapshots of the managed files and their version history Commit: A named snapshot of all managed files in the repository Remote: A connected git repository Push: Transmit commits to a remote Pull: Get commits from a remote Merge: Integrate changes Stash: Local cache of changes Clone: create a copy of a remote Branch: a named tracking variant of files","title":"Git terminology"},{"location":"git/setup/git_install_config/#what-does-this-have-to-do-with-github-or-gitlab","text":"GitHub or GitLab are remotes with a web interface They offer a lot of additional functionality but in the context of git they are not different from the repository you have on disk Git can be used with different workflows but often GitHub or GitLab are used as central hubs to share a project","title":"What does this have to do with GitHub or GitLab?"},{"location":"git/setup/git_install_config/#install-and-config-git","text":"","title":"Install and Config git"},{"location":"git/setup/git_install_config/#install-git-for-your-os","text":"Windows scoop install git MacOS brew install git linux apt install git-all AWS prefers to use linux-like docker images??? debian and ubuntu commands used in practice. Setup global configuration git config --global user.name \"np-completed\" git config --global user.email mmadsen1978@gmail.com","title":"Install git for your OS"},{"location":"git/setup/git_install_config/#list-contents-of-my-git-config","text":"The ~/.gitconfig file will show git details. Also, the local configuration will be in your repository's .git/config file. To view system, global and local values: git config --list ## My current git config shows these details credential.helper = osxkeychain ## Personal access token user.name = np-completed user.email = 93400240 +np-completed@users.noreply.github.com init.defaultbranch = master Change the default email shown in the config file: git config --global user.email 'mmadsen1978@gmail.com'","title":"list contents of my git config"},{"location":"git/setup/git_install_config/#git-credentials","text":"SSH is the ideal method to clone and update repos from command line. To debug connectivity issues, a github Personal Access Token is a technically feasible alternative to SSH. If you want to store git credentials (this will store the password unencrypted !) Not recommended in production. Use only for testing. git config --global credential.helper store If you want to cache your credentials git config credential.helper 'cache --timeout=<timeout>'","title":"git credentials"},{"location":"git/setup/git_install_config/#ssh-keygen","text":"While ssh is the prefered method for users to clone and push to their repos, it is still possible to clone via https. ssh-keygen -t ed25519 -C \"mmadsen1978@gmail.com\" ## Output ## Generating public/private ed25519 key pair. ## Enter file in which to save the key (/Users/np-completed/.ssh/id_ed25519): Next, add the SSH key to your GitHub account Goto your GitHub Account -> Settings Then look for SSH and GPG keys under **Account Settings -> SSH and GPG keys ** After that click on New SSH Key. Assign some meaningful name to your key Copy the content of the key and paste the key inside your GitHub account. cat /Users/elxsj/.ssh/id_ed25519.pub","title":"SSH keygen"},{"location":"git/setup/git_install_config/#proxy-configuration","text":"","title":"Proxy configuration"},{"location":"git/setup/git_install_config/#us-proxy-settings","text":"export http_proxy = http://10.44.255.70:8080 export https_proxy = https://10.44.255.70:8080 Configure the Git client to refer to the cacerts that have the imported certificate: git config --system http.sslCAPath /path/to/cacerts From the official git docs but untested with corporate network. ## This puts password in plain text git config --global http.proxy https://PROXY_USERNAME:PROXY_PASSWORD@PROXY_SERVER:8080","title":"US Proxy Settings"},{"location":"git/setup/git_install_config/#corporate-httphttps-proxies","text":"Misaligned proxies interfere with using git. It is technically feasible to temporarily bypass SSL but this deviates from best practices. Do not do this in production! git config --global http.sslVerify false","title":"Corporate HTTP/HTTPS proxies"},{"location":"git/setup/git_install_config/#clone-repo","text":"Clone repo and bypass SSL Do not do this in production! Useful to check if certificates require updating. git -c http.sslVerify = false clone <repository-name>","title":"Clone repo"},{"location":"git/setup/git_install_config/#git-branch-default-name","text":"If you want to use master instead of main as default branch name. git config --global init.defaultBranch master Update local branch name from main to master . git branch -m main master git fetch origin git branch -u origin/master master git remote set-head origin -a","title":"Git branch default name."},{"location":"git/setup/git_install_config/#some-aspects-to-consider-setting-up-a-repository","text":"Binary files can be handled by git, but large files make some operations in git slow For pure data repositories specialized software like DVC might be a better option Removing changes from the git history is very hard - think twice if you want big files in your repository!","title":"Some aspects to consider setting up a repository"},{"location":"git/setup/git_install_config/#jupyter-and-rmarkdown-notebooks","text":"Jupyter and RMarkdown Notebooks embed code with output. They can get very big (file size) They may save many image files Changes on the cell (python) or hook (rmarkdown) outputs can hide changes in the code It can be very hard to merge concurrent changes Clean the output before committing","title":"Jupyter and RMarkdown notebooks"},{"location":"git/setup/git_install_config/#github-stats","text":"Gather insights into the usage of different programming languages and their dependencies within our organization. Useful Links UI URL: https://devtools-np.monsanto.net/github-stats UI Repo: https://github.platforms.engineering/APALA4/github-stats-ui API Docs: https://devtools-np.monsanto.net/github-stats-api/v1/docs API Repo: https://github.platforms.engineering/APALA4/github-stats-api Vulnerabilities API: https://ossindex.sonatype.org/rest","title":"github Stats"},{"location":"git/setup/git_install_config/#github-hooks","text":"Many people are familair with github actions and its common to use github actions to perform code formatting or testing. Another way to do this is using git hooks .","title":"Github Hooks"},{"location":"git/setup/git_install_config/#references","text":"Git official docs Git visual cheat sheet Pro Git by Scott Chacon and Ben Straub is available to read online for free . A hard copy can be purchased online.","title":"References"},{"location":"git/setup/ssh_config/","text":"Configure Multiple SSH Hosts on One Machine \u00b6 Most of us have multiple (mostly two) GitHub accounts, personal and work account. You need to have the ability to push and pull to multiple accounts. This post is about how to setup and use them on a single machine using HTTPS or SSH. Note: The instructions below have all been executed on macOS and should work fine on all Unix based operating systems. Using SSH \u00b6 Step 1: Generate new SSH Keys \u00b6 Before generating new SSH keys, check for existing SSH keys. To list existing ssh keys, go to the terminal, run command ls -al ~/.ssh , files with extension .pub are your SSH keys. You need to have ssh keys for each account. If you are planning to use two accounts personal and work, there should be two SSH keys, if not, you have to generate them. If you see No such file or directory after running ls -al ~/.ssh command, go ahead and create ~/.ssh directory with command mkdir -p ~/.ssh . Generate ssh keys for host \u00b6 Create a ssh pub/priv file ssh-keygen -t rsa -b 2048 -C \"mmadsen1978@gmail.com\" \\ -f '/home/npcomplete/.ssh/id_rsa_meg_github' -b bit size, i.e. the length of the key -t type of key to create -f filename, use full path Add SSH key to Github \u00b6 To get the email associated with your work account: Go to GitHub Log in to your work account Navigate to Settings Copy the associated email On running the command, you will see: Generating public/private rsa key pair. Enter file in which to save the key (/Users/<current user>/.ssh/id_rsa): Enter file name /Users/<current-user>/.ssh/id_rsa_work . Default will be id_rsa. But to differentiate between work and personal, use id_rsa_work_ and id_rsa_personal . After entering the key file name, you will be asked for entering passphrase. A passphrase is similar to a password. The purpose of the passphrase is usually to encrypt the private key. This makes the key file by itself useless to an attacker. You can just hit enter to skip Passphrase. You will see your generated key and key\u2019s randomart image. Generate SSH key for Personal account \u00b6 Follow the same steps as generating an SSH key for the work account. Save the key file as id_rsa_personal . After generating SSH keys for both work and personal account, if you enter ls -al ~/.ssh , you will see list of your keys as below: -rw------- 1 <current-user> staff 2655 Apr 5 02:18 id_rsa_work -rw-r--r-- 1 <current-user> staff 579 Apr 5 02:18 id_rsa_work.pub -rw------- 1 <current-user> staff 2655 Apr 5 02:11 id_rsa_personal -rw-r--r-- 1 <current-user> staff 571 Apr 5 02:11 id_rsa_personal.pub id_rsa_work.pub and id_rsa_personal.pub are your public SSH keys. id_rsa_work and id_rsa_personal (keys without .pub extension) are private keys. SSH keys always come in twos. The private key is stored on the client (your machine). The public key is stored on the remote machine. When you makes an attempt to connect to a remote machine (GitHub here but true in general) via SSH, the SSH protocol will check your computer for the private key that matches the public key stored on the remote machine. If they matches, the connection is successful. Step 2: Add generated SSH Keys to GitHub accounts \u00b6 For Work account \u00b6 To associate SSH key with GitHub account: Go to GitHub Login to your work account Navigate to Settings Go to SSH and GPG Keys section from the sidebar Click on New SSH Key button Paste key in Key text field. You will most likely receive a mail from GitHub that a new public key is added to your account. For Personal account \u00b6 Use key id_rsa_personal and follow the same step as work account for personal account. Step 3: Add SSH configuration rules \u00b6 Register new SSH keys To make sure if ssh-agent is running, run command eval \"$(ssh-agent -s)\" . Before configuring rules, register the new SSH Keys with the ssh-agent. Open the terminal and run: ssh-add ~/.ssh/id_rsa_work ssh-add ~/.ssh/id_rsa_personal The agent will forget the ssh key if it has not been used recently. ssh-add -l # list all keys in use by ssh agent ssh-add -D # Remove all entries in use by ssh agent ssh-add ~/.ssh/MY_KEY # Add relevant SSH key macOS ssh agent and keychain \u00b6 A private SSH keys may need a passphrase to connect to servers or git repositories. To prevent re-entering passphrase we add SSH-keys to SSH-agent running on your macOS system using the following command: ssh-add -K ~/.ssh/[your-secure-ssh-key-name] Create SSH Config File and Add Rules \u00b6 We need to add SSH config rules for different hosts to specify which identity file to use for which domain. This is way to tell SSH when to use work account and when to use personal account while performing git/github related stuff. Open newly created config file with command vim ~/.ssh/config. You can choose other editor to edit file if you are not comfortable with vim. Add rules to config file as below: # Rule for Work account Host github.com-work HostName github.com User git IdentityFile ~/.ssh/id_rsa_work # Rule for Personal account Host github.com-personal HostName github.com User git IdentityFile ~/.ssh/id_rsa_personal Things to be taken care \u00b6 Check Remote URL git remote -v This will show the correct host from the ssh config. $ git remote -v origin git@github.com-npcompleted:np-completed/aws_cloud_practitioner.git ( fetch ) origin git@github.com-npcompleted:np-completed/aws_cloud_practitioner.git ( push ) Update remote url The repository remote url requires an update after updating a ssh key and the config. Copy the host from ssh config: git@github.com-npcompleted Change remote url: git remote set-url origin git@github.com-npcompleted:np-completed/aws_cloud_practitioner.git Syntax git remote set-url <remote-name> git@<host_ssh_config>:<username>/<repo-name>.git Clone using ssh config host git clone git@github.com-personal:<username>/<repo-name>.git All set, rest will be taken care by SSH configuration!","title":"Setup SSH"},{"location":"git/setup/ssh_config/#configure-multiple-ssh-hosts-on-one-machine","text":"Most of us have multiple (mostly two) GitHub accounts, personal and work account. You need to have the ability to push and pull to multiple accounts. This post is about how to setup and use them on a single machine using HTTPS or SSH. Note: The instructions below have all been executed on macOS and should work fine on all Unix based operating systems.","title":"Configure Multiple SSH Hosts on One Machine"},{"location":"git/setup/ssh_config/#using-ssh","text":"","title":"Using SSH"},{"location":"git/setup/ssh_config/#step-1-generate-new-ssh-keys","text":"Before generating new SSH keys, check for existing SSH keys. To list existing ssh keys, go to the terminal, run command ls -al ~/.ssh , files with extension .pub are your SSH keys. You need to have ssh keys for each account. If you are planning to use two accounts personal and work, there should be two SSH keys, if not, you have to generate them. If you see No such file or directory after running ls -al ~/.ssh command, go ahead and create ~/.ssh directory with command mkdir -p ~/.ssh .","title":"Step 1: Generate new SSH Keys"},{"location":"git/setup/ssh_config/#generate-ssh-keys-for-host","text":"Create a ssh pub/priv file ssh-keygen -t rsa -b 2048 -C \"mmadsen1978@gmail.com\" \\ -f '/home/npcomplete/.ssh/id_rsa_meg_github' -b bit size, i.e. the length of the key -t type of key to create -f filename, use full path","title":"Generate ssh keys for host"},{"location":"git/setup/ssh_config/#add-ssh-key-to-github","text":"To get the email associated with your work account: Go to GitHub Log in to your work account Navigate to Settings Copy the associated email On running the command, you will see: Generating public/private rsa key pair. Enter file in which to save the key (/Users/<current user>/.ssh/id_rsa): Enter file name /Users/<current-user>/.ssh/id_rsa_work . Default will be id_rsa. But to differentiate between work and personal, use id_rsa_work_ and id_rsa_personal . After entering the key file name, you will be asked for entering passphrase. A passphrase is similar to a password. The purpose of the passphrase is usually to encrypt the private key. This makes the key file by itself useless to an attacker. You can just hit enter to skip Passphrase. You will see your generated key and key\u2019s randomart image.","title":"Add SSH key to Github"},{"location":"git/setup/ssh_config/#generate-ssh-key-for-personal-account","text":"Follow the same steps as generating an SSH key for the work account. Save the key file as id_rsa_personal . After generating SSH keys for both work and personal account, if you enter ls -al ~/.ssh , you will see list of your keys as below: -rw------- 1 <current-user> staff 2655 Apr 5 02:18 id_rsa_work -rw-r--r-- 1 <current-user> staff 579 Apr 5 02:18 id_rsa_work.pub -rw------- 1 <current-user> staff 2655 Apr 5 02:11 id_rsa_personal -rw-r--r-- 1 <current-user> staff 571 Apr 5 02:11 id_rsa_personal.pub id_rsa_work.pub and id_rsa_personal.pub are your public SSH keys. id_rsa_work and id_rsa_personal (keys without .pub extension) are private keys. SSH keys always come in twos. The private key is stored on the client (your machine). The public key is stored on the remote machine. When you makes an attempt to connect to a remote machine (GitHub here but true in general) via SSH, the SSH protocol will check your computer for the private key that matches the public key stored on the remote machine. If they matches, the connection is successful.","title":"Generate SSH key for Personal account"},{"location":"git/setup/ssh_config/#step-2-add-generated-ssh-keys-to-github-accounts","text":"","title":"Step 2: Add generated SSH Keys to GitHub accounts"},{"location":"git/setup/ssh_config/#for-work-account","text":"To associate SSH key with GitHub account: Go to GitHub Login to your work account Navigate to Settings Go to SSH and GPG Keys section from the sidebar Click on New SSH Key button Paste key in Key text field. You will most likely receive a mail from GitHub that a new public key is added to your account.","title":"For Work account"},{"location":"git/setup/ssh_config/#for-personal-account","text":"Use key id_rsa_personal and follow the same step as work account for personal account.","title":"For Personal account"},{"location":"git/setup/ssh_config/#step-3-add-ssh-configuration-rules","text":"Register new SSH keys To make sure if ssh-agent is running, run command eval \"$(ssh-agent -s)\" . Before configuring rules, register the new SSH Keys with the ssh-agent. Open the terminal and run: ssh-add ~/.ssh/id_rsa_work ssh-add ~/.ssh/id_rsa_personal The agent will forget the ssh key if it has not been used recently. ssh-add -l # list all keys in use by ssh agent ssh-add -D # Remove all entries in use by ssh agent ssh-add ~/.ssh/MY_KEY # Add relevant SSH key","title":"Step 3: Add SSH configuration rules"},{"location":"git/setup/ssh_config/#macos-ssh-agent-and-keychain","text":"A private SSH keys may need a passphrase to connect to servers or git repositories. To prevent re-entering passphrase we add SSH-keys to SSH-agent running on your macOS system using the following command: ssh-add -K ~/.ssh/[your-secure-ssh-key-name]","title":"macOS ssh agent and keychain"},{"location":"git/setup/ssh_config/#create-ssh-config-file-and-add-rules","text":"We need to add SSH config rules for different hosts to specify which identity file to use for which domain. This is way to tell SSH when to use work account and when to use personal account while performing git/github related stuff. Open newly created config file with command vim ~/.ssh/config. You can choose other editor to edit file if you are not comfortable with vim. Add rules to config file as below: # Rule for Work account Host github.com-work HostName github.com User git IdentityFile ~/.ssh/id_rsa_work # Rule for Personal account Host github.com-personal HostName github.com User git IdentityFile ~/.ssh/id_rsa_personal","title":"Create SSH Config File and Add Rules"},{"location":"git/setup/ssh_config/#things-to-be-taken-care","text":"Check Remote URL git remote -v This will show the correct host from the ssh config. $ git remote -v origin git@github.com-npcompleted:np-completed/aws_cloud_practitioner.git ( fetch ) origin git@github.com-npcompleted:np-completed/aws_cloud_practitioner.git ( push ) Update remote url The repository remote url requires an update after updating a ssh key and the config. Copy the host from ssh config: git@github.com-npcompleted Change remote url: git remote set-url origin git@github.com-npcompleted:np-completed/aws_cloud_practitioner.git Syntax git remote set-url <remote-name> git@<host_ssh_config>:<username>/<repo-name>.git Clone using ssh config host git clone git@github.com-personal:<username>/<repo-name>.git All set, rest will be taken care by SSH configuration!","title":"Things to be taken care"},{"location":"hosting-and-deployment/enable_github_pages/","text":"Enable Github Pages \u00b6 Github pages requires permissions to be enabled but the process is pretty easy. Create a public repository Navigate to Settings > Pages Configure the branch and build directory. \u2757 github.io requires using the owner name to make the web site visible.","title":"Enable GH Pages"},{"location":"hosting-and-deployment/enable_github_pages/#enable-github-pages","text":"Github pages requires permissions to be enabled but the process is pretty easy. Create a public repository Navigate to Settings > Pages Configure the branch and build directory. \u2757 github.io requires using the owner name to make the web site visible.","title":"Enable Github Pages"},{"location":"hosting-and-deployment/gh-pages-private-repo/","text":"Github Pages Private Source Code \u00b6 Github pages are great, it provides a free static page hosting, but the only caveat is the repository has to be public repository. And, if you want to keep your source private, you will have to opt for premium plans to host pages from private repository. Here is what I have done, github allows unlimited private repositories, so I created a new private repo where I kept my source code and another repo where my site is hosted. Create a private github repository Create a public github repository https://github.com/np-completed/np-completed.github.io Save PAT for github action Host site URL https://np-completed.github.io/ Create Personal Token \u00b6 Create a personal access token. Navigate to Settings and create a PAT. Select Developer settings Select Personal access tokens Now generate a new token, with repo permissions. Once you are done copy the generated token, we will need to set this token during our build. Create secret in private repository \u00b6 Go to your private repo and navigate to the settings. We will add the PAT created a moment ago: Create github action in private repository \u00b6 This is where the magic begins, we will build a github action in our private repo. You will need to create a file at .github/workflows/ci.yml name : Build & Publish on : push : branches : [ gh_pages ] pull_request : branches : [ gh_pages ] jobs : build : runs-on : ubuntu-latest steps : - name : Checkout local code uses : actions/checkout@v3 with : path : code token : ${{ secrets.GH_PAGES}} ref : gh_pages # - name: Show Directory Files # run : | # cd code # ls -la - name : python uses : actions/setup-python@v4 with : python-version : \"3.10\" - name : Checkout public repo site uses : actions/checkout@v3 with : token : ${{ secrets.GH_PAGES}} repository : np-completed/np-completed.github.io ref : gh_pages path : site - name : Install dependencies run : python3 -m pip install -r code/requirements.txt - name : Build website run : mkdocs build --config-file code/mkdocs.yml - name : Clean Website run : | pushd site git rm -rf . popd - name : Copy website run : | pushd site # cp -rvf ../code/build/* . cp -rvf ../code/site/* . popd ls -la site/ # ls -la code/site/ - name : Deploy and Publish run : | git config --global user.email \"${GITHUB_ACTOR}@users.noreply.github.com\" git config --global user.name \"github-actions\" pushd site git add . git commit -m \"mkdocs build from Action ${GITHUB_SHA}\" git push origin gh_pages popd Each push changes to the private repository triggers the github action. Next, the GH action job will be executed, to build and publish the site to the public repo without exposing your source code.","title":"Private Repo with GH Pages"},{"location":"hosting-and-deployment/gh-pages-private-repo/#github-pages-private-source-code","text":"Github pages are great, it provides a free static page hosting, but the only caveat is the repository has to be public repository. And, if you want to keep your source private, you will have to opt for premium plans to host pages from private repository. Here is what I have done, github allows unlimited private repositories, so I created a new private repo where I kept my source code and another repo where my site is hosted. Create a private github repository Create a public github repository https://github.com/np-completed/np-completed.github.io Save PAT for github action Host site URL https://np-completed.github.io/","title":"Github Pages Private Source Code"},{"location":"hosting-and-deployment/gh-pages-private-repo/#create-personal-token","text":"Create a personal access token. Navigate to Settings and create a PAT. Select Developer settings Select Personal access tokens Now generate a new token, with repo permissions. Once you are done copy the generated token, we will need to set this token during our build.","title":"Create Personal Token"},{"location":"hosting-and-deployment/gh-pages-private-repo/#create-secret-in-private-repository","text":"Go to your private repo and navigate to the settings. We will add the PAT created a moment ago:","title":"Create secret in private repository"},{"location":"hosting-and-deployment/gh-pages-private-repo/#create-github-action-in-private-repository","text":"This is where the magic begins, we will build a github action in our private repo. You will need to create a file at .github/workflows/ci.yml name : Build & Publish on : push : branches : [ gh_pages ] pull_request : branches : [ gh_pages ] jobs : build : runs-on : ubuntu-latest steps : - name : Checkout local code uses : actions/checkout@v3 with : path : code token : ${{ secrets.GH_PAGES}} ref : gh_pages # - name: Show Directory Files # run : | # cd code # ls -la - name : python uses : actions/setup-python@v4 with : python-version : \"3.10\" - name : Checkout public repo site uses : actions/checkout@v3 with : token : ${{ secrets.GH_PAGES}} repository : np-completed/np-completed.github.io ref : gh_pages path : site - name : Install dependencies run : python3 -m pip install -r code/requirements.txt - name : Build website run : mkdocs build --config-file code/mkdocs.yml - name : Clean Website run : | pushd site git rm -rf . popd - name : Copy website run : | pushd site # cp -rvf ../code/build/* . cp -rvf ../code/site/* . popd ls -la site/ # ls -la code/site/ - name : Deploy and Publish run : | git config --global user.email \"${GITHUB_ACTOR}@users.noreply.github.com\" git config --global user.name \"github-actions\" pushd site git add . git commit -m \"mkdocs build from Action ${GITHUB_SHA}\" git push origin gh_pages popd Each push changes to the private repository triggers the github action. Next, the GH action job will be executed, to build and publish the site to the public repo without exposing your source code.","title":"Create github action in private repository"},{"location":"hosting-and-deployment/gh-pages/","text":"Host on GitHub Pages \u00b6 Demo site on GitHub Pages (build & deploy with GitHub Actions) Build and deploy with GitHub Actions \u00b6 peaceiris/actions-gh-pages: GitHub Actions for deploying to GitHub Pages with Static Site Generators Go to the repository and read the latest README.md for more details. Build and deploy with mkdocs gh-deploy \u00b6 pipenv \u00b6 pipenv run deploy # OR pipenv shell mkdocs gh-deploy # OR pipenv run mkdocs gh-deploy","title":"Github Pages"},{"location":"hosting-and-deployment/gh-pages/#host-on-github-pages","text":"Demo site on GitHub Pages (build & deploy with GitHub Actions)","title":"Host on GitHub Pages"},{"location":"hosting-and-deployment/gh-pages/#build-and-deploy-with-github-actions","text":"peaceiris/actions-gh-pages: GitHub Actions for deploying to GitHub Pages with Static Site Generators Go to the repository and read the latest README.md for more details.","title":"Build and deploy with GitHub Actions"},{"location":"hosting-and-deployment/gh-pages/#build-and-deploy-with-mkdocs-gh-deploy","text":"","title":"Build and deploy with mkdocs gh-deploy"},{"location":"hosting-and-deployment/gh-pages/#pipenv","text":"pipenv run deploy # OR pipenv shell mkdocs gh-deploy # OR pipenv run mkdocs gh-deploy","title":"pipenv"},{"location":"markdown/content/","text":"Collapsible Content \u00b6 These sections collapse to hide content. A widget expands it to show the content. Library: dataclasses The dataclasses package helps you to wrap your data in a Python class. This would be as opposed to holding it in a dictionary. This give the data structure features similar to a javascript object. If you have used the Python package you are already familar with a library that is essentially dataclasses package used for data validation called pydantic . Dataclasses work in an almost identical way without the data validation part. So here is dataclasses in action. Instead of holding your data in a dictionary, you can can define a class for it and call it with dot notation like below. from dataclasses import dataclass , field @dataclass class Chemical : common_name : str symbol : str mass : float potassium = Chemical ( \"Potassium\" , \"K\" , 39.1 ) print ( potassium . mass * 2 ) YT Video References: v1 , v2","title":"Content"},{"location":"markdown/content/#collapsible-content","text":"These sections collapse to hide content. A widget expands it to show the content. Library: dataclasses The dataclasses package helps you to wrap your data in a Python class. This would be as opposed to holding it in a dictionary. This give the data structure features similar to a javascript object. If you have used the Python package you are already familar with a library that is essentially dataclasses package used for data validation called pydantic . Dataclasses work in an almost identical way without the data validation part. So here is dataclasses in action. Instead of holding your data in a dictionary, you can can define a class for it and call it with dot notation like below. from dataclasses import dataclass , field @dataclass class Chemical : common_name : str symbol : str mass : float potassium = Chemical ( \"Potassium\" , \"K\" , 39.1 ) print ( potassium . mass * 2 ) YT Video References: v1 , v2","title":"Collapsible Content"},{"location":"markdown/emoji_list/","text":"emoji-cheat-sheet \u00b6 This cheat sheet is automatically generated from GitHub Emoji API and Unicode Full Emoji List . Table of Contents \u00b6 Smileys & Emotion People & Body Animals & Nature Food & Drink Travel & Places Activities Objects Symbols Flags GitHub Custom Emoji Smileys & Emotion \u00b6 Face Smiling Face Affection Face Tongue Face Hand Face Neutral Skeptical Face Sleepy Face Unwell Face Hat Face Glasses Face Concerned Face Negative Face Costume Cat Face Monkey Face Emotion Face Smiling \u00b6 ico shortcode ico shortcode top :grinning: :smiley: top top :smile: :grin: top top :laughing: :satisfied: :sweat_smile: top top :rofl: :joy: top top :slightly_smiling_face: :upside_down_face: top top :wink: :blush: top top :innocent: top Face Affection \u00b6 ico shortcode ico shortcode top :smiling_face_with_three_hearts: :smiling_face_with_three_hearts: :heart_eyes: top top :star_struck: :kissing_heart: top top :kissing: :relaxed: top top :kissing_closed_eyes: :kissing_smiling_eyes: top top :smiling_face_with_tear: top Face Tongue \u00b6 ico shortcode ico shortcode top :yum: :stuck_out_tongue: top top :stuck_out_tongue_winking_eye: :zany_face: top top :stuck_out_tongue_closed_eyes: :money_mouth_face: top Face Hand \u00b6 ico shortcode ico shortcode top :hugs: :hugs: :hand_over_mouth: :hand_over_mouth: top top :shushing_face: :thinking: top Face Neutral Skeptical \u00b6 ico shortcode ico shortcode top :zipper_mouth_face: :raised_eyebrow: :raised_eyebrow: top top :neutral_face: :expressionless: top top :no_mouth: :face_in_clouds: top top :smirk: :unamused: top top :roll_eyes: :roll_eyes: :grimacing: top top :face_exhaling: :lying_face: top Face Sleepy \u00b6 ico shortcode ico shortcode top :relieved: :pensive: top top :sleepy: :drooling_face: top top :sleeping: top Face Unwell \u00b6 ico shortcode ico shortcode top :mask: :face_with_thermometer: top top :face_with_head_bandage: :nauseated_face: top top :vomiting_face: :vomiting_face: :sneezing_face: top top :hot_face: :cold_face: top top :woozy_face: :dizzy_face: top top :face_with_spiral_eyes: :exploding_head: top Face Hat \u00b6 ico shortcode ico shortcode top :cowboy_hat_face: :cowboy_hat_face: :partying_face: top top :disguised_face: top Face Glasses \u00b6 ico shortcode ico shortcode top :sunglasses: :nerd_face: top top :monocle_face: :monocle_face: top Face Concerned \u00b6 ico shortcode ico shortcode top :confused: :worried: top top :slightly_frowning_face: :frowning_face: :frowning_face: top top :open_mouth: :hushed: top top :astonished: :flushed: top top :pleading_face: :frowning: top top :anguished: :fearful: top top :cold_sweat: :disappointed_relieved: top top :cry: :sob: top top :scream: :confounded: top top :persevere: :disappointed: top top :sweat: :weary: top top :tired_face: :yawning_face: top Face Negative \u00b6 ico shortcode ico shortcode top :triumph: :pout: :pout: :rage: top top :angry: :cursing_face: :cursing_face: top top :smiling_imp: :imp: top top :skull: :skull_and_crossbones: top Face Costume \u00b6 ico shortcode ico shortcode top :hankey: :poop: :shit: :clown_face: top top :japanese_ogre: :japanese_goblin: top top :ghost: :alien: top top :space_invader: :robot: top Cat Face \u00b6 ico shortcode ico shortcode top :smiley_cat: :smile_cat: top top :joy_cat: :heart_eyes_cat: top top :smirk_cat: :kissing_cat: top top :scream_cat: :crying_cat_face: top top :pouting_cat: top Monkey Face \u00b6 ico shortcode ico shortcode top :see_no_evil: :hear_no_evil: top top :speak_no_evil: top Emotion \u00b6 ico shortcode ico shortcode top :kiss: :love_letter: top top :cupid: :gift_heart: top top :sparkling_heart: :heartpulse: top top :heartbeat: :revolving_hearts: top top :two_hearts: :heart_decoration: top top :heavy_heart_exclamation: :heavy_heart_exclamation: :broken_heart: top top :heart_on_fire: :mending_heart: top top :heart: :orange_heart: top top :yellow_heart: :green_heart: top top :blue_heart: :purple_heart: top top :brown_heart: :black_heart: top top :white_heart: :100: top top :anger: :boom: :collision: top top :dizzy: :sweat_drops: top top :dash: :hole: top top :bomb: :speech_balloon: top top :eye_speech_bubble: :eye_speech_bubble: :left_speech_bubble: top top :right_anger_bubble: :thought_balloon: top top :zzz: top People & Body \u00b6 Hand Fingers Open Hand Fingers Partial Hand Single Finger Hand Fingers Closed Hands Hand Prop Body Parts Person Person Gesture Person Role Person Fantasy Person Activity Person Sport Person Resting Family Person Symbol Hand Fingers Open \u00b6 ico shortcode ico shortcode top :wave: :raised_back_of_hand: top top :raised_hand_with_fingers_splayed: :hand: :hand: :raised_hand: top top :vulcan_salute: :vulcan_salute: top Hand Fingers Partial \u00b6 ico shortcode ico shortcode top :ok_hand: :pinched_fingers: top top :pinching_hand: :v: top top :crossed_fingers: :crossed_fingers: :love_you_gesture: top top :metal: :call_me_hand: top Hand Single Finger \u00b6 ico shortcode ico shortcode top :point_left: :point_right: top top :point_up_2: :fu: :fu: :middle_finger: top top :point_down: :point_up: top Hand Fingers Closed \u00b6 ico shortcode ico shortcode top :+1: :thumbsup: :-1: :thumbsdown: top top :fist: :fist_raised: :facepunch: :facepunch: :fist_oncoming: :punch: top top :fist_left: :fist_left: :fist_right: :fist_right: top Hands \u00b6 ico shortcode ico shortcode top :clap: :raised_hands: top top :open_hands: :palms_up_together: top top :handshake: :pray: top Hand Prop \u00b6 ico shortcode ico shortcode top :writing_hand: :nail_care: top top :selfie: top Body Parts \u00b6 ico shortcode ico shortcode top :muscle: :mechanical_arm: top top :mechanical_leg: :leg: top top :foot: :ear: top top :ear_with_hearing_aid: :nose: top top :brain: :anatomical_heart: top top :lungs: :tooth: top top :bone: :eyes: top top :eye: :tongue: top top :lips: top Person \u00b6 ico shortcode ico shortcode top :baby: :child: top top :boy: :girl: top top :adult: :blond_haired_person: top top :man: :bearded_person: top top :man_beard: :woman_beard: top top :red_haired_man: :red_haired_man: :curly_haired_man: :curly_haired_man: top top :white_haired_man: :white_haired_man: :bald_man: :bald_man: top top :woman: :red_haired_woman: :red_haired_woman: top top :person_red_hair: :curly_haired_woman: :curly_haired_woman: top top :person_curly_hair: :white_haired_woman: :white_haired_woman: top top :person_white_hair: :bald_woman: :bald_woman: top top :person_bald: :blond_haired_woman: :blond_haired_woman: :blonde_woman: top top :blond_haired_man: :blond_haired_man: :older_adult: top top :older_man: :older_woman: top Person Gesture \u00b6 ico shortcode ico shortcode top :frowning_person: :frowning_person: :frowning_man: :frowning_man: top top :frowning_woman: :frowning_woman: :pouting_face: :pouting_face: top top :pouting_man: :pouting_man: :pouting_woman: :pouting_woman: top top :no_good: :ng_man: :ng_man: :no_good_man: top top :ng_woman: :ng_woman: :no_good_woman: :ok_person: :ok_person: top top :ok_man: :ok_man: :ok_woman: top top :information_desk_person: :tipping_hand_person: :sassy_man: :sassy_man: :tipping_hand_man: top top :sassy_woman: :sassy_woman: :tipping_hand_woman: :raising_hand: top top :raising_hand_man: :raising_hand_man: :raising_hand_woman: :raising_hand_woman: top top :deaf_person: :deaf_man: top top :deaf_woman: :bow: top top :bowing_man: :bowing_man: :bowing_woman: :bowing_woman: top top :facepalm: :man_facepalming: top top :woman_facepalming: :shrug: top top :man_shrugging: :woman_shrugging: top Person Role \u00b6 ico shortcode ico shortcode top :health_worker: :man_health_worker: top top :woman_health_worker: :student: top top :man_student: :woman_student: top top :teacher: :man_teacher: top top :woman_teacher: :judge: top top :man_judge: :woman_judge: top top :farmer: :man_farmer: top top :woman_farmer: :cook: top top :man_cook: :woman_cook: top top :mechanic: :man_mechanic: top top :woman_mechanic: :factory_worker: top top :man_factory_worker: :woman_factory_worker: top top :office_worker: :man_office_worker: top top :woman_office_worker: :scientist: top top :man_scientist: :woman_scientist: top top :technologist: :man_technologist: top top :woman_technologist: :singer: top top :man_singer: :woman_singer: top top :artist: :man_artist: top top :woman_artist: :pilot: top top :man_pilot: :woman_pilot: top top :astronaut: :man_astronaut: top top :woman_astronaut: :firefighter: top top :man_firefighter: :woman_firefighter: top top :cop: :police_officer: :policeman: :policeman: top top :policewoman: :policewoman: :detective: top top :male_detective: :male_detective: :female_detective: :female_detective: top top :guard: :guardsman: top top :guardswoman: :guardswoman: :ninja: top top :construction_worker: :construction_worker_man: :construction_worker_man: top top :construction_worker_woman: :construction_worker_woman: :prince: top top :princess: :person_with_turban: :person_with_turban: top top :man_with_turban: :woman_with_turban: :woman_with_turban: top top :man_with_gua_pi_mao: :woman_with_headscarf: top top :person_in_tuxedo: :man_in_tuxedo: top top :woman_in_tuxedo: :person_with_veil: top top :man_with_veil: :bride_with_veil: :bride_with_veil: :woman_with_veil: top top :pregnant_woman: :breast_feeding: top top :woman_feeding_baby: :man_feeding_baby: top top :person_feeding_baby: top Person Fantasy \u00b6 ico shortcode ico shortcode top :angel: :santa: top top :mrs_claus: :mx_claus: top top :superhero: :superhero_man: :superhero_man: top top :superhero_woman: :superhero_woman: :supervillain: top top :supervillain_man: :supervillain_man: :supervillain_woman: :supervillain_woman: top top :mage: :mage_man: :mage_man: top top :mage_woman: :mage_woman: :fairy: top top :fairy_man: :fairy_man: :fairy_woman: :fairy_woman: top top :vampire: :vampire_man: :vampire_man: top top :vampire_woman: :vampire_woman: :merperson: top top :merman: :mermaid: top top :elf: :elf_man: :elf_man: top top :elf_woman: :elf_woman: :genie: top top :genie_man: :genie_man: :genie_woman: :genie_woman: top top :zombie: :zombie_man: :zombie_man: top top :zombie_woman: :zombie_woman: top Person Activity \u00b6 ico shortcode ico shortcode top :massage: :massage_man: :massage_man: top top :massage_woman: :massage_woman: :haircut: top top :haircut_man: :haircut_man: :haircut_woman: :haircut_woman: top top :walking: :walking_man: :walking_man: top top :walking_woman: :walking_woman: :standing_person: :standing_person: top top :standing_man: :standing_man: :standing_woman: :standing_woman: top top :kneeling_person: :kneeling_person: :kneeling_man: :kneeling_man: top top :kneeling_woman: :kneeling_woman: :person_with_probing_cane: top top :man_with_probing_cane: :woman_with_probing_cane: top top :person_in_motorized_wheelchair: :man_in_motorized_wheelchair: top top :woman_in_motorized_wheelchair: :person_in_manual_wheelchair: top top :man_in_manual_wheelchair: :woman_in_manual_wheelchair: top top :runner: :running: :running_man: :running_man: top top :running_woman: :running_woman: :dancer: :woman_dancing: top top :man_dancing: :business_suit_levitating: :business_suit_levitating: top top :dancers: :dancing_men: :dancing_men: top top :dancing_women: :dancing_women: :sauna_person: :sauna_person: top top :sauna_man: :sauna_man: :sauna_woman: :sauna_woman: top top :climbing: :climbing: :climbing_man: :climbing_man: top top :climbing_woman: :climbing_woman: top Person Sport \u00b6 ico shortcode ico shortcode top :person_fencing: :horse_racing: top top :skier: :snowboarder: top top :golfing: :golfing: :golfing_man: :golfing_man: top top :golfing_woman: :golfing_woman: :surfer: top top :surfing_man: :surfing_man: :surfing_woman: :surfing_woman: top top :rowboat: :rowing_man: :rowing_man: top top :rowing_woman: :rowing_woman: :swimmer: top top :swimming_man: :swimming_man: :swimming_woman: :swimming_woman: top top :bouncing_ball_person: :bouncing_ball_person: :basketball_man: :basketball_man: :bouncing_ball_man: top top :basketball_woman: :basketball_woman: :bouncing_ball_woman: :weight_lifting: :weight_lifting: top top :weight_lifting_man: :weight_lifting_man: :weight_lifting_woman: :weight_lifting_woman: top top :bicyclist: :biking_man: :biking_man: top top :biking_woman: :biking_woman: :mountain_bicyclist: top top :mountain_biking_man: :mountain_biking_man: :mountain_biking_woman: :mountain_biking_woman: top top :cartwheeling: :cartwheeling: :man_cartwheeling: top top :woman_cartwheeling: :wrestling: top top :men_wrestling: :women_wrestling: top top :water_polo: :man_playing_water_polo: top top :woman_playing_water_polo: :handball_person: :handball_person: top top :man_playing_handball: :woman_playing_handball: top top :juggling_person: :juggling_person: :man_juggling: top top :woman_juggling: top Person Resting \u00b6 ico shortcode ico shortcode top :lotus_position: :lotus_position: :lotus_position_man: :lotus_position_man: top top :lotus_position_woman: :lotus_position_woman: :bath: top top :sleeping_bed: :sleeping_bed: top Family \u00b6 ico shortcode ico shortcode top :people_holding_hands: :two_women_holding_hands: top top :couple: :two_men_holding_hands: top top :couplekiss: :couplekiss_man_woman: :couplekiss_man_woman: top top :couplekiss_man_man: :couplekiss_man_man: :couplekiss_woman_woman: :couplekiss_woman_woman: top top :couple_with_heart: :couple_with_heart_woman_man: top top :couple_with_heart_man_man: :couple_with_heart_man_man: :couple_with_heart_woman_woman: :couple_with_heart_woman_woman: top top :family: :family_man_woman_boy: top top :family_man_woman_girl: :family_man_woman_girl: :family_man_woman_girl_boy: :family_man_woman_girl_boy: top top :family_man_woman_boy_boy: :family_man_woman_boy_boy: :family_man_woman_girl_girl: :family_man_woman_girl_girl: top top :family_man_man_boy: :family_man_man_boy: :family_man_man_girl: :family_man_man_girl: top top :family_man_man_girl_boy: :family_man_man_girl_boy: :family_man_man_boy_boy: :family_man_man_boy_boy: top top :family_man_man_girl_girl: :family_man_man_girl_girl: :family_woman_woman_boy: :family_woman_woman_boy: top top :family_woman_woman_girl: :family_woman_woman_girl: :family_woman_woman_girl_boy: :family_woman_woman_girl_boy: top top :family_woman_woman_boy_boy: :family_woman_woman_boy_boy: :family_woman_woman_girl_girl: :family_woman_woman_girl_girl: top top :family_man_boy: :family_man_boy_boy: top top :family_man_girl: :family_man_girl_boy: top top :family_man_girl_girl: :family_woman_boy: top top :family_woman_boy_boy: :family_woman_girl: top top :family_woman_girl_boy: :family_woman_girl_girl: top Person Symbol \u00b6 ico shortcode ico shortcode top :speaking_head: :bust_in_silhouette: top top :busts_in_silhouette: :people_hugging: top top :footprints: top Animals & Nature \u00b6 Animal Mammal Animal Bird Animal Amphibian Animal Reptile Animal Marine Animal Bug Plant Flower Plant Other Animal Mammal \u00b6 ico shortcode ico shortcode top :monkey_face: :monkey: top top :gorilla: :orangutan: top top :dog: :dog2: top top :guide_dog: :service_dog: top top :poodle: :wolf: top top :fox_face: :raccoon: top top :cat: :cat2: top top :black_cat: :lion: top top :tiger: :tiger2: top top :leopard: :horse: top top :racehorse: :unicorn: top top :zebra: :deer: top top :bison: :cow: top top :ox: :water_buffalo: top top :cow2: :pig: top top :pig2: :boar: top top :pig_nose: :ram: top top :sheep: :goat: top top :dromedary_camel: :camel: top top :llama: :giraffe: top top :elephant: :mammoth: top top :rhinoceros: :hippopotamus: top top :mouse: :mouse2: top top :rat: :hamster: top top :rabbit: :rabbit2: top top :chipmunk: :beaver: top top :hedgehog: :bat: top top :bear: :polar_bear: top top :koala: :panda_face: top top :sloth: :otter: top top :skunk: :kangaroo: top top :badger: :feet: :paw_prints: top Animal Bird \u00b6 ico shortcode ico shortcode top :turkey: :chicken: top top :rooster: :hatching_chick: top top :baby_chick: :hatched_chick: top top :bird: :penguin: top top :dove: :eagle: top top :duck: :swan: top top :owl: :dodo: top top :feather: :flamingo: top top :peacock: :parrot: top Animal Amphibian \u00b6 ico shortcode top :frog: top Animal Reptile \u00b6 ico shortcode ico shortcode top :crocodile: :turtle: top top :lizard: :snake: top top :dragon_face: :dragon: top top :sauropod: :t-rex: :t-rex: top Animal Marine \u00b6 ico shortcode ico shortcode top :whale: :whale2: top top :dolphin: :flipper: :seal: top top :fish: :tropical_fish: top top :blowfish: :shark: top top :octopus: :shell: top Animal Bug \u00b6 ico shortcode ico shortcode top :snail: :butterfly: top top :bug: :ant: top top :bee: :honeybee: :beetle: top top :lady_beetle: :cricket: top top :cockroach: :spider: top top :spider_web: :scorpion: top top :mosquito: :fly: top top :worm: :microbe: top Plant Flower \u00b6 ico shortcode ico shortcode top :bouquet: :cherry_blossom: top top :white_flower: :rosette: top top :rose: :wilted_flower: top top :hibiscus: :sunflower: top top :blossom: :tulip: top Plant Other \u00b6 ico shortcode ico shortcode top :seedling: :potted_plant: top top :evergreen_tree: :deciduous_tree: top top :palm_tree: :cactus: top top :ear_of_rice: :herb: top top :shamrock: :four_leaf_clover: top top :maple_leaf: :fallen_leaf: top top :leaves: top Food & Drink \u00b6 Food Fruit Food Vegetable Food Prepared Food Asian Food Marine Food Sweet Drink Dishware Food Fruit \u00b6 ico shortcode ico shortcode top :grapes: :melon: top top :watermelon: :mandarin: :mandarin: :orange: :tangerine: top top :lemon: :banana: top top :pineapple: :mango: top top :apple: :green_apple: top top :pear: :peach: top top :cherries: :strawberry: top top :blueberries: :kiwi_fruit: :kiwi_fruit: top top :tomato: :olive: top top :coconut: top Food Vegetable \u00b6 ico shortcode ico shortcode top :avocado: :eggplant: top top :potato: :carrot: top top :corn: :hot_pepper: top top :bell_pepper: :cucumber: top top :leafy_green: :broccoli: top top :garlic: :onion: top top :mushroom: :peanuts: top top :chestnut: top Food Prepared \u00b6 ico shortcode ico shortcode top :bread: :croissant: top top :baguette_bread: :flatbread: top top :pretzel: :bagel: top top :pancakes: :waffle: top top :cheese: :meat_on_bone: top top :poultry_leg: :cut_of_meat: top top :bacon: :hamburger: top top :fries: :pizza: top top :hotdog: :sandwich: top top :taco: :burrito: top top :tamale: :stuffed_flatbread: top top :falafel: :egg: top top :fried_egg: :fried_egg: :shallow_pan_of_food: top top :stew: :fondue: top top :bowl_with_spoon: :green_salad: top top :popcorn: :butter: top top :salt: :canned_food: top Food Asian \u00b6 ico shortcode ico shortcode top :bento: :rice_cracker: top top :rice_ball: :rice: top top :curry: :ramen: top top :spaghetti: :sweet_potato: top top :oden: :sushi: top top :fried_shrimp: :fish_cake: top top :moon_cake: :dango: top top :dumpling: :fortune_cookie: top top :takeout_box: top Food Marine \u00b6 ico shortcode ico shortcode top :crab: :lobster: top top :shrimp: :squid: top top :oyster: top Food Sweet \u00b6 ico shortcode ico shortcode top :icecream: :shaved_ice: top top :ice_cream: :doughnut: top top :cookie: :birthday: top top :cake: :cupcake: top top :pie: :chocolate_bar: top top :candy: :lollipop: top top :custard: :honey_pot: top Drink \u00b6 ico shortcode ico shortcode top :baby_bottle: :milk_glass: :milk_glass: top top :coffee: :teapot: top top :tea: :sake: top top :champagne: :wine_glass: top top :cocktail: :tropical_drink: top top :beer: :beers: top top :clinking_glasses: :clinking_glasses: :tumbler_glass: top top :cup_with_straw: :bubble_tea: top top :beverage_box: :mate: top top :ice_cube: top Dishware \u00b6 ico shortcode ico shortcode top :chopsticks: :plate_with_cutlery: :plate_with_cutlery: top top :fork_and_knife: :spoon: top top :hocho: :hocho: :knife: :amphora: top Travel & Places \u00b6 Place Map Place Geographic Place Building Place Religious Place Other Transport Ground Transport Water Transport Air Hotel Time Sky & Weather Place Map \u00b6 ico shortcode ico shortcode top :earth_africa: :earth_americas: top top :earth_asia: :globe_with_meridians: top top :world_map: :japan: top top :compass: top Place Geographic \u00b6 ico shortcode ico shortcode top :mountain_snow: :mountain: top top :volcano: :mount_fuji: top top :camping: :beach_umbrella: top top :desert: :desert_island: top top :national_park: top Place Building \u00b6 ico shortcode ico shortcode top :stadium: :classical_building: top top :building_construction: :bricks: top top :rock: :wood: top top :hut: :houses: :houses: top top :derelict_house: :derelict_house: :house: top top :house_with_garden: :office: top top :post_office: :european_post_office: top top :hospital: :bank: top top :hotel: :love_hotel: top top :convenience_store: :school: top top :department_store: :factory: top top :japanese_castle: :european_castle: top top :wedding: :tokyo_tower: top top :statue_of_liberty: top Place Religious \u00b6 ico shortcode ico shortcode top :church: :mosque: top top :hindu_temple: :synagogue: top top :shinto_shrine: :kaaba: top Place Other \u00b6 ico shortcode ico shortcode top :fountain: :tent: top top :foggy: :night_with_stars: top top :cityscape: :sunrise_over_mountains: top top :sunrise: :city_sunset: top top :city_sunrise: :bridge_at_night: top top :hotsprings: :carousel_horse: top top :ferris_wheel: :roller_coaster: top top :barber: :circus_tent: top Transport Ground \u00b6 ico shortcode ico shortcode top :steam_locomotive: :railway_car: top top :bullettrain_side: :bullettrain_front: top top :train2: :metro: top top :light_rail: :station: top top :tram: :monorail: top top :mountain_railway: :train: top top :bus: :oncoming_bus: top top :trolleybus: :minibus: top top :ambulance: :fire_engine: top top :police_car: :oncoming_police_car: top top :taxi: :oncoming_taxi: top top :car: :car: :red_car: :oncoming_automobile: top top :blue_car: :pickup_truck: top top :truck: :articulated_lorry: top top :tractor: :racing_car: top top :motorcycle: :motor_scooter: top top :manual_wheelchair: :motorized_wheelchair: top top :auto_rickshaw: :bike: top top :kick_scooter: :kick_scooter: :skateboard: top top :roller_skate: :busstop: top top :motorway: :railway_track: top top :oil_drum: :fuelpump: top top :rotating_light: :traffic_light: top top :vertical_traffic_light: :stop_sign: top top :construction: top Transport Water \u00b6 ico shortcode ico shortcode top :anchor: :boat: :boat: :sailboat: top top :canoe: :speedboat: top top :passenger_ship: :ferry: top top :motor_boat: :motor_boat: :ship: top Transport Air \u00b6 ico shortcode ico shortcode top :airplane: :small_airplane: top top :flight_departure: :flight_departure: :flight_arrival: :flight_arrival: top top :parachute: :seat: top top :helicopter: :suspension_railway: top top :mountain_cableway: :aerial_tramway: top top :artificial_satellite: :artificial_satellite: :rocket: top top :flying_saucer: top Hotel \u00b6 ico shortcode ico shortcode top :bellhop_bell: :luggage: top Time \u00b6 ico shortcode ico shortcode top :hourglass: :hourglass_flowing_sand: top top :watch: :alarm_clock: top top :stopwatch: :timer_clock: top top :mantelpiece_clock: :mantelpiece_clock: :clock12: top top :clock1230: :clock1: top top :clock130: :clock2: top top :clock230: :clock3: top top :clock330: :clock4: top top :clock430: :clock5: top top :clock530: :clock6: top top :clock630: :clock7: top top :clock730: :clock8: top top :clock830: :clock9: top top :clock930: :clock10: top top :clock1030: :clock11: top top :clock1130: top Sky & Weather \u00b6 ico shortcode ico shortcode top :new_moon: :waxing_crescent_moon: top top :first_quarter_moon: :moon: :moon: :waxing_gibbous_moon: top top :full_moon: :waning_gibbous_moon: top top :last_quarter_moon: :waning_crescent_moon: top top :crescent_moon: :new_moon_with_face: top top :first_quarter_moon_with_face: :last_quarter_moon_with_face: top top :thermometer: :sunny: top top :full_moon_with_face: :sun_with_face: top top :ringed_planet: :star: top top :star2: :stars: top top :milky_way: :cloud: top top :partly_sunny: :cloud_with_lightning_and_rain: :cloud_with_lightning_and_rain: top top :sun_behind_small_cloud: :sun_behind_small_cloud: :sun_behind_large_cloud: :sun_behind_large_cloud: top top :sun_behind_rain_cloud: :sun_behind_rain_cloud: :cloud_with_rain: top top :cloud_with_snow: :cloud_with_lightning: top top :tornado: :tornado: :fog: top top :wind_face: :wind_face: :cyclone: top top :rainbow: :closed_umbrella: top top :open_umbrella: :open_umbrella: :umbrella: top top :parasol_on_ground: :parasol_on_ground: :zap: top top :snowflake: :snowman_with_snow: :snowman_with_snow: top top :snowman: :comet: top top :fire: :droplet: top top :ocean: top Activities \u00b6 Event Award Medal Sport Game Arts & Crafts Event \u00b6 ico shortcode ico shortcode top :jack_o_lantern: :christmas_tree: top top :fireworks: :sparkler: top top :firecracker: :sparkles: top top :balloon: :tada: top top :confetti_ball: :tanabata_tree: top top :bamboo: :dolls: top top :flags: :wind_chime: top top :rice_scene: :red_envelope: top top :ribbon: :gift: top top :reminder_ribbon: :tickets: top top :ticket: top Award Medal \u00b6 ico shortcode ico shortcode top :medal_military: :medal_military: :trophy: top top :medal_sports: :medal_sports: :1st_place_medal: :1st_place_medal: top top :2nd_place_medal: :2nd_place_medal: :3rd_place_medal: :3rd_place_medal: top Sport \u00b6 ico shortcode ico shortcode top :soccer: :baseball: top top :softball: :basketball: top top :volleyball: :football: top top :rugby_football: :tennis: top top :flying_disc: :bowling: top top :cricket_game: :field_hockey: top top :ice_hockey: :ice_hockey: :lacrosse: top top :ping_pong: :badminton: top top :boxing_glove: :martial_arts_uniform: top top :goal_net: :golf: top top :ice_skate: :fishing_pole_and_fish: top top :diving_mask: :running_shirt_with_sash: top top :ski: :sled: top top :curling_stone: top Game \u00b6 ico shortcode ico shortcode top :dart: :yo_yo: top top :kite: :8ball: top top :crystal_ball: :magic_wand: top top :nazar_amulet: :video_game: top top :joystick: :slot_machine: top top :game_die: :jigsaw: top top :teddy_bear: :pinata: :pinata: top top :nesting_dolls: :spades: top top :hearts: :diamonds: top top :clubs: :chess_pawn: top top :black_joker: :mahjong: top top :flower_playing_cards: top Arts & Crafts \u00b6 ico shortcode ico shortcode top :performing_arts: :framed_picture: :framed_picture: top top :art: :thread: top top :sewing_needle: :yarn: top top :knot: top Objects \u00b6 Clothing Sound Music Musical Instrument Phone Computer Light & Video Book Paper Money Mail Writing Office Lock Tool Science Medical Household Other Object Clothing \u00b6 ico shortcode ico shortcode top :eyeglasses: :dark_sunglasses: top top :goggles: :lab_coat: top top :safety_vest: :necktie: top top :shirt: :tshirt: :jeans: top top :scarf: :gloves: top top :coat: :socks: top top :dress: :kimono: top top :sari: :one_piece_swimsuit: top top :swim_brief: :swim_brief: :shorts: top top :bikini: :womans_clothes: top top :purse: :handbag: top top :pouch: :shopping: :shopping: top top :school_satchel: :thong_sandal: top top :mans_shoe: :shoe: :athletic_shoe: top top :hiking_boot: :flat_shoe: :flat_shoe: top top :high_heel: :sandal: top top :ballet_shoes: :boot: top top :crown: :womans_hat: top top :tophat: :mortar_board: top top :billed_cap: :military_helmet: top top :rescue_worker_helmet: :rescue_worker_helmet: :prayer_beads: top top :lipstick: :ring: top top :gem: top Sound \u00b6 ico shortcode ico shortcode top :mute: :speaker: top top :sound: :loud_sound: top top :loudspeaker: :mega: top top :postal_horn: :bell: top top :no_bell: top Music \u00b6 ico shortcode ico shortcode top :musical_score: :musical_note: top top :notes: :studio_microphone: top top :level_slider: :control_knobs: top top :microphone: :headphones: top top :radio: top Musical Instrument \u00b6 ico shortcode ico shortcode top :saxophone: :accordion: top top :guitar: :musical_keyboard: top top :trumpet: :violin: top top :banjo: :drum: top top :long_drum: top Phone \u00b6 ico shortcode ico shortcode top :iphone: :iphone: :calling: top top :phone: :phone: :telephone: :telephone_receiver: top top :pager: :fax: top Computer \u00b6 ico shortcode ico shortcode top :battery: :electric_plug: top top :computer: :desktop_computer: top top :printer: :keyboard: top top :computer_mouse: :computer_mouse: :trackball: top top :minidisc: :floppy_disk: top top :cd: :dvd: top top :abacus: top Light & Video \u00b6 ico shortcode ico shortcode top :movie_camera: :film_strip: :film_strip: top top :film_projector: :clapper: top top :tv: :camera: top top :camera_flash: :camera_flash: :video_camera: top top :vhs: :mag: top top :mag_right: :candle: top top :bulb: :flashlight: top top :izakaya_lantern: :lantern: :diya_lamp: top Book Paper \u00b6 ico shortcode ico shortcode top :notebook_with_decorative_cover: :closed_book: top top :book: :open_book: :green_book: top top :blue_book: :orange_book: top top :books: :notebook: top top :ledger: :page_with_curl: top top :scroll: :page_facing_up: top top :newspaper: :newspaper_roll: :newspaper_roll: top top :bookmark_tabs: :bookmark: top top :label: top Money \u00b6 ico shortcode ico shortcode top :moneybag: :coin: top top :yen: :dollar: top top :euro: :pound: top top :money_with_wings: :credit_card: top top :receipt: :chart: top Mail \u00b6 ico shortcode ico shortcode top :envelope: :e-mail: :email: top top :incoming_envelope: :envelope_with_arrow: top top :outbox_tray: :inbox_tray: top top :package: :mailbox: top top :mailbox_closed: :mailbox_with_mail: top top :mailbox_with_no_mail: :postbox: top top :ballot_box: top Writing \u00b6 ico shortcode ico shortcode top :pencil2: :black_nib: top top :fountain_pen: :fountain_pen: :pen: :pen: top top :paintbrush: :crayon: top top :memo: :pencil: top Office \u00b6 ico shortcode ico shortcode top :briefcase: :file_folder: top top :open_file_folder: :card_index_dividers: top top :date: :calendar: top top :spiral_notepad: :spiral_notepad: :spiral_calendar: :spiral_calendar: top top :card_index: :chart_with_upwards_trend: top top :chart_with_downwards_trend: :bar_chart: top top :clipboard: :pushpin: top top :round_pushpin: :paperclip: top top :paperclips: :straight_ruler: top top :triangular_ruler: :scissors: top top :card_file_box: :file_cabinet: top top :wastebasket: top Lock \u00b6 ico shortcode ico shortcode top :lock: :unlock: top top :lock_with_ink_pen: :closed_lock_with_key: top top :key: :old_key: top Tool \u00b6 ico shortcode ico shortcode top :hammer: :axe: top top :pick: :hammer_and_pick: top top :hammer_and_wrench: :dagger: top top :crossed_swords: :gun: top top :boomerang: :bow_and_arrow: top top :shield: :carpentry_saw: top top :wrench: :screwdriver: top top :nut_and_bolt: :gear: top top :clamp: :clamp: :balance_scale: :balance_scale: top top :probing_cane: :link: top top :chains: :hook: top top :toolbox: :magnet: top top :ladder: top Science \u00b6 ico shortcode ico shortcode top :alembic: :test_tube: top top :petri_dish: :dna: top top :microscope: :telescope: top top :satellite: top Medical \u00b6 ico shortcode ico shortcode top :syringe: :drop_of_blood: top top :pill: :adhesive_bandage: top top :stethoscope: top Household \u00b6 ico shortcode ico shortcode top :door: :elevator: top top :mirror: :window: top top :bed: :couch_and_lamp: top top :chair: :toilet: top top :plunger: :shower: top top :bathtub: :mouse_trap: top top :razor: :lotion_bottle: :lotion_bottle: top top :safety_pin: :broom: top top :basket: :roll_of_paper: top top :bucket: :soap: top top :toothbrush: :sponge: top top :fire_extinguisher: :shopping_cart: top Other Object \u00b6 ico shortcode ico shortcode top :smoking: :coffin: top top :headstone: :funeral_urn: top top :moyai: :placard: top Symbols \u00b6 Transport Sign Warning Arrow Religion Zodiac Av Symbol Gender Math Punctuation Currency Other Symbol Keycap Alphanum Geometric Transport Sign \u00b6 ico shortcode ico shortcode top :atm: :put_litter_in_its_place: top top :potable_water: :wheelchair: top top :mens: :womens: top top :restroom: :baby_symbol: top top :wc: :passport_control: top top :customs: :baggage_claim: top top :left_luggage: top Warning \u00b6 ico shortcode ico shortcode top :warning: :children_crossing: top top :no_entry: :no_entry_sign: top top :no_bicycles: :no_smoking: top top :do_not_litter: :non-potable_water: top top :no_pedestrians: :no_mobile_phones: top top :underage: :radioactive: top top :biohazard: top Arrow \u00b6 ico shortcode ico shortcode top :arrow_up: :arrow_upper_right: top top :arrow_right: :arrow_lower_right: top top :arrow_down: :arrow_lower_left: top top :arrow_left: :arrow_upper_left: top top :arrow_up_down: :left_right_arrow: top top :leftwards_arrow_with_hook: :arrow_right_hook: top top :arrow_heading_up: :arrow_heading_down: top top :arrows_clockwise: :arrows_counterclockwise: top top :back: :end: top top :on: :soon: top top :top: top Religion \u00b6 ico shortcode ico shortcode top :place_of_worship: :atom_symbol: top top :om: :star_of_david: top top :wheel_of_dharma: :yin_yang: top top :latin_cross: :orthodox_cross: top top :star_and_crescent: :peace_symbol: top top :menorah: :six_pointed_star: top Zodiac \u00b6 ico shortcode ico shortcode top :aries: :taurus: top top :gemini: :cancer: top top :leo: :virgo: top top :libra: :scorpius: top top :sagittarius: :capricorn: top top :aquarius: :pisces: top top :ophiuchus: top Av Symbol \u00b6 ico shortcode ico shortcode top :twisted_rightwards_arrows: :repeat: top top :repeat_one: :arrow_forward: top top :fast_forward: :next_track_button: :next_track_button: top top :play_or_pause_button: :play_or_pause_button: :arrow_backward: top top :rewind: :previous_track_button: :previous_track_button: top top :arrow_up_small: :arrow_double_up: top top :arrow_down_small: :arrow_double_down: top top :pause_button: :stop_button: top top :record_button: :eject_button: :eject_button: top top :cinema: :low_brightness: top top :high_brightness: :signal_strength: top top :vibration_mode: :mobile_phone_off: top Gender \u00b6 ico shortcode ico shortcode top :female_sign: :male_sign: top top :transgender_symbol: top Math \u00b6 ico shortcode ico shortcode top :heavy_multiplication_x: :heavy_plus_sign: top top :heavy_minus_sign: :heavy_division_sign: top top :infinity: top Punctuation \u00b6 ico shortcode ico shortcode top :bangbang: :interrobang: top top :question: :grey_question: top top :grey_exclamation: :exclamation: :heavy_exclamation_mark: top top :wavy_dash: top Currency \u00b6 ico shortcode ico shortcode top :currency_exchange: :heavy_dollar_sign: top Other Symbol \u00b6 ico shortcode ico shortcode top :medical_symbol: :recycle: top top :fleur_de_lis: :fleur_de_lis: :trident: top top :name_badge: :beginner: top top :o: :white_check_mark: top top :ballot_box_with_check: :heavy_check_mark: top top :x: :negative_squared_cross_mark: top top :curly_loop: :loop: top top :part_alternation_mark: :eight_spoked_asterisk: top top :eight_pointed_black_star: :sparkle: top top :copyright: :registered: top top :tm: top Keycap \u00b6 ico shortcode ico shortcode top :hash: :asterisk: top top :zero: :one: top top :two: :three: top top :four: :five: top top :six: :seven: top top :eight: :nine: top top :keycap_ten: top Alphanum \u00b6 ico shortcode ico shortcode top :capital_abcd: :abcd: top top :1234: :symbols: top top :abc: :a: top top :ab: :b: top top :cl: :cool: top top :free: :information_source: top top :id: :m: top top :new: :ng: top top :o2: :ok: top top :parking: :sos: top top :up: :vs: top top :koko: :sa: top top :u6708: :u6709: top top :u6307: :ideograph_advantage: top top :u5272: :u7121: top top :u7981: :accept: top top :u7533: :u5408: top top :u7a7a: :congratulations: top top :secret: :u55b6: top top :u6e80: top Geometric \u00b6 ico shortcode ico shortcode top :red_circle: :orange_circle: top top :yellow_circle: :green_circle: top top :large_blue_circle: :large_blue_circle: :purple_circle: top top :brown_circle: :black_circle: top top :white_circle: :red_square: top top :orange_square: :yellow_square: top top :green_square: :blue_square: top top :purple_square: :brown_square: top top :black_large_square: :white_large_square: top top :black_medium_square: :white_medium_square: top top :black_medium_small_square: :white_medium_small_square: top top :black_small_square: :white_small_square: top top :large_orange_diamond: :large_blue_diamond: top top :small_orange_diamond: :small_blue_diamond: top top :small_red_triangle: :small_red_triangle_down: top top :diamond_shape_with_a_dot_inside: :radio_button: top top :white_square_button: :black_square_button: top Flags \u00b6 Flag Country Flag Subdivision Flag Flag \u00b6 ico shortcode ico shortcode top :checkered_flag: :triangular_flag_on_post: top top :crossed_flags: :black_flag: :black_flag: top top :white_flag: :white_flag: :rainbow_flag: top top :transgender_flag: :pirate_flag: top Country Flag \u00b6 ico shortcode ico shortcode top :ascension_island: :ascension_island: :andorra: :andorra: top top :united_arab_emirates: :united_arab_emirates: :afghanistan: :afghanistan: top top :antigua_barbuda: :antigua_barbuda: :anguilla: :anguilla: top top :albania: :albania: :armenia: :armenia: top top :angola: :angola: :antarctica: :antarctica: top top :argentina: :argentina: :american_samoa: :american_samoa: top top :austria: :austria: :australia: :australia: top top :aruba: :aruba: :aland_islands: :aland_islands: top top :azerbaijan: :azerbaijan: :bosnia_herzegovina: :bosnia_herzegovina: top top :barbados: :barbados: :bangladesh: :bangladesh: top top :belgium: :belgium: :burkina_faso: :burkina_faso: top top :bulgaria: :bulgaria: :bahrain: :bahrain: top top :burundi: :burundi: :benin: :benin: top top :st_barthelemy: :st_barthelemy: :bermuda: :bermuda: top top :brunei: :brunei: :bolivia: :bolivia: top top :caribbean_netherlands: :caribbean_netherlands: :brazil: :brazil: top top :bahamas: :bahamas: :bhutan: :bhutan: top top :bouvet_island: :bouvet_island: :botswana: :botswana: top top :belarus: :belarus: :belize: :belize: top top :canada: :canada: :cocos_islands: :cocos_islands: top top :congo_kinshasa: :congo_kinshasa: :central_african_republic: :central_african_republic: top top :congo_brazzaville: :congo_brazzaville: :switzerland: :switzerland: top top :cote_divoire: :cote_divoire: :cook_islands: :cook_islands: top top :chile: :cameroon: :cameroon: top top :cn: :colombia: :colombia: top top :clipperton_island: :clipperton_island: :costa_rica: :costa_rica: top top :cuba: :cuba: :cape_verde: :cape_verde: top top :curacao: :curacao: :christmas_island: :christmas_island: top top :cyprus: :cyprus: :czech_republic: :czech_republic: top top :de: :diego_garcia: :diego_garcia: top top :djibouti: :djibouti: :denmark: :denmark: top top :dominica: :dominica: :dominican_republic: :dominican_republic: top top :algeria: :algeria: :ceuta_melilla: :ceuta_melilla: top top :ecuador: :ecuador: :estonia: :estonia: top top :egypt: :egypt: :western_sahara: :western_sahara: top top :eritrea: :eritrea: :es: top top :ethiopia: :ethiopia: :eu: :european_union: top top :finland: :finland: :fiji: :fiji: top top :falkland_islands: :falkland_islands: :micronesia: :micronesia: top top :faroe_islands: :faroe_islands: :fr: top top :gabon: :gabon: :gb: :uk: top top :grenada: :grenada: :georgia: :georgia: top top :french_guiana: :french_guiana: :guernsey: :guernsey: top top :ghana: :ghana: :gibraltar: :gibraltar: top top :greenland: :greenland: :gambia: :gambia: top top :guinea: :guinea: :guadeloupe: :guadeloupe: top top :equatorial_guinea: :equatorial_guinea: :greece: :greece: top top :south_georgia_south_sandwich_islands: :south_georgia_south_sandwich_islands: :guatemala: :guatemala: top top :guam: :guam: :guinea_bissau: :guinea_bissau: top top :guyana: :guyana: :hong_kong: :hong_kong: top top :heard_mcdonald_islands: :heard_mcdonald_islands: :honduras: :honduras: top top :croatia: :croatia: :haiti: :haiti: top top :hungary: :hungary: :canary_islands: :canary_islands: top top :indonesia: :ireland: :ireland: top top :israel: :israel: :isle_of_man: :isle_of_man: top top :india: :india: :british_indian_ocean_territory: :british_indian_ocean_territory: top top :iraq: :iraq: :iran: :iran: top top :iceland: :iceland: :it: top top :jersey: :jersey: :jamaica: :jamaica: top top :jordan: :jordan: :jp: top top :kenya: :kenya: :kyrgyzstan: :kyrgyzstan: top top :cambodia: :cambodia: :kiribati: :kiribati: top top :comoros: :comoros: :st_kitts_nevis: :st_kitts_nevis: top top :north_korea: :north_korea: :kr: top top :kuwait: :kuwait: :cayman_islands: :cayman_islands: top top :kazakhstan: :kazakhstan: :laos: :laos: top top :lebanon: :lebanon: :st_lucia: :st_lucia: top top :liechtenstein: :liechtenstein: :sri_lanka: :sri_lanka: top top :liberia: :liberia: :lesotho: :lesotho: top top :lithuania: :lithuania: :luxembourg: :luxembourg: top top :latvia: :latvia: :libya: :libya: top top :morocco: :morocco: :monaco: :monaco: top top :moldova: :moldova: :montenegro: :montenegro: top top :st_martin: :st_martin: :madagascar: :madagascar: top top :marshall_islands: :marshall_islands: :macedonia: :macedonia: top top :mali: :mali: :myanmar: :myanmar: top top :mongolia: :mongolia: :macau: :macau: top top :northern_mariana_islands: :northern_mariana_islands: :martinique: :martinique: top top :mauritania: :mauritania: :montserrat: :montserrat: top top :malta: :malta: :mauritius: :mauritius: top top :maldives: :maldives: :malawi: :malawi: top top :mexico: :mexico: :malaysia: :malaysia: top top :mozambique: :mozambique: :namibia: :namibia: top top :new_caledonia: :new_caledonia: :niger: :niger: top top :norfolk_island: :norfolk_island: :nigeria: top top :nicaragua: :nicaragua: :netherlands: :netherlands: top top :norway: :norway: :nepal: :nepal: top top :nauru: :nauru: :niue: :niue: top top :new_zealand: :new_zealand: :oman: :oman: top top :panama: :panama: :peru: :peru: top top :french_polynesia: :french_polynesia: :papua_new_guinea: :papua_new_guinea: top top :philippines: :philippines: :pakistan: :pakistan: top top :poland: :poland: :st_pierre_miquelon: :st_pierre_miquelon: top top :pitcairn_islands: :pitcairn_islands: :puerto_rico: :puerto_rico: top top :palestinian_territories: :palestinian_territories: :portugal: :portugal: top top :palau: :palau: :paraguay: :paraguay: top top :qatar: :qatar: :reunion: :reunion: top top :romania: :romania: :serbia: :serbia: top top :ru: :rwanda: :rwanda: top top :saudi_arabia: :saudi_arabia: :solomon_islands: :solomon_islands: top top :seychelles: :seychelles: :sudan: :sudan: top top :sweden: :sweden: :singapore: :singapore: top top :st_helena: :st_helena: :slovenia: :slovenia: top top :svalbard_jan_mayen: :svalbard_jan_mayen: :slovakia: :slovakia: top top :sierra_leone: :sierra_leone: :san_marino: :san_marino: top top :senegal: :senegal: :somalia: :somalia: top top :suriname: :suriname: :south_sudan: :south_sudan: top top :sao_tome_principe: :sao_tome_principe: :el_salvador: :el_salvador: top top :sint_maarten: :sint_maarten: :syria: :syria: top top :swaziland: :swaziland: :tristan_da_cunha: :tristan_da_cunha: top top :turks_caicos_islands: :turks_caicos_islands: :chad: :chad: top top :french_southern_territories: :french_southern_territories: :togo: :togo: top top :thailand: :thailand: :tajikistan: :tajikistan: top top :tokelau: :tokelau: :timor_leste: :timor_leste: top top :turkmenistan: :tunisia: :tunisia: top top :tonga: :tonga: :tr: top top :trinidad_tobago: :trinidad_tobago: :tuvalu: top top :taiwan: :taiwan: :tanzania: :tanzania: top top :ukraine: :ukraine: :uganda: :uganda: top top :us_outlying_islands: :us_outlying_islands: :united_nations: top top :us: :uruguay: :uruguay: top top :uzbekistan: :uzbekistan: :vatican_city: :vatican_city: top top :st_vincent_grenadines: :st_vincent_grenadines: :venezuela: :venezuela: top top :british_virgin_islands: :british_virgin_islands: :us_virgin_islands: :us_virgin_islands: top top :vietnam: :vietnam: :vanuatu: :vanuatu: top top :wallis_futuna: :wallis_futuna: :samoa: :samoa: top top :kosovo: :kosovo: :yemen: :yemen: top top :mayotte: :mayotte: :south_africa: :south_africa: top top :zambia: :zambia: :zimbabwe: :zimbabwe: top Subdivision Flag \u00b6 ico shortcode ico shortcode top :england: :scotland: top top :wales: top GitHub Custom Emoji \u00b6 ico shortcode ico shortcode top :atom: :basecamp: :basecamp: top top :basecampy: :basecampy: :bowtie: :bowtie: top top :electron: :electron: :feelsgood: :feelsgood: top top :finnadie: :finnadie: :goberserk: :goberserk: top top :godmode: :godmode: :hurtrealbad: :hurtrealbad: top top :neckbeard: :neckbeard: :octocat: :octocat: top top :rage1: :rage1: :rage2: :rage2: top top :rage3: :rage3: :rage4: :rage4: top top :shipit: :shipit: :suspect: :suspect: top top :trollface: :trollface: top","title":"markdown emoji list"},{"location":"markdown/emoji_list/#emoji-cheat-sheet","text":"This cheat sheet is automatically generated from GitHub Emoji API and Unicode Full Emoji List .","title":"emoji-cheat-sheet"},{"location":"markdown/emoji_list/#table-of-contents","text":"Smileys & Emotion People & Body Animals & Nature Food & Drink Travel & Places Activities Objects Symbols Flags GitHub Custom Emoji","title":"Table of Contents"},{"location":"markdown/emoji_list/#smileys-emotion","text":"Face Smiling Face Affection Face Tongue Face Hand Face Neutral Skeptical Face Sleepy Face Unwell Face Hat Face Glasses Face Concerned Face Negative Face Costume Cat Face Monkey Face Emotion","title":"Smileys &amp; Emotion"},{"location":"markdown/emoji_list/#face-smiling","text":"ico shortcode ico shortcode top :grinning: :smiley: top top :smile: :grin: top top :laughing: :satisfied: :sweat_smile: top top :rofl: :joy: top top :slightly_smiling_face: :upside_down_face: top top :wink: :blush: top top :innocent: top","title":"Face Smiling"},{"location":"markdown/emoji_list/#face-affection","text":"ico shortcode ico shortcode top :smiling_face_with_three_hearts: :smiling_face_with_three_hearts: :heart_eyes: top top :star_struck: :kissing_heart: top top :kissing: :relaxed: top top :kissing_closed_eyes: :kissing_smiling_eyes: top top :smiling_face_with_tear: top","title":"Face Affection"},{"location":"markdown/emoji_list/#face-tongue","text":"ico shortcode ico shortcode top :yum: :stuck_out_tongue: top top :stuck_out_tongue_winking_eye: :zany_face: top top :stuck_out_tongue_closed_eyes: :money_mouth_face: top","title":"Face Tongue"},{"location":"markdown/emoji_list/#face-hand","text":"ico shortcode ico shortcode top :hugs: :hugs: :hand_over_mouth: :hand_over_mouth: top top :shushing_face: :thinking: top","title":"Face Hand"},{"location":"markdown/emoji_list/#face-neutral-skeptical","text":"ico shortcode ico shortcode top :zipper_mouth_face: :raised_eyebrow: :raised_eyebrow: top top :neutral_face: :expressionless: top top :no_mouth: :face_in_clouds: top top :smirk: :unamused: top top :roll_eyes: :roll_eyes: :grimacing: top top :face_exhaling: :lying_face: top","title":"Face Neutral Skeptical"},{"location":"markdown/emoji_list/#face-sleepy","text":"ico shortcode ico shortcode top :relieved: :pensive: top top :sleepy: :drooling_face: top top :sleeping: top","title":"Face Sleepy"},{"location":"markdown/emoji_list/#face-unwell","text":"ico shortcode ico shortcode top :mask: :face_with_thermometer: top top :face_with_head_bandage: :nauseated_face: top top :vomiting_face: :vomiting_face: :sneezing_face: top top :hot_face: :cold_face: top top :woozy_face: :dizzy_face: top top :face_with_spiral_eyes: :exploding_head: top","title":"Face Unwell"},{"location":"markdown/emoji_list/#face-hat","text":"ico shortcode ico shortcode top :cowboy_hat_face: :cowboy_hat_face: :partying_face: top top :disguised_face: top","title":"Face Hat"},{"location":"markdown/emoji_list/#face-glasses","text":"ico shortcode ico shortcode top :sunglasses: :nerd_face: top top :monocle_face: :monocle_face: top","title":"Face Glasses"},{"location":"markdown/emoji_list/#face-concerned","text":"ico shortcode ico shortcode top :confused: :worried: top top :slightly_frowning_face: :frowning_face: :frowning_face: top top :open_mouth: :hushed: top top :astonished: :flushed: top top :pleading_face: :frowning: top top :anguished: :fearful: top top :cold_sweat: :disappointed_relieved: top top :cry: :sob: top top :scream: :confounded: top top :persevere: :disappointed: top top :sweat: :weary: top top :tired_face: :yawning_face: top","title":"Face Concerned"},{"location":"markdown/emoji_list/#face-negative","text":"ico shortcode ico shortcode top :triumph: :pout: :pout: :rage: top top :angry: :cursing_face: :cursing_face: top top :smiling_imp: :imp: top top :skull: :skull_and_crossbones: top","title":"Face Negative"},{"location":"markdown/emoji_list/#face-costume","text":"ico shortcode ico shortcode top :hankey: :poop: :shit: :clown_face: top top :japanese_ogre: :japanese_goblin: top top :ghost: :alien: top top :space_invader: :robot: top","title":"Face Costume"},{"location":"markdown/emoji_list/#cat-face","text":"ico shortcode ico shortcode top :smiley_cat: :smile_cat: top top :joy_cat: :heart_eyes_cat: top top :smirk_cat: :kissing_cat: top top :scream_cat: :crying_cat_face: top top :pouting_cat: top","title":"Cat Face"},{"location":"markdown/emoji_list/#monkey-face","text":"ico shortcode ico shortcode top :see_no_evil: :hear_no_evil: top top :speak_no_evil: top","title":"Monkey Face"},{"location":"markdown/emoji_list/#emotion","text":"ico shortcode ico shortcode top :kiss: :love_letter: top top :cupid: :gift_heart: top top :sparkling_heart: :heartpulse: top top :heartbeat: :revolving_hearts: top top :two_hearts: :heart_decoration: top top :heavy_heart_exclamation: :heavy_heart_exclamation: :broken_heart: top top :heart_on_fire: :mending_heart: top top :heart: :orange_heart: top top :yellow_heart: :green_heart: top top :blue_heart: :purple_heart: top top :brown_heart: :black_heart: top top :white_heart: :100: top top :anger: :boom: :collision: top top :dizzy: :sweat_drops: top top :dash: :hole: top top :bomb: :speech_balloon: top top :eye_speech_bubble: :eye_speech_bubble: :left_speech_bubble: top top :right_anger_bubble: :thought_balloon: top top :zzz: top","title":"Emotion"},{"location":"markdown/emoji_list/#people-body","text":"Hand Fingers Open Hand Fingers Partial Hand Single Finger Hand Fingers Closed Hands Hand Prop Body Parts Person Person Gesture Person Role Person Fantasy Person Activity Person Sport Person Resting Family Person Symbol","title":"People &amp; Body"},{"location":"markdown/emoji_list/#hand-fingers-open","text":"ico shortcode ico shortcode top :wave: :raised_back_of_hand: top top :raised_hand_with_fingers_splayed: :hand: :hand: :raised_hand: top top :vulcan_salute: :vulcan_salute: top","title":"Hand Fingers Open"},{"location":"markdown/emoji_list/#hand-fingers-partial","text":"ico shortcode ico shortcode top :ok_hand: :pinched_fingers: top top :pinching_hand: :v: top top :crossed_fingers: :crossed_fingers: :love_you_gesture: top top :metal: :call_me_hand: top","title":"Hand Fingers Partial"},{"location":"markdown/emoji_list/#hand-single-finger","text":"ico shortcode ico shortcode top :point_left: :point_right: top top :point_up_2: :fu: :fu: :middle_finger: top top :point_down: :point_up: top","title":"Hand Single Finger"},{"location":"markdown/emoji_list/#hand-fingers-closed","text":"ico shortcode ico shortcode top :+1: :thumbsup: :-1: :thumbsdown: top top :fist: :fist_raised: :facepunch: :facepunch: :fist_oncoming: :punch: top top :fist_left: :fist_left: :fist_right: :fist_right: top","title":"Hand Fingers Closed"},{"location":"markdown/emoji_list/#hands","text":"ico shortcode ico shortcode top :clap: :raised_hands: top top :open_hands: :palms_up_together: top top :handshake: :pray: top","title":"Hands"},{"location":"markdown/emoji_list/#hand-prop","text":"ico shortcode ico shortcode top :writing_hand: :nail_care: top top :selfie: top","title":"Hand Prop"},{"location":"markdown/emoji_list/#body-parts","text":"ico shortcode ico shortcode top :muscle: :mechanical_arm: top top :mechanical_leg: :leg: top top :foot: :ear: top top :ear_with_hearing_aid: :nose: top top :brain: :anatomical_heart: top top :lungs: :tooth: top top :bone: :eyes: top top :eye: :tongue: top top :lips: top","title":"Body Parts"},{"location":"markdown/emoji_list/#person","text":"ico shortcode ico shortcode top :baby: :child: top top :boy: :girl: top top :adult: :blond_haired_person: top top :man: :bearded_person: top top :man_beard: :woman_beard: top top :red_haired_man: :red_haired_man: :curly_haired_man: :curly_haired_man: top top :white_haired_man: :white_haired_man: :bald_man: :bald_man: top top :woman: :red_haired_woman: :red_haired_woman: top top :person_red_hair: :curly_haired_woman: :curly_haired_woman: top top :person_curly_hair: :white_haired_woman: :white_haired_woman: top top :person_white_hair: :bald_woman: :bald_woman: top top :person_bald: :blond_haired_woman: :blond_haired_woman: :blonde_woman: top top :blond_haired_man: :blond_haired_man: :older_adult: top top :older_man: :older_woman: top","title":"Person"},{"location":"markdown/emoji_list/#person-gesture","text":"ico shortcode ico shortcode top :frowning_person: :frowning_person: :frowning_man: :frowning_man: top top :frowning_woman: :frowning_woman: :pouting_face: :pouting_face: top top :pouting_man: :pouting_man: :pouting_woman: :pouting_woman: top top :no_good: :ng_man: :ng_man: :no_good_man: top top :ng_woman: :ng_woman: :no_good_woman: :ok_person: :ok_person: top top :ok_man: :ok_man: :ok_woman: top top :information_desk_person: :tipping_hand_person: :sassy_man: :sassy_man: :tipping_hand_man: top top :sassy_woman: :sassy_woman: :tipping_hand_woman: :raising_hand: top top :raising_hand_man: :raising_hand_man: :raising_hand_woman: :raising_hand_woman: top top :deaf_person: :deaf_man: top top :deaf_woman: :bow: top top :bowing_man: :bowing_man: :bowing_woman: :bowing_woman: top top :facepalm: :man_facepalming: top top :woman_facepalming: :shrug: top top :man_shrugging: :woman_shrugging: top","title":"Person Gesture"},{"location":"markdown/emoji_list/#person-role","text":"ico shortcode ico shortcode top :health_worker: :man_health_worker: top top :woman_health_worker: :student: top top :man_student: :woman_student: top top :teacher: :man_teacher: top top :woman_teacher: :judge: top top :man_judge: :woman_judge: top top :farmer: :man_farmer: top top :woman_farmer: :cook: top top :man_cook: :woman_cook: top top :mechanic: :man_mechanic: top top :woman_mechanic: :factory_worker: top top :man_factory_worker: :woman_factory_worker: top top :office_worker: :man_office_worker: top top :woman_office_worker: :scientist: top top :man_scientist: :woman_scientist: top top :technologist: :man_technologist: top top :woman_technologist: :singer: top top :man_singer: :woman_singer: top top :artist: :man_artist: top top :woman_artist: :pilot: top top :man_pilot: :woman_pilot: top top :astronaut: :man_astronaut: top top :woman_astronaut: :firefighter: top top :man_firefighter: :woman_firefighter: top top :cop: :police_officer: :policeman: :policeman: top top :policewoman: :policewoman: :detective: top top :male_detective: :male_detective: :female_detective: :female_detective: top top :guard: :guardsman: top top :guardswoman: :guardswoman: :ninja: top top :construction_worker: :construction_worker_man: :construction_worker_man: top top :construction_worker_woman: :construction_worker_woman: :prince: top top :princess: :person_with_turban: :person_with_turban: top top :man_with_turban: :woman_with_turban: :woman_with_turban: top top :man_with_gua_pi_mao: :woman_with_headscarf: top top :person_in_tuxedo: :man_in_tuxedo: top top :woman_in_tuxedo: :person_with_veil: top top :man_with_veil: :bride_with_veil: :bride_with_veil: :woman_with_veil: top top :pregnant_woman: :breast_feeding: top top :woman_feeding_baby: :man_feeding_baby: top top :person_feeding_baby: top","title":"Person Role"},{"location":"markdown/emoji_list/#person-fantasy","text":"ico shortcode ico shortcode top :angel: :santa: top top :mrs_claus: :mx_claus: top top :superhero: :superhero_man: :superhero_man: top top :superhero_woman: :superhero_woman: :supervillain: top top :supervillain_man: :supervillain_man: :supervillain_woman: :supervillain_woman: top top :mage: :mage_man: :mage_man: top top :mage_woman: :mage_woman: :fairy: top top :fairy_man: :fairy_man: :fairy_woman: :fairy_woman: top top :vampire: :vampire_man: :vampire_man: top top :vampire_woman: :vampire_woman: :merperson: top top :merman: :mermaid: top top :elf: :elf_man: :elf_man: top top :elf_woman: :elf_woman: :genie: top top :genie_man: :genie_man: :genie_woman: :genie_woman: top top :zombie: :zombie_man: :zombie_man: top top :zombie_woman: :zombie_woman: top","title":"Person Fantasy"},{"location":"markdown/emoji_list/#person-activity","text":"ico shortcode ico shortcode top :massage: :massage_man: :massage_man: top top :massage_woman: :massage_woman: :haircut: top top :haircut_man: :haircut_man: :haircut_woman: :haircut_woman: top top :walking: :walking_man: :walking_man: top top :walking_woman: :walking_woman: :standing_person: :standing_person: top top :standing_man: :standing_man: :standing_woman: :standing_woman: top top :kneeling_person: :kneeling_person: :kneeling_man: :kneeling_man: top top :kneeling_woman: :kneeling_woman: :person_with_probing_cane: top top :man_with_probing_cane: :woman_with_probing_cane: top top :person_in_motorized_wheelchair: :man_in_motorized_wheelchair: top top :woman_in_motorized_wheelchair: :person_in_manual_wheelchair: top top :man_in_manual_wheelchair: :woman_in_manual_wheelchair: top top :runner: :running: :running_man: :running_man: top top :running_woman: :running_woman: :dancer: :woman_dancing: top top :man_dancing: :business_suit_levitating: :business_suit_levitating: top top :dancers: :dancing_men: :dancing_men: top top :dancing_women: :dancing_women: :sauna_person: :sauna_person: top top :sauna_man: :sauna_man: :sauna_woman: :sauna_woman: top top :climbing: :climbing: :climbing_man: :climbing_man: top top :climbing_woman: :climbing_woman: top","title":"Person Activity"},{"location":"markdown/emoji_list/#person-sport","text":"ico shortcode ico shortcode top :person_fencing: :horse_racing: top top :skier: :snowboarder: top top :golfing: :golfing: :golfing_man: :golfing_man: top top :golfing_woman: :golfing_woman: :surfer: top top :surfing_man: :surfing_man: :surfing_woman: :surfing_woman: top top :rowboat: :rowing_man: :rowing_man: top top :rowing_woman: :rowing_woman: :swimmer: top top :swimming_man: :swimming_man: :swimming_woman: :swimming_woman: top top :bouncing_ball_person: :bouncing_ball_person: :basketball_man: :basketball_man: :bouncing_ball_man: top top :basketball_woman: :basketball_woman: :bouncing_ball_woman: :weight_lifting: :weight_lifting: top top :weight_lifting_man: :weight_lifting_man: :weight_lifting_woman: :weight_lifting_woman: top top :bicyclist: :biking_man: :biking_man: top top :biking_woman: :biking_woman: :mountain_bicyclist: top top :mountain_biking_man: :mountain_biking_man: :mountain_biking_woman: :mountain_biking_woman: top top :cartwheeling: :cartwheeling: :man_cartwheeling: top top :woman_cartwheeling: :wrestling: top top :men_wrestling: :women_wrestling: top top :water_polo: :man_playing_water_polo: top top :woman_playing_water_polo: :handball_person: :handball_person: top top :man_playing_handball: :woman_playing_handball: top top :juggling_person: :juggling_person: :man_juggling: top top :woman_juggling: top","title":"Person Sport"},{"location":"markdown/emoji_list/#person-resting","text":"ico shortcode ico shortcode top :lotus_position: :lotus_position: :lotus_position_man: :lotus_position_man: top top :lotus_position_woman: :lotus_position_woman: :bath: top top :sleeping_bed: :sleeping_bed: top","title":"Person Resting"},{"location":"markdown/emoji_list/#family","text":"ico shortcode ico shortcode top :people_holding_hands: :two_women_holding_hands: top top :couple: :two_men_holding_hands: top top :couplekiss: :couplekiss_man_woman: :couplekiss_man_woman: top top :couplekiss_man_man: :couplekiss_man_man: :couplekiss_woman_woman: :couplekiss_woman_woman: top top :couple_with_heart: :couple_with_heart_woman_man: top top :couple_with_heart_man_man: :couple_with_heart_man_man: :couple_with_heart_woman_woman: :couple_with_heart_woman_woman: top top :family: :family_man_woman_boy: top top :family_man_woman_girl: :family_man_woman_girl: :family_man_woman_girl_boy: :family_man_woman_girl_boy: top top :family_man_woman_boy_boy: :family_man_woman_boy_boy: :family_man_woman_girl_girl: :family_man_woman_girl_girl: top top :family_man_man_boy: :family_man_man_boy: :family_man_man_girl: :family_man_man_girl: top top :family_man_man_girl_boy: :family_man_man_girl_boy: :family_man_man_boy_boy: :family_man_man_boy_boy: top top :family_man_man_girl_girl: :family_man_man_girl_girl: :family_woman_woman_boy: :family_woman_woman_boy: top top :family_woman_woman_girl: :family_woman_woman_girl: :family_woman_woman_girl_boy: :family_woman_woman_girl_boy: top top :family_woman_woman_boy_boy: :family_woman_woman_boy_boy: :family_woman_woman_girl_girl: :family_woman_woman_girl_girl: top top :family_man_boy: :family_man_boy_boy: top top :family_man_girl: :family_man_girl_boy: top top :family_man_girl_girl: :family_woman_boy: top top :family_woman_boy_boy: :family_woman_girl: top top :family_woman_girl_boy: :family_woman_girl_girl: top","title":"Family"},{"location":"markdown/emoji_list/#person-symbol","text":"ico shortcode ico shortcode top :speaking_head: :bust_in_silhouette: top top :busts_in_silhouette: :people_hugging: top top :footprints: top","title":"Person Symbol"},{"location":"markdown/emoji_list/#animals-nature","text":"Animal Mammal Animal Bird Animal Amphibian Animal Reptile Animal Marine Animal Bug Plant Flower Plant Other","title":"Animals &amp; Nature"},{"location":"markdown/emoji_list/#animal-mammal","text":"ico shortcode ico shortcode top :monkey_face: :monkey: top top :gorilla: :orangutan: top top :dog: :dog2: top top :guide_dog: :service_dog: top top :poodle: :wolf: top top :fox_face: :raccoon: top top :cat: :cat2: top top :black_cat: :lion: top top :tiger: :tiger2: top top :leopard: :horse: top top :racehorse: :unicorn: top top :zebra: :deer: top top :bison: :cow: top top :ox: :water_buffalo: top top :cow2: :pig: top top :pig2: :boar: top top :pig_nose: :ram: top top :sheep: :goat: top top :dromedary_camel: :camel: top top :llama: :giraffe: top top :elephant: :mammoth: top top :rhinoceros: :hippopotamus: top top :mouse: :mouse2: top top :rat: :hamster: top top :rabbit: :rabbit2: top top :chipmunk: :beaver: top top :hedgehog: :bat: top top :bear: :polar_bear: top top :koala: :panda_face: top top :sloth: :otter: top top :skunk: :kangaroo: top top :badger: :feet: :paw_prints: top","title":"Animal Mammal"},{"location":"markdown/emoji_list/#animal-bird","text":"ico shortcode ico shortcode top :turkey: :chicken: top top :rooster: :hatching_chick: top top :baby_chick: :hatched_chick: top top :bird: :penguin: top top :dove: :eagle: top top :duck: :swan: top top :owl: :dodo: top top :feather: :flamingo: top top :peacock: :parrot: top","title":"Animal Bird"},{"location":"markdown/emoji_list/#animal-amphibian","text":"ico shortcode top :frog: top","title":"Animal Amphibian"},{"location":"markdown/emoji_list/#animal-reptile","text":"ico shortcode ico shortcode top :crocodile: :turtle: top top :lizard: :snake: top top :dragon_face: :dragon: top top :sauropod: :t-rex: :t-rex: top","title":"Animal Reptile"},{"location":"markdown/emoji_list/#animal-marine","text":"ico shortcode ico shortcode top :whale: :whale2: top top :dolphin: :flipper: :seal: top top :fish: :tropical_fish: top top :blowfish: :shark: top top :octopus: :shell: top","title":"Animal Marine"},{"location":"markdown/emoji_list/#animal-bug","text":"ico shortcode ico shortcode top :snail: :butterfly: top top :bug: :ant: top top :bee: :honeybee: :beetle: top top :lady_beetle: :cricket: top top :cockroach: :spider: top top :spider_web: :scorpion: top top :mosquito: :fly: top top :worm: :microbe: top","title":"Animal Bug"},{"location":"markdown/emoji_list/#plant-flower","text":"ico shortcode ico shortcode top :bouquet: :cherry_blossom: top top :white_flower: :rosette: top top :rose: :wilted_flower: top top :hibiscus: :sunflower: top top :blossom: :tulip: top","title":"Plant Flower"},{"location":"markdown/emoji_list/#plant-other","text":"ico shortcode ico shortcode top :seedling: :potted_plant: top top :evergreen_tree: :deciduous_tree: top top :palm_tree: :cactus: top top :ear_of_rice: :herb: top top :shamrock: :four_leaf_clover: top top :maple_leaf: :fallen_leaf: top top :leaves: top","title":"Plant Other"},{"location":"markdown/emoji_list/#food-drink","text":"Food Fruit Food Vegetable Food Prepared Food Asian Food Marine Food Sweet Drink Dishware","title":"Food &amp; Drink"},{"location":"markdown/emoji_list/#food-fruit","text":"ico shortcode ico shortcode top :grapes: :melon: top top :watermelon: :mandarin: :mandarin: :orange: :tangerine: top top :lemon: :banana: top top :pineapple: :mango: top top :apple: :green_apple: top top :pear: :peach: top top :cherries: :strawberry: top top :blueberries: :kiwi_fruit: :kiwi_fruit: top top :tomato: :olive: top top :coconut: top","title":"Food Fruit"},{"location":"markdown/emoji_list/#food-vegetable","text":"ico shortcode ico shortcode top :avocado: :eggplant: top top :potato: :carrot: top top :corn: :hot_pepper: top top :bell_pepper: :cucumber: top top :leafy_green: :broccoli: top top :garlic: :onion: top top :mushroom: :peanuts: top top :chestnut: top","title":"Food Vegetable"},{"location":"markdown/emoji_list/#food-prepared","text":"ico shortcode ico shortcode top :bread: :croissant: top top :baguette_bread: :flatbread: top top :pretzel: :bagel: top top :pancakes: :waffle: top top :cheese: :meat_on_bone: top top :poultry_leg: :cut_of_meat: top top :bacon: :hamburger: top top :fries: :pizza: top top :hotdog: :sandwich: top top :taco: :burrito: top top :tamale: :stuffed_flatbread: top top :falafel: :egg: top top :fried_egg: :fried_egg: :shallow_pan_of_food: top top :stew: :fondue: top top :bowl_with_spoon: :green_salad: top top :popcorn: :butter: top top :salt: :canned_food: top","title":"Food Prepared"},{"location":"markdown/emoji_list/#food-asian","text":"ico shortcode ico shortcode top :bento: :rice_cracker: top top :rice_ball: :rice: top top :curry: :ramen: top top :spaghetti: :sweet_potato: top top :oden: :sushi: top top :fried_shrimp: :fish_cake: top top :moon_cake: :dango: top top :dumpling: :fortune_cookie: top top :takeout_box: top","title":"Food Asian"},{"location":"markdown/emoji_list/#food-marine","text":"ico shortcode ico shortcode top :crab: :lobster: top top :shrimp: :squid: top top :oyster: top","title":"Food Marine"},{"location":"markdown/emoji_list/#food-sweet","text":"ico shortcode ico shortcode top :icecream: :shaved_ice: top top :ice_cream: :doughnut: top top :cookie: :birthday: top top :cake: :cupcake: top top :pie: :chocolate_bar: top top :candy: :lollipop: top top :custard: :honey_pot: top","title":"Food Sweet"},{"location":"markdown/emoji_list/#drink","text":"ico shortcode ico shortcode top :baby_bottle: :milk_glass: :milk_glass: top top :coffee: :teapot: top top :tea: :sake: top top :champagne: :wine_glass: top top :cocktail: :tropical_drink: top top :beer: :beers: top top :clinking_glasses: :clinking_glasses: :tumbler_glass: top top :cup_with_straw: :bubble_tea: top top :beverage_box: :mate: top top :ice_cube: top","title":"Drink"},{"location":"markdown/emoji_list/#dishware","text":"ico shortcode ico shortcode top :chopsticks: :plate_with_cutlery: :plate_with_cutlery: top top :fork_and_knife: :spoon: top top :hocho: :hocho: :knife: :amphora: top","title":"Dishware"},{"location":"markdown/emoji_list/#travel-places","text":"Place Map Place Geographic Place Building Place Religious Place Other Transport Ground Transport Water Transport Air Hotel Time Sky & Weather","title":"Travel &amp; Places"},{"location":"markdown/emoji_list/#place-map","text":"ico shortcode ico shortcode top :earth_africa: :earth_americas: top top :earth_asia: :globe_with_meridians: top top :world_map: :japan: top top :compass: top","title":"Place Map"},{"location":"markdown/emoji_list/#place-geographic","text":"ico shortcode ico shortcode top :mountain_snow: :mountain: top top :volcano: :mount_fuji: top top :camping: :beach_umbrella: top top :desert: :desert_island: top top :national_park: top","title":"Place Geographic"},{"location":"markdown/emoji_list/#place-building","text":"ico shortcode ico shortcode top :stadium: :classical_building: top top :building_construction: :bricks: top top :rock: :wood: top top :hut: :houses: :houses: top top :derelict_house: :derelict_house: :house: top top :house_with_garden: :office: top top :post_office: :european_post_office: top top :hospital: :bank: top top :hotel: :love_hotel: top top :convenience_store: :school: top top :department_store: :factory: top top :japanese_castle: :european_castle: top top :wedding: :tokyo_tower: top top :statue_of_liberty: top","title":"Place Building"},{"location":"markdown/emoji_list/#place-religious","text":"ico shortcode ico shortcode top :church: :mosque: top top :hindu_temple: :synagogue: top top :shinto_shrine: :kaaba: top","title":"Place Religious"},{"location":"markdown/emoji_list/#place-other","text":"ico shortcode ico shortcode top :fountain: :tent: top top :foggy: :night_with_stars: top top :cityscape: :sunrise_over_mountains: top top :sunrise: :city_sunset: top top :city_sunrise: :bridge_at_night: top top :hotsprings: :carousel_horse: top top :ferris_wheel: :roller_coaster: top top :barber: :circus_tent: top","title":"Place Other"},{"location":"markdown/emoji_list/#transport-ground","text":"ico shortcode ico shortcode top :steam_locomotive: :railway_car: top top :bullettrain_side: :bullettrain_front: top top :train2: :metro: top top :light_rail: :station: top top :tram: :monorail: top top :mountain_railway: :train: top top :bus: :oncoming_bus: top top :trolleybus: :minibus: top top :ambulance: :fire_engine: top top :police_car: :oncoming_police_car: top top :taxi: :oncoming_taxi: top top :car: :car: :red_car: :oncoming_automobile: top top :blue_car: :pickup_truck: top top :truck: :articulated_lorry: top top :tractor: :racing_car: top top :motorcycle: :motor_scooter: top top :manual_wheelchair: :motorized_wheelchair: top top :auto_rickshaw: :bike: top top :kick_scooter: :kick_scooter: :skateboard: top top :roller_skate: :busstop: top top :motorway: :railway_track: top top :oil_drum: :fuelpump: top top :rotating_light: :traffic_light: top top :vertical_traffic_light: :stop_sign: top top :construction: top","title":"Transport Ground"},{"location":"markdown/emoji_list/#transport-water","text":"ico shortcode ico shortcode top :anchor: :boat: :boat: :sailboat: top top :canoe: :speedboat: top top :passenger_ship: :ferry: top top :motor_boat: :motor_boat: :ship: top","title":"Transport Water"},{"location":"markdown/emoji_list/#transport-air","text":"ico shortcode ico shortcode top :airplane: :small_airplane: top top :flight_departure: :flight_departure: :flight_arrival: :flight_arrival: top top :parachute: :seat: top top :helicopter: :suspension_railway: top top :mountain_cableway: :aerial_tramway: top top :artificial_satellite: :artificial_satellite: :rocket: top top :flying_saucer: top","title":"Transport Air"},{"location":"markdown/emoji_list/#hotel","text":"ico shortcode ico shortcode top :bellhop_bell: :luggage: top","title":"Hotel"},{"location":"markdown/emoji_list/#time","text":"ico shortcode ico shortcode top :hourglass: :hourglass_flowing_sand: top top :watch: :alarm_clock: top top :stopwatch: :timer_clock: top top :mantelpiece_clock: :mantelpiece_clock: :clock12: top top :clock1230: :clock1: top top :clock130: :clock2: top top :clock230: :clock3: top top :clock330: :clock4: top top :clock430: :clock5: top top :clock530: :clock6: top top :clock630: :clock7: top top :clock730: :clock8: top top :clock830: :clock9: top top :clock930: :clock10: top top :clock1030: :clock11: top top :clock1130: top","title":"Time"},{"location":"markdown/emoji_list/#sky-weather","text":"ico shortcode ico shortcode top :new_moon: :waxing_crescent_moon: top top :first_quarter_moon: :moon: :moon: :waxing_gibbous_moon: top top :full_moon: :waning_gibbous_moon: top top :last_quarter_moon: :waning_crescent_moon: top top :crescent_moon: :new_moon_with_face: top top :first_quarter_moon_with_face: :last_quarter_moon_with_face: top top :thermometer: :sunny: top top :full_moon_with_face: :sun_with_face: top top :ringed_planet: :star: top top :star2: :stars: top top :milky_way: :cloud: top top :partly_sunny: :cloud_with_lightning_and_rain: :cloud_with_lightning_and_rain: top top :sun_behind_small_cloud: :sun_behind_small_cloud: :sun_behind_large_cloud: :sun_behind_large_cloud: top top :sun_behind_rain_cloud: :sun_behind_rain_cloud: :cloud_with_rain: top top :cloud_with_snow: :cloud_with_lightning: top top :tornado: :tornado: :fog: top top :wind_face: :wind_face: :cyclone: top top :rainbow: :closed_umbrella: top top :open_umbrella: :open_umbrella: :umbrella: top top :parasol_on_ground: :parasol_on_ground: :zap: top top :snowflake: :snowman_with_snow: :snowman_with_snow: top top :snowman: :comet: top top :fire: :droplet: top top :ocean: top","title":"Sky &amp; Weather"},{"location":"markdown/emoji_list/#activities","text":"Event Award Medal Sport Game Arts & Crafts","title":"Activities"},{"location":"markdown/emoji_list/#event","text":"ico shortcode ico shortcode top :jack_o_lantern: :christmas_tree: top top :fireworks: :sparkler: top top :firecracker: :sparkles: top top :balloon: :tada: top top :confetti_ball: :tanabata_tree: top top :bamboo: :dolls: top top :flags: :wind_chime: top top :rice_scene: :red_envelope: top top :ribbon: :gift: top top :reminder_ribbon: :tickets: top top :ticket: top","title":"Event"},{"location":"markdown/emoji_list/#award-medal","text":"ico shortcode ico shortcode top :medal_military: :medal_military: :trophy: top top :medal_sports: :medal_sports: :1st_place_medal: :1st_place_medal: top top :2nd_place_medal: :2nd_place_medal: :3rd_place_medal: :3rd_place_medal: top","title":"Award Medal"},{"location":"markdown/emoji_list/#sport","text":"ico shortcode ico shortcode top :soccer: :baseball: top top :softball: :basketball: top top :volleyball: :football: top top :rugby_football: :tennis: top top :flying_disc: :bowling: top top :cricket_game: :field_hockey: top top :ice_hockey: :ice_hockey: :lacrosse: top top :ping_pong: :badminton: top top :boxing_glove: :martial_arts_uniform: top top :goal_net: :golf: top top :ice_skate: :fishing_pole_and_fish: top top :diving_mask: :running_shirt_with_sash: top top :ski: :sled: top top :curling_stone: top","title":"Sport"},{"location":"markdown/emoji_list/#game","text":"ico shortcode ico shortcode top :dart: :yo_yo: top top :kite: :8ball: top top :crystal_ball: :magic_wand: top top :nazar_amulet: :video_game: top top :joystick: :slot_machine: top top :game_die: :jigsaw: top top :teddy_bear: :pinata: :pinata: top top :nesting_dolls: :spades: top top :hearts: :diamonds: top top :clubs: :chess_pawn: top top :black_joker: :mahjong: top top :flower_playing_cards: top","title":"Game"},{"location":"markdown/emoji_list/#arts-crafts","text":"ico shortcode ico shortcode top :performing_arts: :framed_picture: :framed_picture: top top :art: :thread: top top :sewing_needle: :yarn: top top :knot: top","title":"Arts &amp; Crafts"},{"location":"markdown/emoji_list/#objects","text":"Clothing Sound Music Musical Instrument Phone Computer Light & Video Book Paper Money Mail Writing Office Lock Tool Science Medical Household Other Object","title":"Objects"},{"location":"markdown/emoji_list/#clothing","text":"ico shortcode ico shortcode top :eyeglasses: :dark_sunglasses: top top :goggles: :lab_coat: top top :safety_vest: :necktie: top top :shirt: :tshirt: :jeans: top top :scarf: :gloves: top top :coat: :socks: top top :dress: :kimono: top top :sari: :one_piece_swimsuit: top top :swim_brief: :swim_brief: :shorts: top top :bikini: :womans_clothes: top top :purse: :handbag: top top :pouch: :shopping: :shopping: top top :school_satchel: :thong_sandal: top top :mans_shoe: :shoe: :athletic_shoe: top top :hiking_boot: :flat_shoe: :flat_shoe: top top :high_heel: :sandal: top top :ballet_shoes: :boot: top top :crown: :womans_hat: top top :tophat: :mortar_board: top top :billed_cap: :military_helmet: top top :rescue_worker_helmet: :rescue_worker_helmet: :prayer_beads: top top :lipstick: :ring: top top :gem: top","title":"Clothing"},{"location":"markdown/emoji_list/#sound","text":"ico shortcode ico shortcode top :mute: :speaker: top top :sound: :loud_sound: top top :loudspeaker: :mega: top top :postal_horn: :bell: top top :no_bell: top","title":"Sound"},{"location":"markdown/emoji_list/#music","text":"ico shortcode ico shortcode top :musical_score: :musical_note: top top :notes: :studio_microphone: top top :level_slider: :control_knobs: top top :microphone: :headphones: top top :radio: top","title":"Music"},{"location":"markdown/emoji_list/#musical-instrument","text":"ico shortcode ico shortcode top :saxophone: :accordion: top top :guitar: :musical_keyboard: top top :trumpet: :violin: top top :banjo: :drum: top top :long_drum: top","title":"Musical Instrument"},{"location":"markdown/emoji_list/#phone","text":"ico shortcode ico shortcode top :iphone: :iphone: :calling: top top :phone: :phone: :telephone: :telephone_receiver: top top :pager: :fax: top","title":"Phone"},{"location":"markdown/emoji_list/#computer","text":"ico shortcode ico shortcode top :battery: :electric_plug: top top :computer: :desktop_computer: top top :printer: :keyboard: top top :computer_mouse: :computer_mouse: :trackball: top top :minidisc: :floppy_disk: top top :cd: :dvd: top top :abacus: top","title":"Computer"},{"location":"markdown/emoji_list/#light-video","text":"ico shortcode ico shortcode top :movie_camera: :film_strip: :film_strip: top top :film_projector: :clapper: top top :tv: :camera: top top :camera_flash: :camera_flash: :video_camera: top top :vhs: :mag: top top :mag_right: :candle: top top :bulb: :flashlight: top top :izakaya_lantern: :lantern: :diya_lamp: top","title":"Light &amp; Video"},{"location":"markdown/emoji_list/#book-paper","text":"ico shortcode ico shortcode top :notebook_with_decorative_cover: :closed_book: top top :book: :open_book: :green_book: top top :blue_book: :orange_book: top top :books: :notebook: top top :ledger: :page_with_curl: top top :scroll: :page_facing_up: top top :newspaper: :newspaper_roll: :newspaper_roll: top top :bookmark_tabs: :bookmark: top top :label: top","title":"Book Paper"},{"location":"markdown/emoji_list/#money","text":"ico shortcode ico shortcode top :moneybag: :coin: top top :yen: :dollar: top top :euro: :pound: top top :money_with_wings: :credit_card: top top :receipt: :chart: top","title":"Money"},{"location":"markdown/emoji_list/#mail","text":"ico shortcode ico shortcode top :envelope: :e-mail: :email: top top :incoming_envelope: :envelope_with_arrow: top top :outbox_tray: :inbox_tray: top top :package: :mailbox: top top :mailbox_closed: :mailbox_with_mail: top top :mailbox_with_no_mail: :postbox: top top :ballot_box: top","title":"Mail"},{"location":"markdown/emoji_list/#writing","text":"ico shortcode ico shortcode top :pencil2: :black_nib: top top :fountain_pen: :fountain_pen: :pen: :pen: top top :paintbrush: :crayon: top top :memo: :pencil: top","title":"Writing"},{"location":"markdown/emoji_list/#office","text":"ico shortcode ico shortcode top :briefcase: :file_folder: top top :open_file_folder: :card_index_dividers: top top :date: :calendar: top top :spiral_notepad: :spiral_notepad: :spiral_calendar: :spiral_calendar: top top :card_index: :chart_with_upwards_trend: top top :chart_with_downwards_trend: :bar_chart: top top :clipboard: :pushpin: top top :round_pushpin: :paperclip: top top :paperclips: :straight_ruler: top top :triangular_ruler: :scissors: top top :card_file_box: :file_cabinet: top top :wastebasket: top","title":"Office"},{"location":"markdown/emoji_list/#lock","text":"ico shortcode ico shortcode top :lock: :unlock: top top :lock_with_ink_pen: :closed_lock_with_key: top top :key: :old_key: top","title":"Lock"},{"location":"markdown/emoji_list/#tool","text":"ico shortcode ico shortcode top :hammer: :axe: top top :pick: :hammer_and_pick: top top :hammer_and_wrench: :dagger: top top :crossed_swords: :gun: top top :boomerang: :bow_and_arrow: top top :shield: :carpentry_saw: top top :wrench: :screwdriver: top top :nut_and_bolt: :gear: top top :clamp: :clamp: :balance_scale: :balance_scale: top top :probing_cane: :link: top top :chains: :hook: top top :toolbox: :magnet: top top :ladder: top","title":"Tool"},{"location":"markdown/emoji_list/#science","text":"ico shortcode ico shortcode top :alembic: :test_tube: top top :petri_dish: :dna: top top :microscope: :telescope: top top :satellite: top","title":"Science"},{"location":"markdown/emoji_list/#medical","text":"ico shortcode ico shortcode top :syringe: :drop_of_blood: top top :pill: :adhesive_bandage: top top :stethoscope: top","title":"Medical"},{"location":"markdown/emoji_list/#household","text":"ico shortcode ico shortcode top :door: :elevator: top top :mirror: :window: top top :bed: :couch_and_lamp: top top :chair: :toilet: top top :plunger: :shower: top top :bathtub: :mouse_trap: top top :razor: :lotion_bottle: :lotion_bottle: top top :safety_pin: :broom: top top :basket: :roll_of_paper: top top :bucket: :soap: top top :toothbrush: :sponge: top top :fire_extinguisher: :shopping_cart: top","title":"Household"},{"location":"markdown/emoji_list/#other-object","text":"ico shortcode ico shortcode top :smoking: :coffin: top top :headstone: :funeral_urn: top top :moyai: :placard: top","title":"Other Object"},{"location":"markdown/emoji_list/#symbols","text":"Transport Sign Warning Arrow Religion Zodiac Av Symbol Gender Math Punctuation Currency Other Symbol Keycap Alphanum Geometric","title":"Symbols"},{"location":"markdown/emoji_list/#transport-sign","text":"ico shortcode ico shortcode top :atm: :put_litter_in_its_place: top top :potable_water: :wheelchair: top top :mens: :womens: top top :restroom: :baby_symbol: top top :wc: :passport_control: top top :customs: :baggage_claim: top top :left_luggage: top","title":"Transport Sign"},{"location":"markdown/emoji_list/#warning","text":"ico shortcode ico shortcode top :warning: :children_crossing: top top :no_entry: :no_entry_sign: top top :no_bicycles: :no_smoking: top top :do_not_litter: :non-potable_water: top top :no_pedestrians: :no_mobile_phones: top top :underage: :radioactive: top top :biohazard: top","title":"Warning"},{"location":"markdown/emoji_list/#arrow","text":"ico shortcode ico shortcode top :arrow_up: :arrow_upper_right: top top :arrow_right: :arrow_lower_right: top top :arrow_down: :arrow_lower_left: top top :arrow_left: :arrow_upper_left: top top :arrow_up_down: :left_right_arrow: top top :leftwards_arrow_with_hook: :arrow_right_hook: top top :arrow_heading_up: :arrow_heading_down: top top :arrows_clockwise: :arrows_counterclockwise: top top :back: :end: top top :on: :soon: top top :top: top","title":"Arrow"},{"location":"markdown/emoji_list/#religion","text":"ico shortcode ico shortcode top :place_of_worship: :atom_symbol: top top :om: :star_of_david: top top :wheel_of_dharma: :yin_yang: top top :latin_cross: :orthodox_cross: top top :star_and_crescent: :peace_symbol: top top :menorah: :six_pointed_star: top","title":"Religion"},{"location":"markdown/emoji_list/#zodiac","text":"ico shortcode ico shortcode top :aries: :taurus: top top :gemini: :cancer: top top :leo: :virgo: top top :libra: :scorpius: top top :sagittarius: :capricorn: top top :aquarius: :pisces: top top :ophiuchus: top","title":"Zodiac"},{"location":"markdown/emoji_list/#av-symbol","text":"ico shortcode ico shortcode top :twisted_rightwards_arrows: :repeat: top top :repeat_one: :arrow_forward: top top :fast_forward: :next_track_button: :next_track_button: top top :play_or_pause_button: :play_or_pause_button: :arrow_backward: top top :rewind: :previous_track_button: :previous_track_button: top top :arrow_up_small: :arrow_double_up: top top :arrow_down_small: :arrow_double_down: top top :pause_button: :stop_button: top top :record_button: :eject_button: :eject_button: top top :cinema: :low_brightness: top top :high_brightness: :signal_strength: top top :vibration_mode: :mobile_phone_off: top","title":"Av Symbol"},{"location":"markdown/emoji_list/#gender","text":"ico shortcode ico shortcode top :female_sign: :male_sign: top top :transgender_symbol: top","title":"Gender"},{"location":"markdown/emoji_list/#math","text":"ico shortcode ico shortcode top :heavy_multiplication_x: :heavy_plus_sign: top top :heavy_minus_sign: :heavy_division_sign: top top :infinity: top","title":"Math"},{"location":"markdown/emoji_list/#punctuation","text":"ico shortcode ico shortcode top :bangbang: :interrobang: top top :question: :grey_question: top top :grey_exclamation: :exclamation: :heavy_exclamation_mark: top top :wavy_dash: top","title":"Punctuation"},{"location":"markdown/emoji_list/#currency","text":"ico shortcode ico shortcode top :currency_exchange: :heavy_dollar_sign: top","title":"Currency"},{"location":"markdown/emoji_list/#other-symbol","text":"ico shortcode ico shortcode top :medical_symbol: :recycle: top top :fleur_de_lis: :fleur_de_lis: :trident: top top :name_badge: :beginner: top top :o: :white_check_mark: top top :ballot_box_with_check: :heavy_check_mark: top top :x: :negative_squared_cross_mark: top top :curly_loop: :loop: top top :part_alternation_mark: :eight_spoked_asterisk: top top :eight_pointed_black_star: :sparkle: top top :copyright: :registered: top top :tm: top","title":"Other Symbol"},{"location":"markdown/emoji_list/#keycap","text":"ico shortcode ico shortcode top :hash: :asterisk: top top :zero: :one: top top :two: :three: top top :four: :five: top top :six: :seven: top top :eight: :nine: top top :keycap_ten: top","title":"Keycap"},{"location":"markdown/emoji_list/#alphanum","text":"ico shortcode ico shortcode top :capital_abcd: :abcd: top top :1234: :symbols: top top :abc: :a: top top :ab: :b: top top :cl: :cool: top top :free: :information_source: top top :id: :m: top top :new: :ng: top top :o2: :ok: top top :parking: :sos: top top :up: :vs: top top :koko: :sa: top top :u6708: :u6709: top top :u6307: :ideograph_advantage: top top :u5272: :u7121: top top :u7981: :accept: top top :u7533: :u5408: top top :u7a7a: :congratulations: top top :secret: :u55b6: top top :u6e80: top","title":"Alphanum"},{"location":"markdown/emoji_list/#geometric","text":"ico shortcode ico shortcode top :red_circle: :orange_circle: top top :yellow_circle: :green_circle: top top :large_blue_circle: :large_blue_circle: :purple_circle: top top :brown_circle: :black_circle: top top :white_circle: :red_square: top top :orange_square: :yellow_square: top top :green_square: :blue_square: top top :purple_square: :brown_square: top top :black_large_square: :white_large_square: top top :black_medium_square: :white_medium_square: top top :black_medium_small_square: :white_medium_small_square: top top :black_small_square: :white_small_square: top top :large_orange_diamond: :large_blue_diamond: top top :small_orange_diamond: :small_blue_diamond: top top :small_red_triangle: :small_red_triangle_down: top top :diamond_shape_with_a_dot_inside: :radio_button: top top :white_square_button: :black_square_button: top","title":"Geometric"},{"location":"markdown/emoji_list/#flags","text":"Flag Country Flag Subdivision Flag","title":"Flags"},{"location":"markdown/emoji_list/#flag","text":"ico shortcode ico shortcode top :checkered_flag: :triangular_flag_on_post: top top :crossed_flags: :black_flag: :black_flag: top top :white_flag: :white_flag: :rainbow_flag: top top :transgender_flag: :pirate_flag: top","title":"Flag"},{"location":"markdown/emoji_list/#country-flag","text":"ico shortcode ico shortcode top :ascension_island: :ascension_island: :andorra: :andorra: top top :united_arab_emirates: :united_arab_emirates: :afghanistan: :afghanistan: top top :antigua_barbuda: :antigua_barbuda: :anguilla: :anguilla: top top :albania: :albania: :armenia: :armenia: top top :angola: :angola: :antarctica: :antarctica: top top :argentina: :argentina: :american_samoa: :american_samoa: top top :austria: :austria: :australia: :australia: top top :aruba: :aruba: :aland_islands: :aland_islands: top top :azerbaijan: :azerbaijan: :bosnia_herzegovina: :bosnia_herzegovina: top top :barbados: :barbados: :bangladesh: :bangladesh: top top :belgium: :belgium: :burkina_faso: :burkina_faso: top top :bulgaria: :bulgaria: :bahrain: :bahrain: top top :burundi: :burundi: :benin: :benin: top top :st_barthelemy: :st_barthelemy: :bermuda: :bermuda: top top :brunei: :brunei: :bolivia: :bolivia: top top :caribbean_netherlands: :caribbean_netherlands: :brazil: :brazil: top top :bahamas: :bahamas: :bhutan: :bhutan: top top :bouvet_island: :bouvet_island: :botswana: :botswana: top top :belarus: :belarus: :belize: :belize: top top :canada: :canada: :cocos_islands: :cocos_islands: top top :congo_kinshasa: :congo_kinshasa: :central_african_republic: :central_african_republic: top top :congo_brazzaville: :congo_brazzaville: :switzerland: :switzerland: top top :cote_divoire: :cote_divoire: :cook_islands: :cook_islands: top top :chile: :cameroon: :cameroon: top top :cn: :colombia: :colombia: top top :clipperton_island: :clipperton_island: :costa_rica: :costa_rica: top top :cuba: :cuba: :cape_verde: :cape_verde: top top :curacao: :curacao: :christmas_island: :christmas_island: top top :cyprus: :cyprus: :czech_republic: :czech_republic: top top :de: :diego_garcia: :diego_garcia: top top :djibouti: :djibouti: :denmark: :denmark: top top :dominica: :dominica: :dominican_republic: :dominican_republic: top top :algeria: :algeria: :ceuta_melilla: :ceuta_melilla: top top :ecuador: :ecuador: :estonia: :estonia: top top :egypt: :egypt: :western_sahara: :western_sahara: top top :eritrea: :eritrea: :es: top top :ethiopia: :ethiopia: :eu: :european_union: top top :finland: :finland: :fiji: :fiji: top top :falkland_islands: :falkland_islands: :micronesia: :micronesia: top top :faroe_islands: :faroe_islands: :fr: top top :gabon: :gabon: :gb: :uk: top top :grenada: :grenada: :georgia: :georgia: top top :french_guiana: :french_guiana: :guernsey: :guernsey: top top :ghana: :ghana: :gibraltar: :gibraltar: top top :greenland: :greenland: :gambia: :gambia: top top :guinea: :guinea: :guadeloupe: :guadeloupe: top top :equatorial_guinea: :equatorial_guinea: :greece: :greece: top top :south_georgia_south_sandwich_islands: :south_georgia_south_sandwich_islands: :guatemala: :guatemala: top top :guam: :guam: :guinea_bissau: :guinea_bissau: top top :guyana: :guyana: :hong_kong: :hong_kong: top top :heard_mcdonald_islands: :heard_mcdonald_islands: :honduras: :honduras: top top :croatia: :croatia: :haiti: :haiti: top top :hungary: :hungary: :canary_islands: :canary_islands: top top :indonesia: :ireland: :ireland: top top :israel: :israel: :isle_of_man: :isle_of_man: top top :india: :india: :british_indian_ocean_territory: :british_indian_ocean_territory: top top :iraq: :iraq: :iran: :iran: top top :iceland: :iceland: :it: top top :jersey: :jersey: :jamaica: :jamaica: top top :jordan: :jordan: :jp: top top :kenya: :kenya: :kyrgyzstan: :kyrgyzstan: top top :cambodia: :cambodia: :kiribati: :kiribati: top top :comoros: :comoros: :st_kitts_nevis: :st_kitts_nevis: top top :north_korea: :north_korea: :kr: top top :kuwait: :kuwait: :cayman_islands: :cayman_islands: top top :kazakhstan: :kazakhstan: :laos: :laos: top top :lebanon: :lebanon: :st_lucia: :st_lucia: top top :liechtenstein: :liechtenstein: :sri_lanka: :sri_lanka: top top :liberia: :liberia: :lesotho: :lesotho: top top :lithuania: :lithuania: :luxembourg: :luxembourg: top top :latvia: :latvia: :libya: :libya: top top :morocco: :morocco: :monaco: :monaco: top top :moldova: :moldova: :montenegro: :montenegro: top top :st_martin: :st_martin: :madagascar: :madagascar: top top :marshall_islands: :marshall_islands: :macedonia: :macedonia: top top :mali: :mali: :myanmar: :myanmar: top top :mongolia: :mongolia: :macau: :macau: top top :northern_mariana_islands: :northern_mariana_islands: :martinique: :martinique: top top :mauritania: :mauritania: :montserrat: :montserrat: top top :malta: :malta: :mauritius: :mauritius: top top :maldives: :maldives: :malawi: :malawi: top top :mexico: :mexico: :malaysia: :malaysia: top top :mozambique: :mozambique: :namibia: :namibia: top top :new_caledonia: :new_caledonia: :niger: :niger: top top :norfolk_island: :norfolk_island: :nigeria: top top :nicaragua: :nicaragua: :netherlands: :netherlands: top top :norway: :norway: :nepal: :nepal: top top :nauru: :nauru: :niue: :niue: top top :new_zealand: :new_zealand: :oman: :oman: top top :panama: :panama: :peru: :peru: top top :french_polynesia: :french_polynesia: :papua_new_guinea: :papua_new_guinea: top top :philippines: :philippines: :pakistan: :pakistan: top top :poland: :poland: :st_pierre_miquelon: :st_pierre_miquelon: top top :pitcairn_islands: :pitcairn_islands: :puerto_rico: :puerto_rico: top top :palestinian_territories: :palestinian_territories: :portugal: :portugal: top top :palau: :palau: :paraguay: :paraguay: top top :qatar: :qatar: :reunion: :reunion: top top :romania: :romania: :serbia: :serbia: top top :ru: :rwanda: :rwanda: top top :saudi_arabia: :saudi_arabia: :solomon_islands: :solomon_islands: top top :seychelles: :seychelles: :sudan: :sudan: top top :sweden: :sweden: :singapore: :singapore: top top :st_helena: :st_helena: :slovenia: :slovenia: top top :svalbard_jan_mayen: :svalbard_jan_mayen: :slovakia: :slovakia: top top :sierra_leone: :sierra_leone: :san_marino: :san_marino: top top :senegal: :senegal: :somalia: :somalia: top top :suriname: :suriname: :south_sudan: :south_sudan: top top :sao_tome_principe: :sao_tome_principe: :el_salvador: :el_salvador: top top :sint_maarten: :sint_maarten: :syria: :syria: top top :swaziland: :swaziland: :tristan_da_cunha: :tristan_da_cunha: top top :turks_caicos_islands: :turks_caicos_islands: :chad: :chad: top top :french_southern_territories: :french_southern_territories: :togo: :togo: top top :thailand: :thailand: :tajikistan: :tajikistan: top top :tokelau: :tokelau: :timor_leste: :timor_leste: top top :turkmenistan: :tunisia: :tunisia: top top :tonga: :tonga: :tr: top top :trinidad_tobago: :trinidad_tobago: :tuvalu: top top :taiwan: :taiwan: :tanzania: :tanzania: top top :ukraine: :ukraine: :uganda: :uganda: top top :us_outlying_islands: :us_outlying_islands: :united_nations: top top :us: :uruguay: :uruguay: top top :uzbekistan: :uzbekistan: :vatican_city: :vatican_city: top top :st_vincent_grenadines: :st_vincent_grenadines: :venezuela: :venezuela: top top :british_virgin_islands: :british_virgin_islands: :us_virgin_islands: :us_virgin_islands: top top :vietnam: :vietnam: :vanuatu: :vanuatu: top top :wallis_futuna: :wallis_futuna: :samoa: :samoa: top top :kosovo: :kosovo: :yemen: :yemen: top top :mayotte: :mayotte: :south_africa: :south_africa: top top :zambia: :zambia: :zimbabwe: :zimbabwe: top","title":"Country Flag"},{"location":"markdown/emoji_list/#subdivision-flag","text":"ico shortcode ico shortcode top :england: :scotland: top top :wales: top","title":"Subdivision Flag"},{"location":"markdown/emoji_list/#github-custom-emoji","text":"ico shortcode ico shortcode top :atom: :basecamp: :basecamp: top top :basecampy: :basecampy: :bowtie: :bowtie: top top :electron: :electron: :feelsgood: :feelsgood: top top :finnadie: :finnadie: :goberserk: :goberserk: top top :godmode: :godmode: :hurtrealbad: :hurtrealbad: top top :neckbeard: :neckbeard: :octocat: :octocat: top top :rage1: :rage1: :rage2: :rage2: top top :rage3: :rage3: :rage4: :rage4: top top :shipit: :shipit: :suspect: :suspect: top top :trollface: :trollface: top","title":"GitHub Custom Emoji"},{"location":"markdown/markdown/","text":"Markdown Cheatsheet \u00b6 Add images \u00b6 The following HTML is also legal Markdown: < img src = \"markdownmonstericon.png\" alt = \"Markdown Monster icon\" style = \"float: left; margin-right: 10px;\" />","title":"markdown images"},{"location":"markdown/markdown/#markdown-cheatsheet","text":"","title":"Markdown Cheatsheet"},{"location":"markdown/markdown/#add-images","text":"The following HTML is also legal Markdown: < img src = \"markdownmonstericon.png\" alt = \"Markdown Monster icon\" style = \"float: left; margin-right: 10px;\" />","title":"Add images"},{"location":"markdown/mermaid_graphs/","text":"Mermaid \u00b6 Graph \u00b6 graph and flowchart are interchangeable mermaid flowcharts graph TD; A-->B; A-->C; B-->D; C-->D; Use HTML with mermaid \u00b6 graph TD; A-->B; A-->C; B-->D; C-->D; Nested subgraph \u00b6 graph TD A[Christmas] -->|Get money| B(Go shopping) subgraph Nerve wracking B --> C{Roll a dice} end subgraph Don't look C -->|One| D[Laptop] C -->|Two| E[iPhone] subgraph High Probability C-->|Three| G C-->|Four| G C-->|Five| G[A gift card] end subgraph Low Probability C -->|Six| F[fa:fa-car Car] end end MLOps Architecture \u00b6 graph LR A[GitHub] --> B(CICD <br> GitHub Actions) B --> C{Roll a dice} subgraph ML Prediction C1 --> C2 C1 --> C3(Flask) end subgraph Don't look C -->|Two| E[Flask] subgraph High Probability C-->|Three| G C-->|Four| G C-->|Five| G[A gift card] end subgraph Low Probability C -->|Seven| H[\"\ud83c\udf49\"] end end flowchart TB c1-->a2 subgraph one a1-->a2 end subgraph two b1-->b2 end subgraph three c1-->c2 end one --> two three --> two two --> c2 Sequence Diagram \u00b6 sequenceDiagram participant Alice participant Bob Alice->>John: Hello John, how are you? loop Healthcheck John->>John: Fight against hypochondria end Note right of John: Rational thoughts <br/>prevail! John-->>Alice: Great! John->>Bob: How about you? Bob-->>John: Jolly good! Gantt \u00b6 gantt dateFormat YYYY-MM-DD title Adding GANTT diagram to mermaid excludes weekdays 2014-01-10 section A section Completed task :done, des1, 2014-01-06,2014-01-08 Active task :active, des2, 2014-01-09, 3d Future task : des3, after des2, 5d Future task2 : des4, after des3, 5d GitGraph \u00b6 git graphs Create a feature branch and make some file edits. gitGraph commit id: \"1\" commit id: \"2\" branch feature commit id: \"A\" commit id: \"B\" checkout main commit id: \"3\" commit id: \"4\" gitGraph commit commit branch develop checkout develop commit commit checkout main merge develop commit commit","title":"Mermaid Graphs"},{"location":"markdown/mermaid_graphs/#mermaid","text":"","title":"Mermaid"},{"location":"markdown/mermaid_graphs/#graph","text":"graph and flowchart are interchangeable mermaid flowcharts graph TD; A-->B; A-->C; B-->D; C-->D;","title":"Graph"},{"location":"markdown/mermaid_graphs/#use-html-with-mermaid","text":"graph TD; A-->B; A-->C; B-->D; C-->D;","title":"Use HTML with mermaid"},{"location":"markdown/mermaid_graphs/#nested-subgraph","text":"graph TD A[Christmas] -->|Get money| B(Go shopping) subgraph Nerve wracking B --> C{Roll a dice} end subgraph Don't look C -->|One| D[Laptop] C -->|Two| E[iPhone] subgraph High Probability C-->|Three| G C-->|Four| G C-->|Five| G[A gift card] end subgraph Low Probability C -->|Six| F[fa:fa-car Car] end end","title":"Nested subgraph"},{"location":"markdown/mermaid_graphs/#mlops-architecture","text":"graph LR A[GitHub] --> B(CICD <br> GitHub Actions) B --> C{Roll a dice} subgraph ML Prediction C1 --> C2 C1 --> C3(Flask) end subgraph Don't look C -->|Two| E[Flask] subgraph High Probability C-->|Three| G C-->|Four| G C-->|Five| G[A gift card] end subgraph Low Probability C -->|Seven| H[\"\ud83c\udf49\"] end end flowchart TB c1-->a2 subgraph one a1-->a2 end subgraph two b1-->b2 end subgraph three c1-->c2 end one --> two three --> two two --> c2","title":"MLOps Architecture"},{"location":"markdown/mermaid_graphs/#sequence-diagram","text":"sequenceDiagram participant Alice participant Bob Alice->>John: Hello John, how are you? loop Healthcheck John->>John: Fight against hypochondria end Note right of John: Rational thoughts <br/>prevail! John-->>Alice: Great! John->>Bob: How about you? Bob-->>John: Jolly good!","title":"Sequence Diagram"},{"location":"markdown/mermaid_graphs/#gantt","text":"gantt dateFormat YYYY-MM-DD title Adding GANTT diagram to mermaid excludes weekdays 2014-01-10 section A section Completed task :done, des1, 2014-01-06,2014-01-08 Active task :active, des2, 2014-01-09, 3d Future task : des3, after des2, 5d Future task2 : des4, after des3, 5d","title":"Gantt"},{"location":"markdown/mermaid_graphs/#gitgraph","text":"git graphs Create a feature branch and make some file edits. gitGraph commit id: \"1\" commit id: \"2\" branch feature commit id: \"A\" commit id: \"B\" checkout main commit id: \"3\" commit id: \"4\" gitGraph commit commit branch develop checkout develop commit commit checkout main merge develop commit commit","title":"GitGraph"},{"location":"markdown/mkdocs/","text":"mkdocs.yml \u00b6 Config file for mkdocs. site_name : ML Docs #site_description: Smol-CLS Cloud Enablement Documentation #site_url: https://docs.int.bayer.com/smol_cls_cloud_docs/ site_author : memadsen # edit_uri: edit/master/docs/ edit_uri : 'blob/main/docs' repo_url : https://github.com/bayer-int/smol-cls-docs-dse repo_name : smol-cls-docs-dse # copyright: Copyright &copy; 2021 - 2022 Bayer - Data Science Enablement Team # docs_dir: docs theme : name : material custom_dir : docs/overrides logo : assets/Logo_Bayer.svg.png icon : repo : fontawesome/brands/github # hide: # - toc # - navigation features : - navigation.tabs - navigation.tabs.sticky - navigation.sections - navigation.expand - navigation.instant - navigation.tracking - navigation.top - toc.follow - toc.integrate - search.suggest - search.highlight - search.share - header.autohide - announce.dismiss - content.tabs.link - content.code.annotate favicon : assets/bayer_logo.png palette : # Toggle to light mode - media : \"(prefers-color-scheme: light)\" scheme : default toggle : icon : material/weather-sunny name : Switch to dark mode primary : blue grey accent : blue teal # Toggle to dark mode - media : \"(prefers-color-scheme: dark)\" scheme : slate toggle : icon : material/weather-night name : Switch to light mode primary : blue grey accent : indigo plugins : - search : separator : '[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;' lang : - en - de # - gen-files: # scripts: # - docs/scripts/gen_ref_pages.py # - literate-nav: # nav_file: SUMMARY.md # - section-index # sr- mkdocstrings: # watch: # - src - mike : # these fields are all optional; the defaults are as below... version_selector : true # set to false to leave out the version selector css_dir : css # the directory to put the version selector's CSS javascript_dir : js # the directory to put the version selector's JS canonical_version : null # the version for <link rel=\"canonical\">; `null` # uses the version specified via `mike deploy` - git-revision-date : enabled_if_env : CI - git-revision-date-localized : enable_creation_date : true fallback_to_build_date : true - git-authors : show_contribution : true show_line_count : true count_empty_lines : true fallback_to_empty : true # exclude: # - index.md #enabled: !ENV [ENABLE_GIT_AUTHORS, True] enabled : true #- macros - mkdocs-video extra : version : provider : mike generator : false # analytics: # feedback: # title: Was this page helpful? # ratings: # - icon: material/emoticon-happy-outline # name: This page was helpful # data: 1 # note: >- # Thanks for your feedback! # - icon: material/emoticon-sad-outline # name: This page could be improved # data: 0 # note: >- # Thanks for your feedback! Help us improve this page by # using our <a href=\"...\" target=_blank>feedback form</a>. # consent: # title: Cookie consent # description: >- # GH_PAGES_ELXSJ docs uses cookies to recognize your repeated visits and preferences, as well # as to measure the effectiveness of our documentation and whether users # find what they're searching for. With your consent, you're helping us to # make our documentation better. # cookies: # analytics: Custom name markdown_extensions : - abbr - admonition - attr_list - codehilite - def_list - extra - footnotes - md_in_html - mdx_include : base_path : docs - meta - pymdownx.critic : mode : view - pymdownx.caret - pymdownx.keys - pymdownx.mark - pymdownx.tilde - pymdownx.details - pymdownx.superfences : custom_fences : - name : mermaid class : mermaid format : !!python/name:pymdownx.superfences.fence_code_format - pymdownx.tabbed : alternate_style : true - pymdownx.tasklist : custom_checkbox : true - pymdownx.highlight : anchor_linenums : true - pymdownx.inlinehilite - pymdownx.snippets - def_list - pymdownx.smartsymbols - pymdownx.emoji : emoji_index : !!python/name:materialx.emoji.twemoji emoji_generator : !!python/name:materialx.emoji.to_svg - pymdownx.arithmatex : generic : true - pymdownx.betterem : smart_enable : all - toc : permalink : true baselevel : 1 - tables extra_css : - css/termynal.css extra_javascript : - javascript/termynal.js - javascript/custom.js - https://polyfill.io/v3/polyfill.min.js?features=es6 - https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js #---- ---------------- Navigation ---------------- -----# nav : - Home : index.md","title":"Mkdocs"},{"location":"markdown/mkdocs/#mkdocsyml","text":"Config file for mkdocs. site_name : ML Docs #site_description: Smol-CLS Cloud Enablement Documentation #site_url: https://docs.int.bayer.com/smol_cls_cloud_docs/ site_author : memadsen # edit_uri: edit/master/docs/ edit_uri : 'blob/main/docs' repo_url : https://github.com/bayer-int/smol-cls-docs-dse repo_name : smol-cls-docs-dse # copyright: Copyright &copy; 2021 - 2022 Bayer - Data Science Enablement Team # docs_dir: docs theme : name : material custom_dir : docs/overrides logo : assets/Logo_Bayer.svg.png icon : repo : fontawesome/brands/github # hide: # - toc # - navigation features : - navigation.tabs - navigation.tabs.sticky - navigation.sections - navigation.expand - navigation.instant - navigation.tracking - navigation.top - toc.follow - toc.integrate - search.suggest - search.highlight - search.share - header.autohide - announce.dismiss - content.tabs.link - content.code.annotate favicon : assets/bayer_logo.png palette : # Toggle to light mode - media : \"(prefers-color-scheme: light)\" scheme : default toggle : icon : material/weather-sunny name : Switch to dark mode primary : blue grey accent : blue teal # Toggle to dark mode - media : \"(prefers-color-scheme: dark)\" scheme : slate toggle : icon : material/weather-night name : Switch to light mode primary : blue grey accent : indigo plugins : - search : separator : '[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;' lang : - en - de # - gen-files: # scripts: # - docs/scripts/gen_ref_pages.py # - literate-nav: # nav_file: SUMMARY.md # - section-index # sr- mkdocstrings: # watch: # - src - mike : # these fields are all optional; the defaults are as below... version_selector : true # set to false to leave out the version selector css_dir : css # the directory to put the version selector's CSS javascript_dir : js # the directory to put the version selector's JS canonical_version : null # the version for <link rel=\"canonical\">; `null` # uses the version specified via `mike deploy` - git-revision-date : enabled_if_env : CI - git-revision-date-localized : enable_creation_date : true fallback_to_build_date : true - git-authors : show_contribution : true show_line_count : true count_empty_lines : true fallback_to_empty : true # exclude: # - index.md #enabled: !ENV [ENABLE_GIT_AUTHORS, True] enabled : true #- macros - mkdocs-video extra : version : provider : mike generator : false # analytics: # feedback: # title: Was this page helpful? # ratings: # - icon: material/emoticon-happy-outline # name: This page was helpful # data: 1 # note: >- # Thanks for your feedback! # - icon: material/emoticon-sad-outline # name: This page could be improved # data: 0 # note: >- # Thanks for your feedback! Help us improve this page by # using our <a href=\"...\" target=_blank>feedback form</a>. # consent: # title: Cookie consent # description: >- # GH_PAGES_ELXSJ docs uses cookies to recognize your repeated visits and preferences, as well # as to measure the effectiveness of our documentation and whether users # find what they're searching for. With your consent, you're helping us to # make our documentation better. # cookies: # analytics: Custom name markdown_extensions : - abbr - admonition - attr_list - codehilite - def_list - extra - footnotes - md_in_html - mdx_include : base_path : docs - meta - pymdownx.critic : mode : view - pymdownx.caret - pymdownx.keys - pymdownx.mark - pymdownx.tilde - pymdownx.details - pymdownx.superfences : custom_fences : - name : mermaid class : mermaid format : !!python/name:pymdownx.superfences.fence_code_format - pymdownx.tabbed : alternate_style : true - pymdownx.tasklist : custom_checkbox : true - pymdownx.highlight : anchor_linenums : true - pymdownx.inlinehilite - pymdownx.snippets - def_list - pymdownx.smartsymbols - pymdownx.emoji : emoji_index : !!python/name:materialx.emoji.twemoji emoji_generator : !!python/name:materialx.emoji.to_svg - pymdownx.arithmatex : generic : true - pymdownx.betterem : smart_enable : all - toc : permalink : true baselevel : 1 - tables extra_css : - css/termynal.css extra_javascript : - javascript/termynal.js - javascript/custom.js - https://polyfill.io/v3/polyfill.min.js?features=es6 - https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js #---- ---------------- Navigation ---------------- -----# nav : - Home : index.md","title":"mkdocs.yml"},{"location":"sagemaker/secure-sagemaker-network-firewall-summary/","text":"Securing Amazon SageMaker Studio internet traffic using AWS Network Firewall \u00b6 Building secure machine learning environments with Amazon SageMaker Securing Amazon SageMaker Studio connectivity using a private VPC. Configuring Amazon SageMaker Studio for teams and groups with complete resource isolation Securing Amazon SageMaker Studio connectivity using a private VPC Background \u00b6 Amazon SageMaker Studio is a web-based fully integrated development environment (IDE) where you can perform end-to-end machine learning (ML) development to prepare data and build, train, and deploy models. One of these fundamental security features allows you to launch Studio in your own Amazon Virtual Private Cloud (Amazon VPC). This allows you to control, monitor, and inspect network traffic within and outside your VPC using standard AWS networking and security capabilities. For more information, see Securing Amazon SageMaker Studio connectivity using a private VPC. SM Studio users may want to provide internet access but also have some controls such as domain name or URL filtering and allow access to only specific public repositories and websites, possibly packet inspection, or other network traffic-related security controls. For these cases, AWS Network Firewall and NAT gateway-based deployment may provide a suitable use case. In this post, I outline the use of network firewall to build a secure and compliant environment by restricting and monitoring internet access, inspecting traffic, and using stateless and stateful firewall engine rules to control the network flow between Studio notebooks and the internet. Depending on your security, compliance, and governance rules, you may not need to or cannot completely block internet access from Studio and your AI and ML workloads. You may have requirements beyond the scope of network security controls implemented by security groups and network access control lists (ACLs), such as application protocol protection, deep packet inspection, domain name filtering, and intrusion prevention system (IPS). Your network traffic controls may also require many more rules compared to what is currently supported in security groups and network ACLs. In these scenarios, you can use Network Firewall\u2014a managed network firewall and IPS for your VPC. Solution overview \u00b6 Sagemaker Studio deployed in a VPC, provides internet access control, using the parameter AppNetworkAccessType (via the Amazon SageMaker API ) or by selecting your preference on the console when you create a Studio domain. If you select Public internet Only ( PublicInternetOnly ), all the ingress and egress internet traffic from Amazon SageMaker notebooks flows through an AWS managed internet gateway attached to a VPC in your SageMaker account. The following diagram shows this network configuration. Studio provides public internet egress through a platform-managed VPC for data scientists to download notebooks, packages, and datasets. Traffic to the attached Amazon Elastic File System (Amazon EFS) volume always goes through the customer VPC and never through the public internet egress. To use your own control flow for the internet traffic, like a NAT or internet gateway, you must set the AppNetworkAccessType parameter to VpcOnly (or select VPC Only on the console). When you launch your app, this creates an elastic network interface in the specified subnets in your VPC. You can apply all available layers of security control\u2014 security groups , network ACLs , VPC endpoints , AWS PrivateLink , or Network Firewall endpoints \u2014to the internal network and internet traffic to exercise fine-grained control of network access in Studio. The following diagram shows the VpcOnly network configuration. In this mode, the direct internet access to or from notebooks is completely disabled, and all traffic is routed through an elastic network interface in your private VPC. This also includes traffic from Studio UI widgets and interfaces, such as Experiments , Autopilot , and Model Monitor , to their respective backend SageMaker APIs. VPC only option \u00b6 The solution in this post uses the VpcOnly option and deploys the Studio domain into a VPC with three subnets: SageMaker subnet \u2013 Hosts all Studio workloads. All ingress and egress network flow is controlled by a security group. NAT subnet \u2013 Contains a NAT gateway. We use the NAT gateway to access the internet without exposing any private IP addresses from our private network. Network Firewall subnet \u2013 Contains a Network Firewall endpoint. The route tables are configured so that all inbound and outbound external network traffic is routed via Network Firewall. You can configure stateful and stateless Network Firewall policies to inspect, monitor, and control the traffic. The following diagram shows the overview of the solution architecture and the deployed components. SageMaker resources \u00b6 Create a SageMaker domain and user profile. The solution uses only one Availability Zone and is not highly available. A best practice is to use a Multi-AZ configuration for any production deployment. We create an allow domain list rule to allow internet access to the specified network domains only and block traffic to any domain not on the allow list. AWS CloudFormation resources \u00b6 The source code and AWS CloudFormation template for solution deployment are provided in the GitHub repository . Network Firewall is a Regional service; for more information on Region availability, see the AWS Region Table . To start experimenting with the Network Firewall and stateful rules, you need first to deploy the provided CloudFormation template to the AWS account. Clone the GitHub repository: Create an S3 bucket in the Region where you deploy the solution: aws s3 mb s3://<your s3 bucket name> You can skip this step if you already have an S3 bucket. Deploy the CloudFormation stack: make deploy CFN_ARTEFACT_S3_BUCKET = <your s3 bucket name> The deployment procedure packages the CloudFormation template and copies it to the S3 bucket your provided. Then the CloudFormation template is deployed from the S3 bucket to your AWS account. The stack deploys all the needed resources like VPC, network devices, route tables, security groups, S3 buckets, IAM policies and roles, and VPC endpoints, and also creates a new Studio domain and user profile. When the deployment is complete, you can see the full list of stack output values by running the following command in terminal: aws cloudformation describe-stacks \\ --stack-name sagemaker-studio-demo \\ --output table \\ --query \"Stacks[0].Outputs[*].[OutputKey, OutputValue]\" Launch Studio via the SageMaker console. Experiment with Network Firewall \u00b6 Now you can learn how to control the internet inbound and outbound access with Network Firewall. In this section, we discuss the initial setup, accessing resources not on the allow list, adding domains to the allow list, configuring logging, and additional firewall rules. Initial setup \u00b6 The solution deploys a Network Firewall policy with a stateful rule group with an allow domain list. This policy is attached to the Network Firewall. All inbound and outbound internet traffic is blocked now, except for the .kaggle.com domain, which is on the allow list. Let\u2019s try to access https://kaggle.com by opening a new notebook in Studio and attempting to download the front page from kaggle.com : !wget https://kaggle.com The following screenshot shows that the request succeeds because the domain is allowed by the firewall policy. Users can connect to this and only to this domain from any Studio notebook. Access resources not on the allowed domain list \u00b6 In the Studio notebook, try to clone any public GitHub repository, such as the following: !git clone https://github.com/pytorch/examples.git This operation times out after 5 minutes because any internet traffic except to and from the .kaggle.com domain isn\u2019t allowed and is dropped by the firewall. Add a domain to the allowed domain list \u00b6 To be able to run the git clone command, you must allow internet traffic to the .github.com domain. On the Amazon VPC console, choose Firewall policies. Choose the policy network-firewall-policy- . In the Stateful rule groups section, select the group rule domain-allow-sagemaker- . You can see the domain .kaggle.com on the allow list. Choose Add domain. Enter .github.com . Choose Save. You now have two names on the allow domain list. Firewall policy is propagated in real time to Network Firewall and your changes take effect immediately. Any inbound or outbound traffic from or to these domains is now allowed by the firewall and all other traffic is dropped. To validate the new configuration, go to your Studio notebook and try to clone the same GitHub repository again: !git clone https://github.com/aws-samples/amazon-sagemaker-studio-vpc-networkfirewall.git The operation succeeds this time\u2014Network Firewall allows access to the .github.com domain. Network Firewall logging \u00b6 In this section, you configure Network Firewall logging for your firewall\u2019s stateful engine. Logging gives you detailed information about network traffic, including the time that the stateful engine received a packet, detailed information about the packet, and any stateful rule action taken against the packet. The logs are published to the log destination that you configured, where you can retrieve and view them. On the Amazon VPC console, choose Firewalls . Choose your firewall. Choose the Firewall details tab. In the Logging section, choose Edit . Configure your firewall logging by selecting what log types you want to capture and providing the log destination. For this post, select Alert log type, set Log destination for alerts to CloudWatch Log group, and provide an existing or a new log group where the firewall logs are delivered. Choose Save . To check your settings, go back to Studio and try to access pypi.org to install a Python package: !pip install -U scikit-learn This command fails with ReadTimeoutError because Network Firewall drops any traffic to any domain not on the allow list (which contains only two domains: .github.com and .kaggle.com ). On the Amazon CloudWatch console , navigate to the log group and browse through the recent log streams. The pipy.org domain shows the blocked action. The log event also provides additional details such as various timestamps, protocol, port and IP details, event type, availability zone, and the firewall name. You can continue experimenting with Network Firewall by adding .pypi.org and .pythonhosted.org domains to the allowed domain list. Then validate your access to them via your Studio notebook. Additional firewall rules \u00b6 You can create any other stateless or stateful firewall rules and implement traffic filtering based on a standard stateful 5-tuple rule for network traffic inspection (protocol, source IP, source port, destination IP, destination port). Network Firewall also supports industry standard stateful Suricata compatible IPS rule groups. You can implement protocol-based rules to detect and block any non-standard or promiscuous usage or activity. For more information about creating and managing Network Firewall rule groups, see Rule groups in AWS Network Firewall. Additional security controls with Network Firewall \u00b6 In the previous section, we looked at one feature of the Network Firewall: filtering network traffic based on the domain name. In addition to stateless or stateful firewall rules, Network Firewall provides several tools and features for further security controls and monitoring: Central firewall management and visibility in AWS Firewall Manager . You can centrally manage security policies and automatically enforce mandatory security policies across existing and newly created accounts and VPCs. Network Firewall logging for the firewall\u2019s stateful engine. You can record flow and alert logs, and use the same or different logging destinations for each log type. Stateless rules to filter network traffic based on protocol, source IP addresses, ranges, source port ranges, destination IP addresses and ranges, and TCP flags. Integration into a broader set of AWS security components. For an example, see Automatically block suspicious traffic with AWS Network Firewall and Amazon GuardDuty. Integration in a diverse ecosystem of Network Firewall Partners that complement Network Firewall, enabling the deployment of a comprehensive security architecture. For example use cases, see Full VPC traffic visibility with AWS Network Firewall and Sumo Logic and Splunk Named Launch Partner of AWS Network Firewall. Build secure ML environments \u00b6 A robust security design normally includes multi-layer security controls for the system. For SageMaker environments and workloads, you can use the following AWS security services and concepts to secure, control, and monitor your environment: VPC and private subnets to perform secure API calls to other AWS services and restrict internet access for downloading packages. S3 bucket policies that restrict access to specific VPC endpoints. Encryption of ML model artifacts and other system artifacts that are either in transit or at rest. Requests to the SageMaker API and console are made over a Secure Sockets Layer (SSL) connection. Restricted IAM roles and policies for SageMaker runs and notebook access based on resource tags and project ID. Restricted access to Amazon public services, such as Amazon Elastic Container Registry (Amazon ECR) to VPC endpoints only. For a reference deployment architecture and ready-to-use deployable constructs for your environment, see Amazon SageMaker with Guardrails on AWS. Conclusion \u00b6 In this post, we showed how you can secure, log, and monitor internet ingress and egress traffic in Studio notebooks for your sensitive ML workloads using managed Network Firewall. You can use the provided CloudFormation templates to automate SageMaker deployment as part of your Infrastructure as Code (IaC) strategy. For more information about other possibilities to secure your SageMaker deployments and ML workloads, see Building secure machine learning environments with Amazon SageMaker.","title":"Secure Network Summary"},{"location":"sagemaker/secure-sagemaker-network-firewall-summary/#securing-amazon-sagemaker-studio-internet-traffic-using-aws-network-firewall","text":"Building secure machine learning environments with Amazon SageMaker Securing Amazon SageMaker Studio connectivity using a private VPC. Configuring Amazon SageMaker Studio for teams and groups with complete resource isolation Securing Amazon SageMaker Studio connectivity using a private VPC","title":"Securing Amazon SageMaker Studio internet traffic using AWS Network Firewall"},{"location":"sagemaker/secure-sagemaker-network-firewall-summary/#background","text":"Amazon SageMaker Studio is a web-based fully integrated development environment (IDE) where you can perform end-to-end machine learning (ML) development to prepare data and build, train, and deploy models. One of these fundamental security features allows you to launch Studio in your own Amazon Virtual Private Cloud (Amazon VPC). This allows you to control, monitor, and inspect network traffic within and outside your VPC using standard AWS networking and security capabilities. For more information, see Securing Amazon SageMaker Studio connectivity using a private VPC. SM Studio users may want to provide internet access but also have some controls such as domain name or URL filtering and allow access to only specific public repositories and websites, possibly packet inspection, or other network traffic-related security controls. For these cases, AWS Network Firewall and NAT gateway-based deployment may provide a suitable use case. In this post, I outline the use of network firewall to build a secure and compliant environment by restricting and monitoring internet access, inspecting traffic, and using stateless and stateful firewall engine rules to control the network flow between Studio notebooks and the internet. Depending on your security, compliance, and governance rules, you may not need to or cannot completely block internet access from Studio and your AI and ML workloads. You may have requirements beyond the scope of network security controls implemented by security groups and network access control lists (ACLs), such as application protocol protection, deep packet inspection, domain name filtering, and intrusion prevention system (IPS). Your network traffic controls may also require many more rules compared to what is currently supported in security groups and network ACLs. In these scenarios, you can use Network Firewall\u2014a managed network firewall and IPS for your VPC.","title":"Background"},{"location":"sagemaker/secure-sagemaker-network-firewall-summary/#solution-overview","text":"Sagemaker Studio deployed in a VPC, provides internet access control, using the parameter AppNetworkAccessType (via the Amazon SageMaker API ) or by selecting your preference on the console when you create a Studio domain. If you select Public internet Only ( PublicInternetOnly ), all the ingress and egress internet traffic from Amazon SageMaker notebooks flows through an AWS managed internet gateway attached to a VPC in your SageMaker account. The following diagram shows this network configuration. Studio provides public internet egress through a platform-managed VPC for data scientists to download notebooks, packages, and datasets. Traffic to the attached Amazon Elastic File System (Amazon EFS) volume always goes through the customer VPC and never through the public internet egress. To use your own control flow for the internet traffic, like a NAT or internet gateway, you must set the AppNetworkAccessType parameter to VpcOnly (or select VPC Only on the console). When you launch your app, this creates an elastic network interface in the specified subnets in your VPC. You can apply all available layers of security control\u2014 security groups , network ACLs , VPC endpoints , AWS PrivateLink , or Network Firewall endpoints \u2014to the internal network and internet traffic to exercise fine-grained control of network access in Studio. The following diagram shows the VpcOnly network configuration. In this mode, the direct internet access to or from notebooks is completely disabled, and all traffic is routed through an elastic network interface in your private VPC. This also includes traffic from Studio UI widgets and interfaces, such as Experiments , Autopilot , and Model Monitor , to their respective backend SageMaker APIs.","title":"Solution overview"},{"location":"sagemaker/secure-sagemaker-network-firewall-summary/#vpc-only-option","text":"The solution in this post uses the VpcOnly option and deploys the Studio domain into a VPC with three subnets: SageMaker subnet \u2013 Hosts all Studio workloads. All ingress and egress network flow is controlled by a security group. NAT subnet \u2013 Contains a NAT gateway. We use the NAT gateway to access the internet without exposing any private IP addresses from our private network. Network Firewall subnet \u2013 Contains a Network Firewall endpoint. The route tables are configured so that all inbound and outbound external network traffic is routed via Network Firewall. You can configure stateful and stateless Network Firewall policies to inspect, monitor, and control the traffic. The following diagram shows the overview of the solution architecture and the deployed components.","title":"VPC only option"},{"location":"sagemaker/secure-sagemaker-network-firewall-summary/#sagemaker-resources","text":"Create a SageMaker domain and user profile. The solution uses only one Availability Zone and is not highly available. A best practice is to use a Multi-AZ configuration for any production deployment. We create an allow domain list rule to allow internet access to the specified network domains only and block traffic to any domain not on the allow list.","title":"SageMaker resources"},{"location":"sagemaker/secure-sagemaker-network-firewall-summary/#aws-cloudformation-resources","text":"The source code and AWS CloudFormation template for solution deployment are provided in the GitHub repository . Network Firewall is a Regional service; for more information on Region availability, see the AWS Region Table . To start experimenting with the Network Firewall and stateful rules, you need first to deploy the provided CloudFormation template to the AWS account. Clone the GitHub repository: Create an S3 bucket in the Region where you deploy the solution: aws s3 mb s3://<your s3 bucket name> You can skip this step if you already have an S3 bucket. Deploy the CloudFormation stack: make deploy CFN_ARTEFACT_S3_BUCKET = <your s3 bucket name> The deployment procedure packages the CloudFormation template and copies it to the S3 bucket your provided. Then the CloudFormation template is deployed from the S3 bucket to your AWS account. The stack deploys all the needed resources like VPC, network devices, route tables, security groups, S3 buckets, IAM policies and roles, and VPC endpoints, and also creates a new Studio domain and user profile. When the deployment is complete, you can see the full list of stack output values by running the following command in terminal: aws cloudformation describe-stacks \\ --stack-name sagemaker-studio-demo \\ --output table \\ --query \"Stacks[0].Outputs[*].[OutputKey, OutputValue]\" Launch Studio via the SageMaker console.","title":"AWS CloudFormation resources"},{"location":"sagemaker/secure-sagemaker-network-firewall-summary/#experiment-with-network-firewall","text":"Now you can learn how to control the internet inbound and outbound access with Network Firewall. In this section, we discuss the initial setup, accessing resources not on the allow list, adding domains to the allow list, configuring logging, and additional firewall rules.","title":"Experiment with Network Firewall"},{"location":"sagemaker/secure-sagemaker-network-firewall-summary/#initial-setup","text":"The solution deploys a Network Firewall policy with a stateful rule group with an allow domain list. This policy is attached to the Network Firewall. All inbound and outbound internet traffic is blocked now, except for the .kaggle.com domain, which is on the allow list. Let\u2019s try to access https://kaggle.com by opening a new notebook in Studio and attempting to download the front page from kaggle.com : !wget https://kaggle.com The following screenshot shows that the request succeeds because the domain is allowed by the firewall policy. Users can connect to this and only to this domain from any Studio notebook.","title":"Initial setup"},{"location":"sagemaker/secure-sagemaker-network-firewall-summary/#access-resources-not-on-the-allowed-domain-list","text":"In the Studio notebook, try to clone any public GitHub repository, such as the following: !git clone https://github.com/pytorch/examples.git This operation times out after 5 minutes because any internet traffic except to and from the .kaggle.com domain isn\u2019t allowed and is dropped by the firewall.","title":"Access resources not on the allowed domain list"},{"location":"sagemaker/secure-sagemaker-network-firewall-summary/#add-a-domain-to-the-allowed-domain-list","text":"To be able to run the git clone command, you must allow internet traffic to the .github.com domain. On the Amazon VPC console, choose Firewall policies. Choose the policy network-firewall-policy- . In the Stateful rule groups section, select the group rule domain-allow-sagemaker- . You can see the domain .kaggle.com on the allow list. Choose Add domain. Enter .github.com . Choose Save. You now have two names on the allow domain list. Firewall policy is propagated in real time to Network Firewall and your changes take effect immediately. Any inbound or outbound traffic from or to these domains is now allowed by the firewall and all other traffic is dropped. To validate the new configuration, go to your Studio notebook and try to clone the same GitHub repository again: !git clone https://github.com/aws-samples/amazon-sagemaker-studio-vpc-networkfirewall.git The operation succeeds this time\u2014Network Firewall allows access to the .github.com domain.","title":"Add a domain to the allowed domain list"},{"location":"sagemaker/secure-sagemaker-network-firewall-summary/#network-firewall-logging","text":"In this section, you configure Network Firewall logging for your firewall\u2019s stateful engine. Logging gives you detailed information about network traffic, including the time that the stateful engine received a packet, detailed information about the packet, and any stateful rule action taken against the packet. The logs are published to the log destination that you configured, where you can retrieve and view them. On the Amazon VPC console, choose Firewalls . Choose your firewall. Choose the Firewall details tab. In the Logging section, choose Edit . Configure your firewall logging by selecting what log types you want to capture and providing the log destination. For this post, select Alert log type, set Log destination for alerts to CloudWatch Log group, and provide an existing or a new log group where the firewall logs are delivered. Choose Save . To check your settings, go back to Studio and try to access pypi.org to install a Python package: !pip install -U scikit-learn This command fails with ReadTimeoutError because Network Firewall drops any traffic to any domain not on the allow list (which contains only two domains: .github.com and .kaggle.com ). On the Amazon CloudWatch console , navigate to the log group and browse through the recent log streams. The pipy.org domain shows the blocked action. The log event also provides additional details such as various timestamps, protocol, port and IP details, event type, availability zone, and the firewall name. You can continue experimenting with Network Firewall by adding .pypi.org and .pythonhosted.org domains to the allowed domain list. Then validate your access to them via your Studio notebook.","title":"Network Firewall logging"},{"location":"sagemaker/secure-sagemaker-network-firewall-summary/#additional-firewall-rules","text":"You can create any other stateless or stateful firewall rules and implement traffic filtering based on a standard stateful 5-tuple rule for network traffic inspection (protocol, source IP, source port, destination IP, destination port). Network Firewall also supports industry standard stateful Suricata compatible IPS rule groups. You can implement protocol-based rules to detect and block any non-standard or promiscuous usage or activity. For more information about creating and managing Network Firewall rule groups, see Rule groups in AWS Network Firewall.","title":"Additional firewall rules"},{"location":"sagemaker/secure-sagemaker-network-firewall-summary/#additional-security-controls-with-network-firewall","text":"In the previous section, we looked at one feature of the Network Firewall: filtering network traffic based on the domain name. In addition to stateless or stateful firewall rules, Network Firewall provides several tools and features for further security controls and monitoring: Central firewall management and visibility in AWS Firewall Manager . You can centrally manage security policies and automatically enforce mandatory security policies across existing and newly created accounts and VPCs. Network Firewall logging for the firewall\u2019s stateful engine. You can record flow and alert logs, and use the same or different logging destinations for each log type. Stateless rules to filter network traffic based on protocol, source IP addresses, ranges, source port ranges, destination IP addresses and ranges, and TCP flags. Integration into a broader set of AWS security components. For an example, see Automatically block suspicious traffic with AWS Network Firewall and Amazon GuardDuty. Integration in a diverse ecosystem of Network Firewall Partners that complement Network Firewall, enabling the deployment of a comprehensive security architecture. For example use cases, see Full VPC traffic visibility with AWS Network Firewall and Sumo Logic and Splunk Named Launch Partner of AWS Network Firewall.","title":"Additional security controls with Network Firewall"},{"location":"sagemaker/secure-sagemaker-network-firewall-summary/#build-secure-ml-environments","text":"A robust security design normally includes multi-layer security controls for the system. For SageMaker environments and workloads, you can use the following AWS security services and concepts to secure, control, and monitor your environment: VPC and private subnets to perform secure API calls to other AWS services and restrict internet access for downloading packages. S3 bucket policies that restrict access to specific VPC endpoints. Encryption of ML model artifacts and other system artifacts that are either in transit or at rest. Requests to the SageMaker API and console are made over a Secure Sockets Layer (SSL) connection. Restricted IAM roles and policies for SageMaker runs and notebook access based on resource tags and project ID. Restricted access to Amazon public services, such as Amazon Elastic Container Registry (Amazon ECR) to VPC endpoints only. For a reference deployment architecture and ready-to-use deployable constructs for your environment, see Amazon SageMaker with Guardrails on AWS.","title":"Build secure ML environments"},{"location":"sagemaker/secure-sagemaker-network-firewall-summary/#conclusion","text":"In this post, we showed how you can secure, log, and monitor internet ingress and egress traffic in Studio notebooks for your sensitive ML workloads using managed Network Firewall. You can use the provided CloudFormation templates to automate SageMaker deployment as part of your Infrastructure as Code (IaC) strategy. For more information about other possibilities to secure your SageMaker deployments and ML workloads, see Building secure machine learning environments with Amazon SageMaker.","title":"Conclusion"},{"location":"sagemaker/secure-sagemaker-network-firewall/","text":"Securing Amazon SageMaker Studio internet traffic using AWS Network Firewall \u00b6 Full github repo with code The work in this document complements previous work: - Building secure machine learning environments with Amazon SageMaker Securing Amazon SageMaker Studio connectivity using a private VPC. Background \u00b6 Amazon SageMaker Studio is a web-based fully integrated development environment (IDE) where you can perform end-to-end machine learning (ML) development to prepare data and build, train, and deploy models. Like other AWS services, Studio supports a rich set of security-related features that allow you to build highly secure and compliant environments. One of these fundamental security features allows you to launch Studio in your own Amazon Virtual Private Cloud (Amazon VPC). This allows you to control, monitor, and inspect network traffic within and outside your VPC using standard AWS networking and security capabilities. For more information, see Securing Amazon SageMaker Studio connectivity using a private VPC. Customers in regulated industries, such as financial services, often don\u2019t allow any internet access in ML environments. They often use only VPC endpoints for AWS services, and connect only to private source code repositories in which all libraries have been vetted both in terms of security and licensing. Customers may want to provide internet access but also have some controls such as domain name or URL filtering and allow access to only specific public repositories and websites, possibly packet inspection, or other network traffic-related security controls. For these cases, AWS Network Firewall and NAT gateway-based deployment may provide a suitable use case. In this post, we show how you can use Network Firewall to build a secure and compliant environment by restricting and monitoring internet access, inspecting traffic, and using stateless and stateful firewall engine rules to control the network flow between Studio notebooks and the internet. Depending on your security, compliance, and governance rules, you may not need to or cannot completely block internet access from Studio and your AI and ML workloads. You may have requirements beyond the scope of network security controls implemented by security groups and network access control lists (ACLs), such as application protocol protection, deep packet inspection, domain name filtering, and intrusion prevention system (IPS). Your network traffic controls may also require many more rules compared to what is currently supported in security groups and network ACLs. In these scenarios, you can use Network Firewall\u2014a managed network firewall and IPS for your VPC. Solution overview \u00b6 When you deploy Studio in your VPC, you control how Studio accesses the internet with the parameter AppNetworkAccessType (via the Amazon SageMaker API ) or by selecting your preference on the console when you create a Studio domain. If you select Public internet Only ( PublicInternetOnly ), all the ingress and egress internet traffic from Amazon SageMaker notebooks flows through an AWS managed internet gateway attached to a VPC in your SageMaker account. The following diagram shows this network configuration. Studio provides public internet egress through a platform-managed VPC for data scientists to download notebooks, packages, and datasets. Traffic to the attached Amazon Elastic File System (Amazon EFS) volume always goes through the customer VPC and never through the public internet egress. To use your own control flow for the internet traffic, like a NAT or internet gateway, you must set the AppNetworkAccessType parameter to VpcOnly (or select VPC Only on the console). When you launch your app, this creates an elastic network interface in the specified subnets in your VPC. You can apply all available layers of security control\u2014 security groups , network ACLs , VPC endpoints , AWS PrivateLink , or Network Firewall endpoints \u2014to the internal network and internet traffic to exercise fine-grained control of network access in Studio. The following diagram shows the VpcOnly network configuration. In this mode, the direct internet access to or from notebooks is completely disabled, and all traffic is routed through an elastic network interface in your private VPC. This also includes traffic from Studio UI widgets and interfaces, such as Experiments , Autopilot , and Model Monitor , to their respective backend SageMaker APIs. For more information about network access parameters when creating a domain, see CreateDomain . The solution in this post uses the VpcOnly option and deploys the Studio domain into a VPC with three subnets: SageMaker subnet \u2013 Hosts all Studio workloads. All ingress and egress network flow is controlled by a security group. NAT subnet \u2013 Contains a NAT gateway. We use the NAT gateway to access the internet without exposing any private IP addresses from our private network. Network Firewall subnet \u2013 Contains a Network Firewall endpoint. The route tables are configured so that all inbound and outbound external network traffic is routed via Network Firewall. You can configure stateful and stateless Network Firewall policies to inspect, monitor, and control the traffic. The following diagram shows the overview of the solution architecture and the deployed components. VPC resources \u00b6 The solution deploys the following resources in your account: A VPC with a specified Classless Inter-Domain Routing (CIDR) block Three private subnets with specified CIDRs Internet gateway, NAT gateway, Network Firewall, and a Network Firewall endpoint in the Network Firewall subnet A Network Firewall policy and stateful domain list group with an allow domain list Elastic IP allocated to the NAT gateway Two security groups for SageMaker workloads and VPC endpoints, respectively Four route tables with configured routes An Amazon S3 VPC endpoint (type Gateway) AWS service access VPC endpoints (type Interface) for various AWS services that need to be accessed from Studio The solution also creates an AWS Identity and Access Management (IAM) execution role for SageMaker notebooks and Studio with preconfigured IAM policies. Network routing for targets outside the VPC is configured in such a way that all ingress and egress internet traffic goes via the Network Firewall and NAT gateway. For details and reference network architectures with Network Firewall and NAT gateway, see Architecture with an internet gateway and a NAT gateway , Deployment models for AWS Network Firewall , and Enforce your AWS Network Firewall protections at scale with AWS Firewall Manager . The AWS re:Invent 2020 video Which inspection architecture is right for you? discusses which inspection architecture is right for your use case. SageMaker resources \u00b6 The solution creates a SageMaker domain and user profile. The solution uses only one Availability Zone and is not highly available. A best practice is to use a Multi-AZ configuration for any production deployment. You can implement the highly available solution by duplicating the Single-AZ setup\u2014subnets, NAT gateway, and Network Firewall endpoints\u2014to additional Availability Zones. You use Network Firewall and its policies to control entry and exit of the internet traffic in your VPC. You create an allow domain list rule to allow internet access to the specified network domains only and block traffic to any domain not on the allow list. AWS CloudFormation resources \u00b6 The source code and AWS CloudFormation template for solution deployment are provided in the GitHub repository . To deploy the solution on your account, you need: An AWS account and the AWS Command Line Interface (AWS CLI) configured with administrator permissions An Amazon Simple Storage Service (Amazon S3) bucket in your account in the same Region where you deploy the solution Network Firewall is a Regional service; for more information on Region availability, see the AWS Region Table . Your CloudFormation stack doesn\u2019t have any required parameters. You may want to change the DomainName or *CIDR parameters to avoid naming conflicts with the existing resources and your VPC CIDR allocations. Otherwise, use the following default values: ProjectName \u2013 sagemaker-studio-vpc-firewall DomainName \u2013 sagemaker-anfw-domain UserProfileName \u2013 anfw-user-profile VPCCIDR \u2013 10.2.0.0/16 FirewallSubnetCIDR \u2013 10.2.1.0/24 NATGatewaySubnetCIDR \u2013 10.2.2.0/24 SageMakerStudioSubnetCIDR \u2013 10.2.3.0/24 Deploy the CloudFormation template To start experimenting with the Network Firewall and stateful rules, you need first to deploy the provided CloudFormation template to your AWS account. Clone the GitHub repository: git clone https://github.com/aws-samples/amazon-sagemaker-studio-vpc-networkfirewall.git cd amazon-sagemaker-studio-vpc-networkfirewall Create an S3 bucket in the Region where you deploy the solution: aws s3 mb s3://<your s3 bucket name> You can skip this step if you already have an S3 bucket. Deploy the CloudFormation stack: make deploy CFN_ARTEFACT_S3_BUCKET = <your s3 bucket name> The deployment procedure packages the CloudFormation template and copies it to the S3 bucket your provided. Then the CloudFormation template is deployed from the S3 bucket to your AWS account. The stack deploys all the needed resources like VPC, network devices, route tables, security groups, S3 buckets, IAM policies and roles, and VPC endpoints, and also creates a new Studio domain and user profile. When the deployment is complete, you can see the full list of stack output values by running the following command in terminal: aws cloudformation describe-stacks \\ --stack-name sagemaker-studio-demo \\ --output table \\ --query \"Stacks[0].Outputs[*].[OutputKey, OutputValue]\" Launch Studio via the SageMaker console. Experiment with Network Firewall \u00b6 Now you can learn how to control the internet inbound and outbound access with Network Firewall. In this section, we discuss the initial setup, accessing resources not on the allow list, adding domains to the allow list, configuring logging, and additional firewall rules. Initial setup \u00b6 The solution deploys a Network Firewall policy with a stateful rule group with an allow domain list. This policy is attached to the Network Firewall. All inbound and outbound internet traffic is blocked now, except for the .kaggle.com domain, which is on the allow list. Let\u2019s try to access https://kaggle.com by opening a new notebook in Studio and attempting to download the front page from kaggle.com : !wget https://kaggle.com The following screenshot shows that the request succeeds because the domain is allowed by the firewall policy. Users can connect to this and only to this domain from any Studio notebook. Access resources not on the allowed domain list \u00b6 In the Studio notebook, try to clone any public GitHub repository, such as the following: !git clone https://github.com/aws-samples/amazon-sagemaker-studio-vpc-networkfirewall.git This operation times out after 5 minutes because any internet traffic except to and from the .kaggle.com domain isn\u2019t allowed and is dropped by Network Firewall. Add a domain to the allowed domain list \u00b6 To be able to run the git clone command, you must allow internet traffic to the .github.com domain. On the Amazon VPC console, choose Firewall policies. Choose the policy network-firewall-policy- . In the Stateful rule groups section, select the group rule domain-allow-sagemaker- . You can see the domain .kaggle.com on the allow list. Choose Add domain. Enter .github.com . Choose Save. You now have two names on the allow domain list. Firewall policy is propagated in real time to Network Firewall and your changes take effect immediately. Any inbound or outbound traffic from or to these domains is now allowed by the firewall and all other traffic is dropped. To validate the new configuration, go to your Studio notebook and try to clone the same GitHub repository again: !git clone https://github.com/aws-samples/amazon-sagemaker-studio-vpc-networkfirewall.git The operation succeeds this time\u2014Network Firewall allows access to the .github.com domain. Network Firewall logging \u00b6 In this section, you configure Network Firewall logging for your firewall\u2019s stateful engine. Logging gives you detailed information about network traffic, including the time that the stateful engine received a packet, detailed information about the packet, and any stateful rule action taken against the packet. The logs are published to the log destination that you configured, where you can retrieve and view them. On the Amazon VPC console, choose Firewalls . Choose your firewall. Choose the Firewall details tab. In the Logging section, choose Edit . Configure your firewall logging by selecting what log types you want to capture and providing the log destination. For this post, select Alert log type, set Log destination for alerts to CloudWatch Log group, and provide an existing or a new log group where the firewall logs are delivered. Choose Save . To check your settings, go back to Studio and try to access pypi.org to install a Python package: !pip install -U scikit-learn This command fails with ReadTimeoutError because Network Firewall drops any traffic to any domain not on the allow list (which contains only two domains: .github.com and .kaggle.com ). On the Amazon CloudWatch console , navigate to the log group and browse through the recent log streams. The pipy.org domain shows the blocked action. The log event also provides additional details such as various timestamps, protocol, port and IP details, event type, availability zone, and the firewall name. You can continue experimenting with Network Firewall by adding .pypi.org and .pythonhosted.org domains to the allowed domain list. Then validate your access to them via your Studio notebook. Additional firewall rules \u00b6 You can create any other stateless or stateful firewall rules and implement traffic filtering based on a standard stateful 5-tuple rule for network traffic inspection (protocol, source IP, source port, destination IP, destination port). Network Firewall also supports industry standard stateful Suricata compatible IPS rule groups. You can implement protocol-based rules to detect and block any non-standard or promiscuous usage or activity. For more information about creating and managing Network Firewall rule groups, see Rule groups in AWS Network Firewall. Additional security controls with Network Firewall \u00b6 In the previous section, we looked at one feature of the Network Firewall: filtering network traffic based on the domain name. In addition to stateless or stateful firewall rules, Network Firewall provides several tools and features for further security controls and monitoring: Central firewall management and visibility in AWS Firewall Manager . You can centrally manage security policies and automatically enforce mandatory security policies across existing and newly created accounts and VPCs. Network Firewall logging for the firewall\u2019s stateful engine. You can record flow and alert logs, and use the same or different logging destinations for each log type. Stateless rules to filter network traffic based on protocol, source IP addresses, ranges, source port ranges, destination IP addresses and ranges, and TCP flags. Integration into a broader set of AWS security components. For an example, see Automatically block suspicious traffic with AWS Network Firewall and Amazon GuardDuty. Integration in a diverse ecosystem of Network Firewall Partners that complement Network Firewall, enabling the deployment of a comprehensive security architecture. For example use cases, see Full VPC traffic visibility with AWS Network Firewall and Sumo Logic and Splunk Named Launch Partner of AWS Network Firewall. Build secure ML environments \u00b6 A robust security design normally includes multi-layer security controls for the system. For SageMaker environments and workloads, you can use the following AWS security services and concepts to secure, control, and monitor your environment: VPC and private subnets to perform secure API calls to other AWS services and restrict internet access for downloading packages. S3 bucket policies that restrict access to specific VPC endpoints. Encryption of ML model artifacts and other system artifacts that are either in transit or at rest. Requests to the SageMaker API and console are made over a Secure Sockets Layer (SSL) connection. Restricted IAM roles and policies for SageMaker runs and notebook access based on resource tags and project ID. Restricted access to Amazon public services, such as Amazon Elastic Container Registry (Amazon ECR) to VPC endpoints only. For a reference deployment architecture and ready-to-use deployable constructs for your environment, see Amazon SageMaker with Guardrails on AWS. Conclusion \u00b6 In this post, we showed how you can secure, log, and monitor internet ingress and egress traffic in Studio notebooks for your sensitive ML workloads using managed Network Firewall. You can use the provided CloudFormation templates to automate SageMaker deployment as part of your Infrastructure as Code (IaC) strategy. For more information about other possibilities to secure your SageMaker deployments and ML workloads, see Building secure machine learning environments with Amazon SageMaker.","title":"Secure Network"},{"location":"sagemaker/secure-sagemaker-network-firewall/#securing-amazon-sagemaker-studio-internet-traffic-using-aws-network-firewall","text":"Full github repo with code The work in this document complements previous work: - Building secure machine learning environments with Amazon SageMaker Securing Amazon SageMaker Studio connectivity using a private VPC.","title":"Securing Amazon SageMaker Studio internet traffic using AWS Network Firewall"},{"location":"sagemaker/secure-sagemaker-network-firewall/#background","text":"Amazon SageMaker Studio is a web-based fully integrated development environment (IDE) where you can perform end-to-end machine learning (ML) development to prepare data and build, train, and deploy models. Like other AWS services, Studio supports a rich set of security-related features that allow you to build highly secure and compliant environments. One of these fundamental security features allows you to launch Studio in your own Amazon Virtual Private Cloud (Amazon VPC). This allows you to control, monitor, and inspect network traffic within and outside your VPC using standard AWS networking and security capabilities. For more information, see Securing Amazon SageMaker Studio connectivity using a private VPC. Customers in regulated industries, such as financial services, often don\u2019t allow any internet access in ML environments. They often use only VPC endpoints for AWS services, and connect only to private source code repositories in which all libraries have been vetted both in terms of security and licensing. Customers may want to provide internet access but also have some controls such as domain name or URL filtering and allow access to only specific public repositories and websites, possibly packet inspection, or other network traffic-related security controls. For these cases, AWS Network Firewall and NAT gateway-based deployment may provide a suitable use case. In this post, we show how you can use Network Firewall to build a secure and compliant environment by restricting and monitoring internet access, inspecting traffic, and using stateless and stateful firewall engine rules to control the network flow between Studio notebooks and the internet. Depending on your security, compliance, and governance rules, you may not need to or cannot completely block internet access from Studio and your AI and ML workloads. You may have requirements beyond the scope of network security controls implemented by security groups and network access control lists (ACLs), such as application protocol protection, deep packet inspection, domain name filtering, and intrusion prevention system (IPS). Your network traffic controls may also require many more rules compared to what is currently supported in security groups and network ACLs. In these scenarios, you can use Network Firewall\u2014a managed network firewall and IPS for your VPC.","title":"Background"},{"location":"sagemaker/secure-sagemaker-network-firewall/#solution-overview","text":"When you deploy Studio in your VPC, you control how Studio accesses the internet with the parameter AppNetworkAccessType (via the Amazon SageMaker API ) or by selecting your preference on the console when you create a Studio domain. If you select Public internet Only ( PublicInternetOnly ), all the ingress and egress internet traffic from Amazon SageMaker notebooks flows through an AWS managed internet gateway attached to a VPC in your SageMaker account. The following diagram shows this network configuration. Studio provides public internet egress through a platform-managed VPC for data scientists to download notebooks, packages, and datasets. Traffic to the attached Amazon Elastic File System (Amazon EFS) volume always goes through the customer VPC and never through the public internet egress. To use your own control flow for the internet traffic, like a NAT or internet gateway, you must set the AppNetworkAccessType parameter to VpcOnly (or select VPC Only on the console). When you launch your app, this creates an elastic network interface in the specified subnets in your VPC. You can apply all available layers of security control\u2014 security groups , network ACLs , VPC endpoints , AWS PrivateLink , or Network Firewall endpoints \u2014to the internal network and internet traffic to exercise fine-grained control of network access in Studio. The following diagram shows the VpcOnly network configuration. In this mode, the direct internet access to or from notebooks is completely disabled, and all traffic is routed through an elastic network interface in your private VPC. This also includes traffic from Studio UI widgets and interfaces, such as Experiments , Autopilot , and Model Monitor , to their respective backend SageMaker APIs. For more information about network access parameters when creating a domain, see CreateDomain . The solution in this post uses the VpcOnly option and deploys the Studio domain into a VPC with three subnets: SageMaker subnet \u2013 Hosts all Studio workloads. All ingress and egress network flow is controlled by a security group. NAT subnet \u2013 Contains a NAT gateway. We use the NAT gateway to access the internet without exposing any private IP addresses from our private network. Network Firewall subnet \u2013 Contains a Network Firewall endpoint. The route tables are configured so that all inbound and outbound external network traffic is routed via Network Firewall. You can configure stateful and stateless Network Firewall policies to inspect, monitor, and control the traffic. The following diagram shows the overview of the solution architecture and the deployed components.","title":"Solution overview"},{"location":"sagemaker/secure-sagemaker-network-firewall/#vpc-resources","text":"The solution deploys the following resources in your account: A VPC with a specified Classless Inter-Domain Routing (CIDR) block Three private subnets with specified CIDRs Internet gateway, NAT gateway, Network Firewall, and a Network Firewall endpoint in the Network Firewall subnet A Network Firewall policy and stateful domain list group with an allow domain list Elastic IP allocated to the NAT gateway Two security groups for SageMaker workloads and VPC endpoints, respectively Four route tables with configured routes An Amazon S3 VPC endpoint (type Gateway) AWS service access VPC endpoints (type Interface) for various AWS services that need to be accessed from Studio The solution also creates an AWS Identity and Access Management (IAM) execution role for SageMaker notebooks and Studio with preconfigured IAM policies. Network routing for targets outside the VPC is configured in such a way that all ingress and egress internet traffic goes via the Network Firewall and NAT gateway. For details and reference network architectures with Network Firewall and NAT gateway, see Architecture with an internet gateway and a NAT gateway , Deployment models for AWS Network Firewall , and Enforce your AWS Network Firewall protections at scale with AWS Firewall Manager . The AWS re:Invent 2020 video Which inspection architecture is right for you? discusses which inspection architecture is right for your use case.","title":"VPC resources"},{"location":"sagemaker/secure-sagemaker-network-firewall/#sagemaker-resources","text":"The solution creates a SageMaker domain and user profile. The solution uses only one Availability Zone and is not highly available. A best practice is to use a Multi-AZ configuration for any production deployment. You can implement the highly available solution by duplicating the Single-AZ setup\u2014subnets, NAT gateway, and Network Firewall endpoints\u2014to additional Availability Zones. You use Network Firewall and its policies to control entry and exit of the internet traffic in your VPC. You create an allow domain list rule to allow internet access to the specified network domains only and block traffic to any domain not on the allow list.","title":"SageMaker resources"},{"location":"sagemaker/secure-sagemaker-network-firewall/#aws-cloudformation-resources","text":"The source code and AWS CloudFormation template for solution deployment are provided in the GitHub repository . To deploy the solution on your account, you need: An AWS account and the AWS Command Line Interface (AWS CLI) configured with administrator permissions An Amazon Simple Storage Service (Amazon S3) bucket in your account in the same Region where you deploy the solution Network Firewall is a Regional service; for more information on Region availability, see the AWS Region Table . Your CloudFormation stack doesn\u2019t have any required parameters. You may want to change the DomainName or *CIDR parameters to avoid naming conflicts with the existing resources and your VPC CIDR allocations. Otherwise, use the following default values: ProjectName \u2013 sagemaker-studio-vpc-firewall DomainName \u2013 sagemaker-anfw-domain UserProfileName \u2013 anfw-user-profile VPCCIDR \u2013 10.2.0.0/16 FirewallSubnetCIDR \u2013 10.2.1.0/24 NATGatewaySubnetCIDR \u2013 10.2.2.0/24 SageMakerStudioSubnetCIDR \u2013 10.2.3.0/24 Deploy the CloudFormation template To start experimenting with the Network Firewall and stateful rules, you need first to deploy the provided CloudFormation template to your AWS account. Clone the GitHub repository: git clone https://github.com/aws-samples/amazon-sagemaker-studio-vpc-networkfirewall.git cd amazon-sagemaker-studio-vpc-networkfirewall Create an S3 bucket in the Region where you deploy the solution: aws s3 mb s3://<your s3 bucket name> You can skip this step if you already have an S3 bucket. Deploy the CloudFormation stack: make deploy CFN_ARTEFACT_S3_BUCKET = <your s3 bucket name> The deployment procedure packages the CloudFormation template and copies it to the S3 bucket your provided. Then the CloudFormation template is deployed from the S3 bucket to your AWS account. The stack deploys all the needed resources like VPC, network devices, route tables, security groups, S3 buckets, IAM policies and roles, and VPC endpoints, and also creates a new Studio domain and user profile. When the deployment is complete, you can see the full list of stack output values by running the following command in terminal: aws cloudformation describe-stacks \\ --stack-name sagemaker-studio-demo \\ --output table \\ --query \"Stacks[0].Outputs[*].[OutputKey, OutputValue]\" Launch Studio via the SageMaker console.","title":"AWS CloudFormation resources"},{"location":"sagemaker/secure-sagemaker-network-firewall/#experiment-with-network-firewall","text":"Now you can learn how to control the internet inbound and outbound access with Network Firewall. In this section, we discuss the initial setup, accessing resources not on the allow list, adding domains to the allow list, configuring logging, and additional firewall rules.","title":"Experiment with Network Firewall"},{"location":"sagemaker/secure-sagemaker-network-firewall/#initial-setup","text":"The solution deploys a Network Firewall policy with a stateful rule group with an allow domain list. This policy is attached to the Network Firewall. All inbound and outbound internet traffic is blocked now, except for the .kaggle.com domain, which is on the allow list. Let\u2019s try to access https://kaggle.com by opening a new notebook in Studio and attempting to download the front page from kaggle.com : !wget https://kaggle.com The following screenshot shows that the request succeeds because the domain is allowed by the firewall policy. Users can connect to this and only to this domain from any Studio notebook.","title":"Initial setup"},{"location":"sagemaker/secure-sagemaker-network-firewall/#access-resources-not-on-the-allowed-domain-list","text":"In the Studio notebook, try to clone any public GitHub repository, such as the following: !git clone https://github.com/aws-samples/amazon-sagemaker-studio-vpc-networkfirewall.git This operation times out after 5 minutes because any internet traffic except to and from the .kaggle.com domain isn\u2019t allowed and is dropped by Network Firewall.","title":"Access resources not on the allowed domain list"},{"location":"sagemaker/secure-sagemaker-network-firewall/#add-a-domain-to-the-allowed-domain-list","text":"To be able to run the git clone command, you must allow internet traffic to the .github.com domain. On the Amazon VPC console, choose Firewall policies. Choose the policy network-firewall-policy- . In the Stateful rule groups section, select the group rule domain-allow-sagemaker- . You can see the domain .kaggle.com on the allow list. Choose Add domain. Enter .github.com . Choose Save. You now have two names on the allow domain list. Firewall policy is propagated in real time to Network Firewall and your changes take effect immediately. Any inbound or outbound traffic from or to these domains is now allowed by the firewall and all other traffic is dropped. To validate the new configuration, go to your Studio notebook and try to clone the same GitHub repository again: !git clone https://github.com/aws-samples/amazon-sagemaker-studio-vpc-networkfirewall.git The operation succeeds this time\u2014Network Firewall allows access to the .github.com domain.","title":"Add a domain to the allowed domain list"},{"location":"sagemaker/secure-sagemaker-network-firewall/#network-firewall-logging","text":"In this section, you configure Network Firewall logging for your firewall\u2019s stateful engine. Logging gives you detailed information about network traffic, including the time that the stateful engine received a packet, detailed information about the packet, and any stateful rule action taken against the packet. The logs are published to the log destination that you configured, where you can retrieve and view them. On the Amazon VPC console, choose Firewalls . Choose your firewall. Choose the Firewall details tab. In the Logging section, choose Edit . Configure your firewall logging by selecting what log types you want to capture and providing the log destination. For this post, select Alert log type, set Log destination for alerts to CloudWatch Log group, and provide an existing or a new log group where the firewall logs are delivered. Choose Save . To check your settings, go back to Studio and try to access pypi.org to install a Python package: !pip install -U scikit-learn This command fails with ReadTimeoutError because Network Firewall drops any traffic to any domain not on the allow list (which contains only two domains: .github.com and .kaggle.com ). On the Amazon CloudWatch console , navigate to the log group and browse through the recent log streams. The pipy.org domain shows the blocked action. The log event also provides additional details such as various timestamps, protocol, port and IP details, event type, availability zone, and the firewall name. You can continue experimenting with Network Firewall by adding .pypi.org and .pythonhosted.org domains to the allowed domain list. Then validate your access to them via your Studio notebook.","title":"Network Firewall logging"},{"location":"sagemaker/secure-sagemaker-network-firewall/#additional-firewall-rules","text":"You can create any other stateless or stateful firewall rules and implement traffic filtering based on a standard stateful 5-tuple rule for network traffic inspection (protocol, source IP, source port, destination IP, destination port). Network Firewall also supports industry standard stateful Suricata compatible IPS rule groups. You can implement protocol-based rules to detect and block any non-standard or promiscuous usage or activity. For more information about creating and managing Network Firewall rule groups, see Rule groups in AWS Network Firewall.","title":"Additional firewall rules"},{"location":"sagemaker/secure-sagemaker-network-firewall/#additional-security-controls-with-network-firewall","text":"In the previous section, we looked at one feature of the Network Firewall: filtering network traffic based on the domain name. In addition to stateless or stateful firewall rules, Network Firewall provides several tools and features for further security controls and monitoring: Central firewall management and visibility in AWS Firewall Manager . You can centrally manage security policies and automatically enforce mandatory security policies across existing and newly created accounts and VPCs. Network Firewall logging for the firewall\u2019s stateful engine. You can record flow and alert logs, and use the same or different logging destinations for each log type. Stateless rules to filter network traffic based on protocol, source IP addresses, ranges, source port ranges, destination IP addresses and ranges, and TCP flags. Integration into a broader set of AWS security components. For an example, see Automatically block suspicious traffic with AWS Network Firewall and Amazon GuardDuty. Integration in a diverse ecosystem of Network Firewall Partners that complement Network Firewall, enabling the deployment of a comprehensive security architecture. For example use cases, see Full VPC traffic visibility with AWS Network Firewall and Sumo Logic and Splunk Named Launch Partner of AWS Network Firewall.","title":"Additional security controls with Network Firewall"},{"location":"sagemaker/secure-sagemaker-network-firewall/#build-secure-ml-environments","text":"A robust security design normally includes multi-layer security controls for the system. For SageMaker environments and workloads, you can use the following AWS security services and concepts to secure, control, and monitor your environment: VPC and private subnets to perform secure API calls to other AWS services and restrict internet access for downloading packages. S3 bucket policies that restrict access to specific VPC endpoints. Encryption of ML model artifacts and other system artifacts that are either in transit or at rest. Requests to the SageMaker API and console are made over a Secure Sockets Layer (SSL) connection. Restricted IAM roles and policies for SageMaker runs and notebook access based on resource tags and project ID. Restricted access to Amazon public services, such as Amazon Elastic Container Registry (Amazon ECR) to VPC endpoints only. For a reference deployment architecture and ready-to-use deployable constructs for your environment, see Amazon SageMaker with Guardrails on AWS.","title":"Build secure ML environments"},{"location":"sagemaker/secure-sagemaker-network-firewall/#conclusion","text":"In this post, we showed how you can secure, log, and monitor internet ingress and egress traffic in Studio notebooks for your sensitive ML workloads using managed Network Firewall. You can use the provided CloudFormation templates to automate SageMaker deployment as part of your Infrastructure as Code (IaC) strategy. For more information about other possibilities to secure your SageMaker deployments and ML workloads, see Building secure machine learning environments with Amazon SageMaker.","title":"Conclusion"}]}